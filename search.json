[
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "2 Enthusiastic Data Scientist",
    "text": "2 Enthusiastic Data Scientist\n\n2.1 Interests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n01-Data Lake Storage\n\n\n\n\n\n\nAzure Cloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI 외 도구 호출 에이전트(Tool Calling Agent)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도구 (Tools)\n\n\n\n\n\n\nAI\n\nLangChain\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 기능과 사람 개입(Human-in-the-loop)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM 에 도구 바인딩(Binding Tools)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도구 호출 에이전트(Tool Calling Agent)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Retriever Convex Combination(CC) 추가\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n한글 단어 리트리버 튜닝\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1 — 세그먼트별 프롬프트 기획\n\n\n\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 — 프롬프트 기획 사례\n\n\n\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 — 프롬프트 구조와 기능 탐구\n\n\n\n\n\n\nAI\n\nLangChain\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1 — 프롬프트 구조와 기능 탐구\n\n\n\n\n\n\nAI\n\nLangChain\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1 — 프롬프트 분석의 필요성과 가치\n\n\n\n\n\n\nAI\n\nLangChain\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 — 사용자 언어 읽기(프롬프트 구조와 유형)\n\n\n\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3 — 대화 분석(사용자-AI 상호작용 매커니즘)\n\n\n\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n프롬프트 엔지니어링 기초 기법 (Basic)\n\n\n\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent\n\n\nAgent Development Content List\n\n\n\nAI\n\nAgent\n\n\n\nAI Agent 개발 과정에서의 설계 결정, 시행착오, 아키텍처 패턴을 다루는 블로그 모음입니다. 코드분석 Agent, 업무 자동화 Agent 등 다양한 Agent 프로젝트의 실전 경험을 정리합니다.\n\n\n\n\n\nFeb 19, 2026\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nLLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙\n\n\n152개 Python 파일에서 1,411개 항목을 추출하며 배운 것들\n\n\n\nAgent\n\nCode_Analysis\n\nPrompt_Engineering\n\n\n\nClaude Code의 분산 에이전트로 대규모 코드베이스(AutoMSA, 152 Python 파일)의 메타데이터를 전수 추출한 실전 경험입니다. 단일 에이전트 실패, 형식 불일치, 원자적 디테일 소실까지 – 각 시행착오에서 도출한 설계 원칙을 정리합니다.\n\n\n\n\n\nFeb 13, 2026\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계\n\n\n코드 파서가 사실을 고정하고, LLM은 해석만 담당한다\n\n\n\nAgent\n\nCode_Analysis\n\nArchitecture\n\n\n\n순수 LLM 접근의 한계(비결정성, 높은 비용, 낮은 재현성)를 극복하기 위한 하이브리드 파이프라인 설계입니다. Python AST로 75%를 결정적으로 추출하고, LLM은 논리명과 의미 해석만 담당하여 99% 재현성을 달성합니다.\n\n\n\n\n\nFeb 13, 2026\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nClaude Code\n\n\nClaude Code Content List\n\n\n\nAI\n\nClaude_Code\n\n\n\nClaude Code(Anthropic의 CLI 기반 AI 코딩 어시스턴트)를 활용한 개발 환경 구축, 팀 설정, 워크플로우 자동화, 그리고 실전 활용 사례를 다루는 블로그 모음입니다.\n\n\n\n\n\nFeb 12, 2026\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nClaude Code Max 팀 설정 완전 가이드\n\n\nSSH 클론부터 bioSkills 425개까지, 팀 개발 환경을 한 번에 구축하기\n\n\n\nAI\n\nClaude_Code\n\nDevOps\n\n\n\nAnthropic의 Claude Code Max를 활용한 팀 개발 환경 구축 과정을 정리합니다. SSH 설정, 팀 설정 설치, 생산성 도구, bioSkills, SpecKit, Hook 시스템까지 실제 세팅 과정을 단계별로 기록한 실전 가이드입니다.\n\n\n\n\n\nFeb 12, 2026\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nParent Document Retriever\n\n\n계층적 문서 구조를 활용한 RAG 최적화\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n문서 청킹의 딜레마(정확한 임베딩 vs 충분한 맥락)를 해결하기 위해 ParentDocumentRetriever가 작은 청크로 검색하고 큰 청크를 반환하는 계층적 검색 전략을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n문맥 압축 검색기(ContextualCompressionRetriever)\n\n\n검색기\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n문서 검색을 위한 다양한 Retriever 패턴과 최적화 기법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMultiVectorRetriever\n\n\n계층적 문서 구조를 활용한 고급 검색 시스템\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n한 문서에 여러 벡터를 생성하여 다양한 관점에서 검색할 수 있는 MultiVectorRetriever의 원리와 구현 방법을 다룬다. DocStore와 VectorStore를 동시에 활용하여 청크 단위 검색과 원본 문서 반환을 유연하게 조합하는 전략을 학습한다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n앙상블 검색기(Ensemble Retriever)\n\n\n검색기\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n문서 검색을 위한 다양한 Retriever 패턴과 최적화 기법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n벡터스토어 기반 검색기(VectorStore-backed Retriever)\n\n\n검색기\n\n\n\nAI\n\nRAG\n\nLangChain\n\n\n\n문서 검색을 위한 다양한 Retriever 패턴과 최적화 기법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMultiQueryRetriever\n\n\n다중 쿼리 생성을 통한 RAG 검색 성능 향상\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n사용자의 단일 쿼리를 여러 관점의 다양한 쿼리로 확장하여 검색 결과를 풍부하게 만드는 MultiQueryRetriever의 원리와 구현 방법을 다룬다. 거리 기반 벡터 검색의 한계를 극복하고 더 포괄적인 검색 결과를 얻는 전략을 학습한다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n긴 문맥 재정렬(LongContextReorder)\n\n\nLost in the Middle 현상 해결과 RAG 성능 최적화\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\nLLM이 긴 문맥의 중간 정보를 간과하는 ‘Lost in the Middle’ 현상의 원인을 분석하고, LangChain의 LongContextReorder를 활용하여 문서 순서를 재배열함으로써 RAG 시스템의 답변 정확도를 향상시키는 방법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)\n\n\n의미적 유사도와 시간 기반 신선도를 결합한 검색\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n의미론적 유사성과 시간 감쇠를 결합하여 최신성과 관련성을 동시에 고려하는 TimeWeightedVectorStoreRetriever의 원리와 활용 방법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\n셀프 쿼리 검색기(Self-Query Retriever)\n\n\n메타데이터 기반 지능형 문서 검색\n\n\n\nAI\n\nRAG\n\nLangChain\n\nAgent\n\n\n\n자연어 질의를 구조화된 쿼리로 변환하여 메타데이터 필터링과 의미적 검색을 동시에 수행하는 SelfQueryRetriever의 원리와 활용 방법을 다룬다.\n\n\n\n\n\nDec 5, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nAzure Cloud\n\n\nAzure Cloud Content List\n\n\n\nAzure Cloud\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nRAG\n\n\nRAG Content List\n\n\n\nAI\n\nLangChain\n\nRAG\n\n\n\nRetrieval-Augmented Generation (RAG) 기술의 전체 구현 과정을 다루는 종합 가이드. LangChain을 활용한 RAG 시스템 구축, 문서 로딩, 임베딩, 벡터 저장소, 검색기, 그리고 고급 RAG 기법까지 단계별로 학습할 수 있는 실습 중심의 블로그 모음입니다.\n\n\n\n\n\nNov 1, 2025\n\n\nJunhyun Lee\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt_Engineering\n\n\nPrompt_Engineering Content List\n\n\n\nAI\n\nPrompt_Engineering\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\nJunhyun Lee\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog",
      "Blog Posts"
    ]
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/index.html",
    "href": "docs/blog/posts/Prompt_Engineering/index.html",
    "title": "Prompt_Engineering",
    "section": "",
    "text": "01-프롬프트 구조(1)\n02-프롬프트 구조(2)\n\n\n\n\n\n01-프롬프트 분석의 필요성과 가치\n02-사용자 언어 읽기(프롬프트 구조와 유형)\n03-대화 분석(사용자-AI 상호작용 매커니즘)\n\n\n\n\n\n01-세그먼트별 프롬프트 기획\n02-프롬프트 기획 사례\n\n\n\n\n\n01-프롬프트 제작의 기초",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Prompt_Engineering"
    ]
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/index.html#prompt_engineering",
    "href": "docs/blog/posts/Prompt_Engineering/index.html#prompt_engineering",
    "title": "Prompt_Engineering",
    "section": "",
    "text": "01-프롬프트 구조(1)\n02-프롬프트 구조(2)\n\n\n\n\n\n01-프롬프트 분석의 필요성과 가치\n02-사용자 언어 읽기(프롬프트 구조와 유형)\n03-대화 분석(사용자-AI 상호작용 매커니즘)\n\n\n\n\n\n01-세그먼트별 프롬프트 기획\n02-프롬프트 기획 사례\n\n\n\n\n\n01-프롬프트 제작의 기초",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Prompt_Engineering"
    ]
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "",
    "text": "이 장을 통해 다음을 학습할 수 있습니다:\n\n내가 쓰는 프롬프트가 어떤 스타일인지 파악: 5가지 프롬프트 유형을 이해하고 자신이 주로 사용하는 스타일을 확인할 수 있습니다.\n보완할 수 있는 요소 파악: 현재 프롬프트에 부족한 요소들을 확인하고, 어떤 부분을 보완할 수 있는지에 대한 아이디어를 얻을 수 있습니다.\n\n프롬프트 작문 유형은 사용자가 AI에게 요청하는 방식에 따라 크게 5가지로 분류할 수 있습니다. 각 유형은 고유한 특징과 사용 목적을 가지고 있으며, 상황에 맞는 유형을 선택하는 것이 효과적인 프롬프트 작성의 핵심입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#명령청유형",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#명령청유형",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.1 명령/청유형",
    "text": "2.1 명령/청유형\n\n2.1.1 특징\n\n직접적이고 명확한 지시나 요청을 하는 유형\n“~해줘”, “~알려줘”, “~말해” 등의 명령형/청유형 문장 사용\n구체적인 작업 수행을 요구\n\n\n\n2.1.2 예시\n\n“업장에 산학 실습을 가는데 실습개요와 목표를 알려줘”\n“우아한 거짓말 독후감 써줘”\n“봉은사역에서 메가박스가는 길 말해”\n“Rtx4090 가장 싸게 사는 방법 알려줘봐”\n“저는 이전에는 없던 창업 아이디어를 찾고 있는데, 어떤 분야를 추천 주시겠어요?”\n\n\n\n2.1.3 프롬프트 작성 실습\n실습 문제\n중요한 회의 일정을 변경해야 하는 상황입니다. 팀원에게 보낼 정중하고 간결한 이메일을 작성하는 프롬프트를 제작해보세요.\n잘 만들어진 프롬프트 예시\n중요한 회의 일정을 변경해야해.\n팀원들에게 보내는 정중하고 간결한 이메일을 작성해.\n다음 사항을 반드시 포함해.\n\n1. 회의 일정 변경 사유: 부득이한 개인 사정으로 연차\n2. 변경된 새로운 회의 일정 알림: 8/7일 오후 2시에서 8/12일 오후 2시, 3A 회의룸으로 변경\n3. 회의 참여를 독려하는 문구 추가\n4. 회의 준비 사항\n5. 마무리 인사말\n\n💡 팁: 이 프롬프트는 명령/청유형의 특징을 잘 보여주는 예시입니다. 명확한 작업 지시와 포함해야 할 항목을 번호로 구분하여 제시하고 있습니다.\n\n결과\n프롬프트를 실행하면 다음과 같은 이메일이 생성됩니다:\n제목: 회의 일정 변경 안내\n\n안녕하세요, 팀원 여러분.\n\n부득이한 개인 사정으로 인해 원래 예정된 회의 일정을 변경하게 되었습니다. 변경된 새로운 회의 일정은 아래와 같습니다.\n\n• 변경 전: 8/7일 오후 2시\n• 변경 후: 8/12일 오후 2시\n• 장소: 3A 회의룸\n\n회의 준비를 위해 관련 자료를 미리 검토해 주시고, 참석 가능 여부를 확인해 주시면 감사하겠습니다. 모든 팀원들의 참여를 부탁드립니다.\n\n추가로 궁금한 점이나 문제가 있으면 언제든지 저에게 연락해 주십시오.\n\n감사합니다.\n\n[귀하의 이름]\n\n\n2.1.4 핵심 포인트\n그래서 명령/청유형 프롬프트의 핵심 포인트는 다음과 같습니다:\n\n명확한 작업 지시: “이메일을 작성해”, “알려줘”처럼 AI가 수행할 작업을 명확하게 지시합니다.\n포함해야 할 항목을 번호로 구분하여 제시: 번호나 불릿 포인트로 포함할 내용을 구조화하여 제시하면 AI가 빠짐없이 처리할 수 있습니다.\n구체적인 정보 제공: 날짜, 시간, 장소 등 구체적인 정보를 포함하면 더 정확한 결과를 얻을 수 있습니다.\n커스텀 영역은 상황에 맞게 직접 입력: 변경 가능한 부분은 대괄호나 명시적 표시로 구분하여 사용자가 상황에 맞게 수정할 수 있도록 합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#역할-지정",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#역할-지정",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.2 역할 지정",
    "text": "2.2 역할 지정\n\n2.2.1 특징\n\nAI에게 특정 역할이나 페르소나를 부여하는 유형\n“너는 ~야”, “당신은 ~입니다” 등의 표현 사용\n역할에 맞는 전문성과 톤을 요구\n\n\n\n2.2.2 예시\n\n“너는 고객서비스 상담사처럼 행동해. 고객의 감정에 최대한 공감할 수 있는 표현을 사용해.”\n“너는 헤르만 헤세야. 헤르만 헤세의 작품을 통해 인간의 감정과 사회적 이슈를 알고 싶은 독자를 위해 상세하게 설명해줘.”\n“당신은 파이썬을 잘하는 프로그래머 입니다. 만약 내가 코드 만들어줘라고 하면 코드를 만들어 주면 됩니다.”\n“당신은 평가자입니다. 6학년 초등학생의 평가 결과를 개조식으로 알려줘야 합니다.”\n\n\n\n2.2.3 프롬프트 작성 실습\n실습 문제\n당신은 신생 친환경 스타트업의 마케팅 책임자입니다. Z세대를 타겟으로 한 소셜 미디어 마케팅 전략 5가지를 제안하는 프롬프트를 작성해보세요.\n잘 만들어진 프롬프트 예시\n당신은 신생 친환경 스타트업의 마케팅 책임자입니다.\nZ세대를 타겟으로 한 소셜 미디어 마케팅 전략 5가지를 제안해주세요.\n&lt;인플루언서 협업&gt;을 소재로 해주세요.\n\n- 각 전략의 구체적인 설명\n- 전략이 효과적인 이유\n- 실행 방법\n- 기대되는 결과\n- 성공 여부를 평가하는 방법\n결과\n프롬프트를 실행하면 다음과 같은 마케팅 전략이 생성됩니다:\n1. 친환경 인플루언서와의 제품 리뷰\n\n구체적인 설명: 친환경 인플루언서에게 제품을 제공하여 그들의 채널에서 제품을 리뷰하게 하고, 제품의 장점을 강조하고 실제 사용 경험을 공유하도록 합니다.\n전략이 효과적인 이유: Z세대는 진정성을 중시하며 인플루언서 리뷰를 신뢰합니다. 친환경 인플루언서와의 협업은 자연스럽게 제품의 가치를 전달할 수 있습니다.\n실행 방법: 관련 인플루언서를 찾아 연락하고, 제품을 제공하여 리뷰를 요청합니다. 콘텐츠 제작을 모니터링하고 필요시 적극적으로 지원합니다.\n기대되는 결과: 제품에 대한 긍정적인 인식 증가, 브랜드 인지도 향상, 초기 구매자 증가\n성공 여부를 평가하는 방법: 인플루언서 게시물 조회수, 좋아요, 댓글 수를 모니터링하고, 리뷰 이후 판매 증가를 추적합니다.\n\n2. 인플루언서 주최 온라인 이벤트\n\n구체적인 설명: 인플루언서가 주최하는 온라인 이벤트를 통해 제품을 홍보합니다. 예를 들어, Q&A 세션, 사용법 튜토리얼, 라이브 스트리밍 등을 통해 실시간으로 사용자와 소통하며 제품을 소개합니다.\n전략이 효과적인 이유: 실시간 소통은 Z세대의 높은 참여를 유도하며, 인터랙티브한 이벤트는 긍정적인 브랜드 이미지를 구축합니다.\n실행 방법: 인플루언서와 협력하여 이벤트 주제와 일정을 결정하고, 소셜 미디어 광고와 게시물로 홍보합니다. 이벤트 당일 기술 지원과 상품을 준비하여 참여를 독려합니다.\n기대되는 결과: 브랜드 참여도 증가, 제품 인지도 향상, 사용자와의 직접적인 소통 강화\n성공 여부를 평가하는 방법: 이벤트 참여자 수, 실시간 시청자 수, 이벤트 이후 제품 판매량, 이벤트 관련 소셜 미디어 언급량을 분석합니다.\n\n3. 소셜 미디어 챌린지\n\n구체적인 설명: 인플루언서가 주도하는 소셜 미디어 챌린지를 기획하여 친환경 제품 사용에 대한 자발적인 참여를 유도합니다.\n전략이 효과적인 이유: Z세대는 트렌디한 챌린지에 참여하는 것을 즐기며, 챌린지는 바이럴 효과를 낼 수 있습니다.\n실행 방법: 인플루언서와 협력하여 챌린지 콘텐츠를 기획하고, 참여 방법과 해시태그를 홍보합니다. 참여자에게 소정의 상품을 제공하거나 우승자에게 특별 혜택을 제공합니다.\n기대되는 결과: 높은 소셜 미디어 참여도, 제품 인지도 향상, 긍정적인 브랜드 이미지 형성\n성공 여부를 평가하는 방법: 챌린지 참여 게시물 수, 관련 해시태그 사용 빈도, 소셜 미디어 언급량, 제품 판매량 변화를 추적합니다.\n\n4. 인플루언서와의 공동 콘텐츠 제작\n\n구체적인 설명: 친환경 라이프스타일을 주제로 인플루언서와 함께 콘텐츠를 제작합니다. 예를 들어, 친환경 제품을 활용한 DIY 프로젝트나 지속 가능한 생활 팁에 대한 영상/게시물을 만듭니다.\n전략이 효과적인 이유: 공동 콘텐츠는 자연스럽게 브랜드와 제품을 노출시키며, 인플루언서의 팬들은 이 콘텐츠를 통해 제품에 관심을 갖게 됩니다.\n실행 방법: 인플루언서와 콘텐츠 아이디어를 논의하고, 제작 및 편집 과정에 협력합니다. 콘텐츠가 게시되면 소셜 미디어 채널을 통해 적극적으로 홍보합니다.\n기대되는 결과: 브랜드 인지도 증가, 콘텐츠를 통한 긍정적인 브랜드 이미지 형성, 신규 고객 확보\n성공 여부를 평가하는 방법: 콘텐츠 조회수, 좋아요, 댓글 수를 분석하고, 콘텐츠 관련 판매 증가를 추적합니다.\n\n5. 인플루언서와의 지속 가능한 캠페인\n\n구체적인 설명: 인플루언서와 함께 장기적인 친환경 캠페인을 기획합니다. 예를 들어, 한 달간 플라스틱 사용 줄이기나 일주일간 비건 식단 도전과 같은 지속 가능한 라이프스타일을 홍보합니다.\n전략이 효과적인 이유: 장기적인 캠페인은 Z세대에게 깊은 인상을 남기며, 지속 가능한 생활 실천에 대한 관심을 높이는 데 효과적입니다.\n실행 방법: 인플루언서와 캠페인 주제와 기간을 설정하고, 소셜 미디어에 캠페인 콘텐츠를 지속적으로 게시합니다. 참여자에게 인센티브를 제공하고 캠페인 진행 상황을 정기적으로 업데이트합니다.\n기대되는 결과: 브랜드의 지속 가능성에 대한 인식 강화, 장기적인 브랜드 로열티 형성, 지속 가능한 라이프스타일 실천 문화 확산\n성공 여부를 평가하는 방법: 캠페인 기간 동안의 참여자 수, 소셜 미디어 참여도, 브랜드 인지도 변화, 장기적인 판매 추세를 분석합니다.\n\n\n\n2.2.4 핵심 포인트\n위의 잘 만들어진 프롬프트 예시를 통해 역할 지정 프롬프트의 핵심 포인트를 살펴보겠습니다:\n\n명확한 역할 정의: 프롬프트 첫 줄에서 “당신은 신생 친환경 스타트업의 마케팅 책임자입니다”라고 명확하게 역할을 정의했습니다. 이렇게 구체적인 역할을 부여하면 AI가 해당 전문가의 지식과 경험을 활용하여 답변할 수 있습니다.\n대상과 맥락 명시: “Z세대를 타겟으로 한”이라는 표현으로 답변의 대상과 맥락을 명확히 했습니다. 이를 통해 역할이 특정 대상에 맞는 전략을 제안할 수 있습니다.\n구체적인 소재나 방향 제시: “&lt;인플루언서 협업&gt;을 소재로 해주세요”처럼 구체적인 소재나 방향을 제시하면 AI가 더 집중된 답변을 생성할 수 있습니다.\n구체적인 출력 형식 지정: 불릿 포인트로 각 전략의 설명, 효과적인 이유, 실행 방법, 기대되는 결과, 평가 방법을 요구했습니다. 이렇게 구조화된 형식을 지정하면 더 체계적이고 실용적인 결과를 얻을 수 있습니다.\n역할에 맞는 전문성과 관점 요구: 마케팅 책임자라는 역할을 부여함으로써, AI는 마케팅 전문가의 관점에서 실무 경험과 업계 지식을 바탕으로 답변하게 됩니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#상황극형",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#상황극형",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.3 상황극형",
    "text": "2.3 상황극형\n\n2.3.1 특징\n\n특정 상황이나 맥락을 설정하여 대화를 진행하는 유형\n“지금 ~에 있어”, “우리는 ~하고 있어” 등의 상황 설정 표현 사용\n역할극이나 시뮬레이션 형태\n\n\n\n2.3.2 예시\n\n“너는 지금 곰하고 같이 있어. 곰을 따라하는 게 너의 역할이야. 크앙, 그르렁, Grrr…., 울음소리로만 대화해야해.”\n“나는 지금 돈이 2만원 밖에 없어. 그런데 번역레포트를 제출해야하는데. 전문가를 구할 돈이 없어. 너가 대신 내 번역물을 프루프 리딩을 해줘.”\n“우리는 지금 심리상담 센터에 같이 있어. 너는 인공지능이야. 말 시작할 때 흐음 으로 항상 시작하면서 심리상담을 해주길 바래.”\n\n\n\n2.3.3 프롬프트 작성 실습\n실습 문제\nChatGPT의 GPTs를 이용하여, 🐻 곰과의 대화 페르소나 챗봇을 만들어보세요.\n잘 만들어진 프롬프트 예시\n너는 지금 곰하고 같이 있어. 곰을 따라하는 게 너의 역할이야.\n\"크앙\", \"그르렁\", \"Grrr ....\", 울음소리로만 대화해야해.\n최대한 곰처럼 흉내내. 말은 너무 길게 하지마.\n곰은 대체로 간결하게 말해.\n다 말하고나서는 ( ) 사용해서, 곰의 언어를 통역해줘.\n대화를 이어갈 수 있도록.\n결과\n\n\n\n2.3.4 핵심 포인트\n위의 잘 만들어진 프롬프트 예시를 통해 상황극형 프롬프트의 핵심 포인트를 살펴보겠습니다:\n\n구체적인 상황과 맥락 설정: 프롬프트 첫 줄에서 “너는 지금 곰하고 같이 있어”라고 현재 상황을 명확하게 설정했습니다. 이렇게 구체적인 상황을 제시하면 AI가 그 맥락 안에서 행동할 수 있습니다.\n역할과 행동 방식 명시: “곰을 따라하는 게 너의 역할이야”로 역할을 명시하고, “크앙”, “그르렁”, “Grrr ….”, 울음소리로만 대화해야 한다고 행동 방식을 구체적으로 지정했습니다.\n스타일과 톤 지정: “최대한 곰처럼 흉내내”, “말은 너무 길게 하지마”, “곰은 대체로 간결하게 말해”처럼 캐릭터에 맞는 스타일과 톤을 명시했습니다.\n대화 형식이나 출력 방식 지정: “다 말하고나서는 ( ) 사용해서, 곰의 언어를 통역해줘”로 대화를 이어갈 수 있도록 통역 형식을 지정했습니다. 이를 통해 사용자가 곰의 언어를 이해하고 대화를 지속할 수 있습니다.\n대화 지속성 고려: “대화를 이어갈 수 있도록”이라는 지시로 단순한 일회성 응답이 아닌 지속적인 대화가 가능하도록 했습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#설명형",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#설명형",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.4 설명형",
    "text": "2.4 설명형\n\n2.4.1 특징\n\n특정 주제나 개념에 대한 상세한 설명을 요구하는 유형\n명령/청유형보다 더 구체적인 요구사항을 포함하는 유형\n뭐뭐를 해 달라고 하는 데에 있어서 좀더 구체적이게 작성\n출력물의 형식, 구조, 포함 요소, 분량 등을 상세하게 지정\n\n\n\n2.4.2 예시\n\n“2023년 알뜰교통카드 신청 안내 - 대중교통비 최대 30까지 절감할 수 있는 교통카드”\n“전문 블로거로써, 키워드와 연관 키워드를 포함해서 제목을 추천하고, 밑에는 메타설명은 키워드를 포함해서 작성해줘.”\n“마크다운 형식의 3000자 내외의 블로그 글로 작성해줘.”\n“SEO 제목의 앞 부분에서 포커스 키워드와 숫자를 추가해 주세요.”\n코딩 문제 설명 및 해결 방법 요청\n\n\n\n2.4.3 프롬프트 작성 실습\n실습 문제\n클라우드 컴퓨팅의 개념과 장점을 기술에 익숙하지 않은 경영진에게 설명하는 A4 1장 이내 분량의 보고서를 작성하는 프롬프트를 제작해보세요.\n잘 만들어진 프롬프트 예시\n설명형은 명령/청유형보다 더 구체적인 요구사항을 포함하므로, 프롬프트를 자세하게 작성하는 것이 중요합니다.\n클라우드 컴퓨팅의 개념과 장점을 A4 이내의 분량으로 작성해줘.\n보고서를 읽을 대상은 기술 개념을 이해하기 어려워하는 경영진이야.\n작성 내용은 아래의 지시를 따라줘.\n\n중심 내용: 클라우드 컴퓨팅의 정의, 기본 원리, 주요 장점\n문장 스타일: 경영진이 이해할 수 있도록 하는 쉬운 예시나 비유를 사용\n\n참고 텍스트를 사용해서 내용을 상세하게 써줘.\n참고 텍스트\n클라우드 컴퓨팅의 여러 이점 https://cloud.google.com/learn/advantages-of-cloud-computing 중 하나는 사용한 만큼만 비용을 지불하면 된다는 것입니다. 따라서 조직은 자체 물리적 데이터 센터와 서버를 구매하고 유지관리할 필요 없이 더 빠르고 효율적으로 확장할 수 있습니다. 간단히 말해 클라우드 컴퓨팅은 네트워크(대부분 인터넷)를 사용하여 사용자를 클라우드 플랫폼에 연결하고 임대된 컴퓨팅 서비스를 요청하고 액세스합니다. 중앙 서버는 클라이언트 기기와 서버 간의 모든 통신을 처리하여 데이터 교환을 용이하게 합니다. 보안 및 개인 정보 보호 기능은 이러한 정보를 안전하게 보호하기 위한 일반적인 구성요소입니다. 클라우드 컴퓨팅 아키텍처를 채택할 때 일률적인 해결책은 없습니다. 다른 회사에 효과적인 해결책이 본인의 비즈니스 요구사항에 맞지 않을 수 있습니다. 사실 이러한 유연성과 다기능성은 클라우드의 대표적 특징 중 하나로, 기업은 변화하는 시장 또는 측정항목에 빠르게 적응할 수 있습니다. 클라우드 컴퓨팅 배포 모델은 퍼블릭 클라우드, 프라이빗 클라우드, 하이브리드 클라우드로 3가지가 있습니다.\n결과\n프롬프트를 실행하면 다음과 같은 보고서가 생성됩니다:\n클라우드 컴퓨팅: 경영진을 위한 이해\n\n1. 클라우드 컴퓨팅의 정의와 기본 원리\n\n클라우드 컴퓨팅은 인터넷을 통해 다양한 IT 리소스(서버, 저장소, 데이터베이스, 네트워킹, 소프트웨어 등)를 제공하는 서비스입니다. 전기나 수도처럼 필요할 때만 사용한 만큼 비용을 지불하는 방식과 비슷하다고 볼 수 있습니다. 클라우드 컴퓨팅을 사용하는 기업은 물리적 서버를 직접 소유하거나 관리할 필요가 없으며, 필요한 만큼 IT 리소스에 접근하여 사용할 수 있습니다. 이러한 리소스는 서비스 제공업체가 원격 서버에 저장하고 있으며, 사용자는 인터넷을 통해 접근합니다.\n\n2. 비즈니스 효과\n\n클라우드 컴퓨팅을 도입하면 기업은 비용 절감, 운영 효율성 향상, 유연한 비즈니스 확장, 원격 근무 지원, 최신 기술 도입 용이 등의 효과를 얻을 수 있습니다. 이를 통해 기업은 경쟁력을 유지하고 변화하는 시장 상황에 빠르게 대응할 수 있습니다. 클라우드 컴퓨팅은 단순한 IT 인프라 변화가 아니라 기업의 전체적인 비즈니스 전략을 혁신하는 중요한 요소입니다. 기업이 더 효율적이고 유연하게 운영할 수 있게 하여, 궁극적으로 더 높은 수익성과 경쟁력을 확보할 수 있게 합니다.\n\n3. 주요 장점\n\n3.1 비용 절감\n클라우드 컴퓨팅 도입 시 초기 자본 지출을 크게 줄일 수 있습니다. IT 인프라 구매 및 유지관리와 관련된 비용을 절감할 수 있으며, 기업은 사용한 리소스에 대해서만 비용을 지불합니다. 예를 들어, 성수기에는 더 많은 리소스를 사용하고 비수기에는 적게 사용할 수 있습니다.\n\n3.2 운영 효율성 향상\n클라우드 컴퓨팅은 자동화된 관리 도구와 효율적인 리소스 할당을 제공합니다. 이를 통해 IT 팀이 더 중요한 업무에 집중할 수 있어 기업 전체의 생산성이 향상됩니다.\n\n3.3 비즈니스 확장의 유연성\n기업이 비즈니스 확장 필요에 빠르게 대응할 수 있습니다. 예를 들어, 새로운 시장 진입이나 신제품 출시 시 필요한 IT 리소스를 빠르게 추가할 수 있습니다. 변화하는 시장 수요에 빠르게 적응하는 데 도움이 됩니다.\n\n3.4 원격 근무 지원\n직원들이 어디서나 업무를 수행할 수 있게 합니다. 특히 재택근무나 원격 근무 시나리오에 유용합니다. 직원들은 인터넷 연결만 있으면 회사의 IT 리소스에 접근할 수 있어 업무 효율성이 증가합니다.\n\n3.5 최신 기술 도입 용이\n클라우드 서비스 제공업체는 최신 기술과 보안 업데이트를 제공합니다. 기업은 추가 비용 없이 최첨단 IT 환경을 유지할 수 있으며, 기술적 경쟁력을 유지하는 데 크게 도움이 됩니다.\n\n\n2.4.4 핵심 포인트\n위의 잘 만들어진 프롬프트 예시를 통해 설명형 프롬프트의 핵심 포인트를 살펴보겠습니다:\n\n명확한 지시문 제공: 프롬프트에서 “작성 내용은 아래의 지시를 따라줘”라고 명시하여 무엇을 해야 하는지 명확하게 지시했습니다. 이렇게 지시문을 명확히 하면 AI가 정확한 방향으로 작업을 수행할 수 있습니다.\n대상 독자 명시: “보고서를 읽을 대상은 기술 개념을 이해하기 어려워하는 경영진이야”라고 명시했습니다. 이렇게 대상 독자를 구체적으로 명시하면 AI가 그에 맞는 수준과 톤으로 내용을 작성할 수 있습니다.\n형식과 분량 지정: “A4 이내의 분량으로 작성해줘”처럼 구체적인 형식과 분량을 지정했습니다. 이를 통해 원하는 길이와 형식의 결과를 얻을 수 있습니다.\n하위항목으로 상세 지침 제공: “중심 내용: 클라우드 컴퓨팅의 정의, 기본 원리, 주요 장점”과 “문장 스타일: 경영진이 이해할 수 있도록 하는 쉬운 예시나 비유를 사용”처럼 하위항목으로 더 구체적인 지침을 제공했습니다. 이렇게 하면 AI가 각 항목을 빠짐없이 처리할 수 있습니다.\n참고 텍스트를 추가하여 필요 정보 제공: “참고 텍스트를 사용해서 내용을 상세하게 써줘”라고 지시하고 실제 참고 텍스트를 제공했습니다. 이렇게 참고 자료를 추가하면 AI가 더 정확하고 상세한 내용을 생성할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#간단형",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#간단형",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.5 간단형",
    "text": "2.5 간단형\n\n2.5.1 특징\n\n짧고 간결한 요청을 하는 유형\n문장을 쓰지 않고 단어만, 구를 입력하는 유형\n굉장히 많은 사용 사례를 가진 유형\n최소한의 단어로 의도 전달\n빠른 답변이나 아이디어 생성에 적합\n\n\n\n2.5.2 예시\n\n“강남맛집”\n“5분 자기소개”\n“멋있는 강아지 이름”\n“비오는 날 카톡 멘트”\n“청렴 뜻”\n\n\n\n2.5.3 프롬프트 작성 실습\n실습 문제\n원격 근무 팀의 협업을 증진시킬 수 있는 창의적인 방법 8가지를 제시하는 프롬프트를 작성해보세요.\n잘 만들어진 프롬프트 예시\n원격 근무 팀의 협업을 증진시킬 수 있는 창의적인 방법 8가지 제시\n결과\n프롬프트를 실행하면 다음과 같은 협업 방법들이 제시됩니다:\n원격 근무 팀의 협업을 증진시키기 위한 창의적인 방법을 아래와 같이 제시합니다:\n\n1. 버추얼 오피스 도입\n버추얼 오피스 플랫폼을 활용하여 팀원들이 온라인에서 실제 오피스처럼 소통하고 협업할 수 있도록 합니다. 예를 들어, 팀원들이 가상 공간에서 자신의 아바타로 모이고, 회의실, 라운지 등 다양한 공간에서 자연스럽게 대화할 수 있습니다.\n\n2. 테마별 온라인 워크숍 개최\n팀원들의 전문성을 향상시키기 위해 다양한 주제의 온라인 워크숍을 정기적으로 개최합니다. 이 워크숍은 팀원들이 서로의 아이디어를 공유하고 협력하는 기회를 제공합니다.\n\n3. 공동 프로젝트 플랫폼 사용\nTrello, Asana, Notion 등의 협업 도구를 활용하여 팀원들이 실시간으로 프로젝트를 관리하고, 진행 상황을 공유할 수 있도록 합니다. 이를 통해 모든 팀원이 프로젝트의 진행 상황을 쉽게 파악할 수 있습니다.\n\n4. 정기적인 버추얼 커피 브레이크\n비공식적인 온라인 커피 브레이크 시간을 마련하여 팀원들이 업무와 관련 없는 대화를 나누며 친목을 도모할 수 있도록 합니다. 이는 팀원들 간의 유대감을 강화하는 데 도움이 됩니다.\n\n5. 온라인 게임 및 이벤트\n팀 빌딩을 위한 온라인 게임이나 이벤트를 정기적으로 개최합니다. 예를 들어, 퀴즈 대회, 온라인 방 탈출 게임, 가상 요리 클래스 등을 통해 팀원들이 즐겁게 협업할 수 있는 기회를 제공합니다.\n\n6. 비디오 기반 소통 장려\n이메일이나 채팅 대신 비디오 메시지나 비디오 콜을 통해 소통하는 문화를 장려합니다. 비디오를 통해 더욱 생생하게 의견을 전달하고, 팀원들 간의 이해를 높일 수 있습니다.\n\n7. 피드백 루프 구축\n주기적으로 팀원들 간의 피드백을 주고받는 시스템을 구축합니다. 예를 들어, 월간 피드백 세션을 통해 팀원들이 서로의 업무에 대해 건설적인 피드백을 주고받으며 성장할 수 있는 기회를 제공합니다.\n\n\n2.5.4 핵심 포인트\n위의 잘 만들어진 프롬프트 예시를 통해 간단형 프롬프트의 핵심 포인트를 살펴보겠습니다:\n\n간결하고 명확한 요청: 프롬프트에서 “원격 근무 팀의 협업을 증진시킬 수 있는 창의적인 방법 8가지 제시”처럼 최소한의 단어로 핵심 의도만 전달합니다. 문장을 쓰지 않고 단어나 구로 요청하는 것이 특징입니다.\n구체적인 개수나 형식 지정 가능: “8가지”처럼 구체적인 수치를 포함하면 더 정확한 결과를 얻을 수 있습니다. 예를 들어 “5분”, “10개” 등 구체적인 수치를 명시할 수 있습니다.\n빠른 응답에 적합: 복잡한 설명이나 상세한 지침 없이 빠르게 아이디어나 정보를 얻고 싶을 때 사용합니다. 간단형은 굉장히 많은 사용 사례를 가진 유형입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#프롬프트의-공통적인-특징",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/02-사용자언어읽기.html#프롬프트의-공통적인-특징",
    "title": "Part 2 — 사용자 언어 읽기(프롬프트 구조와 유형)",
    "section": "2.6 프롬프트의 공통적인 특징",
    "text": "2.6 프롬프트의 공통적인 특징\n효과적인 프롬프트는 다음 7가지 요소를 포함합니다:\n\nTask Description (작업 설명): 수행할 작업의 명확한 설명\nSpecific Instructions (구체적인 지침): 작업 수행 방법에 대한 상세 지침\nContent Elements (내용 요소): 포함해야 할 내용이나 정보\nAudience and Role Specification (청중 및 역할 지침): 대상 독자나 AI의 역할 정의\nLanguage and Style Guidelines (언어 및 스타일 지침): 사용할 언어와 문체\nFormat or Structure Guidelines (형식 또는 구조 지침): 출력 형식이나 구조\nWord or Character Limits (단어 또는 문자 제한): 분량 제한\n\n이러한 요소들을 조합하여 사용자의 의도를 정확히 전달하고, 원하는 결과를 얻을 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "문자의 발명 → 인류의 기록 정확도 향상\n인쇄술 → 지식 대중화\n전화·라디오·TV → 즉각적인 원격 소통\n인터넷·모바일 → 실시간 대화\n생성형 AI → 인간-기계 상호작용 고도화\n\n\n\n\n\nPromptus(나타내다/생산하다)에서 유래\n현재 의미: 행동을 유도하는 신호\nAI에서: 모델에게 답변을 생성하도록 입력하는 문장\n\n\n\n\n\n\n\n\n\n\n\n\n항목\n프롬프트\n프롬프트 엔지니어링\n\n\n\n\n목적\n응답 생성 요청\n원하는 형태로 정확한 응답을 얻기 위한 설계\n\n\n필요성\n기본 입력\n모델 작동 방식·언어적 맥락 이해 필요\n\n\n예시\n“용에 대한 이야기 해줘”\n“용감한 용이 마을을 구하는 5문장 이야기 생성해줘. 대화 포함.”"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#인간-컴퓨터-상호작용의-변화",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#인간-컴퓨터-상호작용의-변화",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "문자의 발명 → 인류의 기록 정확도 향상\n인쇄술 → 지식 대중화\n전화·라디오·TV → 즉각적인 원격 소통\n인터넷·모바일 → 실시간 대화\n생성형 AI → 인간-기계 상호작용 고도화"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트의-정의",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트의-정의",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "Promptus(나타내다/생산하다)에서 유래\n현재 의미: 행동을 유도하는 신호\nAI에서: 모델에게 답변을 생성하도록 입력하는 문장"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-vs-프롬프트-엔지니어링",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-vs-프롬프트-엔지니어링",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "항목\n프롬프트\n프롬프트 엔지니어링\n\n\n\n\n목적\n응답 생성 요청\n원하는 형태로 정확한 응답을 얻기 위한 설계\n\n\n필요성\n기본 입력\n모델 작동 방식·언어적 맥락 이해 필요\n\n\n예시\n“용에 대한 이야기 해줘”\n“용감한 용이 마을을 구하는 5문장 이야기 생성해줘. 대화 포함.”"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#개념",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#개념",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "2.1 2.1 개념",
    "text": "2.1 2.1 개념\n\n방대한 데이터 기반 사전학습 모델\nAutoregression: 이전 토큰 기반으로 다음 단어 예측\n기반 기술: Transformer 구조(셀프 어텐션)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#주요-모델-패밀리",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#주요-모델-패밀리",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "2.2 2.2 주요 모델 패밀리",
    "text": "2.2 2.2 주요 모델 패밀리\n\nOpenAI – GPT-4o: 멀티모달·실시간 처리\nAnthropic – Claude 3: 안전성 중점, 헌법적 AI\nGoogle – Gemini / BERT / T5: 다중 모달 처리"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#모델-튜닝-방식",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#모델-튜닝-방식",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "2.3 2.3 모델 튜닝 방식",
    "text": "2.3 2.3 모델 튜닝 방식\n\nFine-Tuning\nPrompt Engineering\nFeature-Tuning(특징 공간 해석 기반 제어)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#필수-4요소",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#필수-4요소",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "3.1 3.1 필수 4요소",
    "text": "3.1 3.1 필수 4요소\n\nInstruction: 어떤 작업을 해야 하는가\nContext: 배경 정보\nInput Data: 처리 대상 데이터\nOutput Indicator: 출력 형식 규정"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-타입-구조type-ad",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-타입-구조type-ad",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "3.2 3.2 프롬프트 타입 구조(Type A–D)",
    "text": "3.2 3.2 프롬프트 타입 구조(Type A–D)\n\nA: 지시문 + 출력문\nB: 지시문 + 맥락 + 출력문\nC: 지시문 + 맥락 + 예시 + 출력문\nD: 지시문 + 입력값 + 출력문\n\n예시와 함께 구조적으로 프롬프트를 설계하면 LLM이 더 안정적으로 원하는 출력 생성."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#대-원칙",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#대-원칙",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.1 4.1 4대 원칙",
    "text": "4.1 4.1 4대 원칙\n\n범용성: 다양한 상황에서도 동작\n목적 지향성: 특정 사용자 목적 반영\n일관성: 반복 실행해도 동일 품질\n경제성: API 비용 절감"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#scaffolding-prompting",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#scaffolding-prompting",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.2 4.2 Scaffolding Prompting",
    "text": "4.2 4.2 Scaffolding Prompting\n\n인간의 학습 이론(ZPD)을 적용\nUnderstand → Ideation → Analysis → Apply → Evaluate → Generalize\n단계적 사고구조 유도"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#핵심-5-tips",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#핵심-5-tips",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.3 4.3 핵심 5 Tips",
    "text": "4.3 4.3 핵심 5 Tips\n\n4.3.1 Tip1. 최신 모델 사용\n\n\n4.3.2 Tip2. 명확한 동사로 지시\n예: 분석해, 정리해, 요약해, 작성해 등\n\n\n4.3.3 Tip 3. 프롬프트를 구조화하자\n프롬프트를 구조화하면 LLM이 입력을 더 명확하게 해석할 수 있고,\n작업 목적·규칙·입력 데이터·출력 형식을 혼동 없이 처리할 수 있다.\n구조화를 위한 핵심 기법은 Markdown, 코드 프롬프트, XML 태그이다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#markdown-구조화-headers-lists-emphasis",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#markdown-구조화-headers-lists-emphasis",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.4 3.1 Markdown 구조화 (Headers, Lists, Emphasis)",
    "text": "4.4 3.1 Markdown 구조화 (Headers, Lists, Emphasis)\nMarkdown은 LLM이 가장 잘 해석하는 구조적 문법 중 하나이다.\n섹션을 명확히 나누고, 각 부분을 시각적으로 구분해줌으로써\n모델의 오류를 줄이고 반복 작업 시 유지보수성이 높아진다.\n\n4.4.1 1) 헤더(Headers) 활용\n헤더는 프롬프트의 구성을 크게 나누는 데 사용한다.\n# 역할(Role)\n너는 스포츠 기사 분석 엔진이다.\n\n## 작업(Task)\n아래 텍스트에서 장소(도시, 국가, 축구 클럽)를 모두 추출해라.\n\n## 출력 형식(Format)\n- \"장소: A, B, C\" 형태의 한 줄 출력\n- 설명 금지\n\n## 텍스트(Text)\n{{TEXT}}\n\n\n4.4.2 2) 목록(Lists) 활용\n절차, 규칙, 조건을 번호 목록으로 정리하면 LLM이 하나씩 적용하기 쉽다.\n다음 규칙을 따라 작업을 수행해라.\n1. 사람 이름은 제외한다.\n2. 축구 클럽은 장소로 포함한다.\n3. 결과는 쉼표로 구분한다.\n4. 설명 문장은 쓰지 않는다.\n\n\n4.4.3 3) 강조(Emphasis) 사용\n중요 조건을 굵게 또는 기울임으로 표시해 모델의 우선순위를 높인다.\n- 반드시 **한국어**로 작성하라.\n- 출력은 **한 줄**로만 작성한다.\n- *설명 문장은 포함하지 않는다.*"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#코드-프롬프트code-prompting",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#코드-프롬프트code-prompting",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.5 3.2 코드 프롬프트(Code Prompting)",
    "text": "4.5 3.2 코드 프롬프트(Code Prompting)\n자연어 대신 코드 형식으로 의도를 명세하면 애매함이 줄어들고 LLM이 구조적으로 이해해 할루시네이션이 감소한다.\n# 역할 정의\nrole = \"정보 추출 엔진\"\n\n# 작업 정의\ntask = \"텍스트에서 '장소명'을 추출해 정렬된 리스트로 반환\"\n\n# 출력 형식 정의\noutput = {\n    \"places\": [\"string\"]\n}\n코드 방식의 장점:\n\n논리 구조가 명확하게 표현됨\n조건과 출력 형식이 혼동되지 않음\n정형화된 포맷을 자주 재사용 가능"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#xml-태그-기반-구조화-claude-최적화",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#xml-태그-기반-구조화-claude-최적화",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.6 3.3 XML 태그 기반 구조화 (Claude 최적화)",
    "text": "4.6 3.3 XML 태그 기반 구조화 (Claude 최적화)\nXML 태그는 텍스트 내 특정 영역을 명확히 구분해 Claude뿐 아니라 대부분의 LLM에서 구조적 파싱을 돕는 강력한 방식이다.\n\n4.6.1 1) 태그로 영역 지정\n&lt;email&gt;\nShow up at 6AM because I say so.\n&lt;/email&gt;\n\nMake this email more polite.\nLLM은 &lt;email&gt;...&lt;/email&gt; 내부 텍스트만 변환하면 된다는 것을 명확히 인식한다.\n\n\n4.6.2 2) 데이터 템플릿 예시\n&lt;animal&gt;{{ANIMAL}}&lt;/animal&gt;\n입력만 바꿔 여러 데이터를 동일한 템플릿에 적용할 수 있다.\n예:\n&lt;animal&gt;Cow&lt;/animal&gt;\n&lt;animal&gt;Dog&lt;/animal&gt;\n&lt;animal&gt;Seal&lt;/animal&gt;\n장점:\n\n구역별 의미가 명확\n반복 작업에서 수정이 쉽고 안정적\n모델이 태그 단위로 파싱하기 때문에 오류 감소"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#구조화-기법이-필요한-이유",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#구조화-기법이-필요한-이유",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.7 3.4 구조화 기법이 필요한 이유",
    "text": "4.7 3.4 구조화 기법이 필요한 이유\n\n명령어, 입력 데이터, 출력 형식을 명확히 구분해 모델의 해석 오류를 줄인다.\n규칙을 시각적으로 구분하여 작업 수행 품질의 일관성과 재현성을 높인다.\n재사용 가능한 템플릿이 되어 프롬프트 제작 비용과 시간 절감에 기여한다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#구조화-적용-사례-장소명-추출",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#구조화-적용-사례-장소명-추출",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "4.8 3.5 구조화 적용 사례 (장소명 추출)",
    "text": "4.8 3.5 구조화 적용 사례 (장소명 추출)\n# 역할(Role)\n너는 텍스트에서 장소명을 추출하는 엔진이다.\n\n## 작업(Task)\n텍스트 내 도시, 국가, 축구 클럽 이름을 모두 \"장소\"로 간주하고 추출해라.\n\n## 출력 형식(Format)\n- 한 줄로만 출력한다.\n- 형식: `장소: A, B, C`\n- 설명 문장 금지.\n\n## 텍스트(Text)\n음바페는 26일 쿠프 드 프랑스 프랑스컵 올랭피크 리옹과의\n결승전에서 풀타임을 뛰며 2-1로 팀 승리를 도왔다.\n3년 만에 대회 정상에 오른 PSG는 역대 최다 15회 우승으로\n2위 마르세유(10회)를 멀찍이 따돌렸다.\n음바페는 이날 PSG에서의 커리어를 마무리했다.\n\n4.8.1 Tip4. 단문 중심\n\n한국어는 짧고 명확하게 작성\n톤·형식·분량을 구체적으로 지시\n\n\n\n4.8.2 Tip5. 하지 말 것(X) → 할 것(O)\n\n부정 지시보다 긍정적 행동 지시가 안정적"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#llm-한계",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#llm-한계",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "5.1 5.1 LLM 한계",
    "text": "5.1 5.1 LLM 한계\n\n할루시네이션\n편향성\n근거 없는 출처 생성\n수학 및 추론 한계\nPrompt Injection(프롬프트 해킹)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-엔지니어링이-필요한-이유",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/01-Prompt-Structure.html#프롬프트-엔지니어링이-필요한-이유",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "5.2 5.2 프롬프트 엔지니어링이 필요한 이유",
    "text": "5.2 5.2 프롬프트 엔지니어링이 필요한 이유\n\n정확성 향상\n오류 및 편향 감소\n모델 행동 제어\n사용자 경험 개선\n비용 절감"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "이 장 에서는 연역적/귀납적 방법론을 이용한 프롬프트 기획 사례를 소개합니다.\n핵심으로 인식하고 가야할 부분은 아래와 같습니다.\n\n연역적/귀납적 방법론을 이용해 문제 정의\n아이디에이션 노트 작성 및 이를 활용한 프롬프트 기획"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#예시-왜-sosi-유형이-다른-유형보다-많을까",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#예시-왜-sosi-유형이-다른-유형보다-많을까",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.1 예시: 왜 SO/SI 유형이 다른 유형보다 많을까?",
    "text": "2.1 예시: 왜 SO/SI 유형이 다른 유형보다 많을까?\n연역적 접근방식\n\n가설1: 사용자는 생성형 AI를 어려워 한다. 사용할 줄 모른다.\n가설2: 사용자는 질문을 귀찮아 하거나, 프롬프트를 작성하는 것을 귀찮아 한다.\n가설3: 사용자의 현재 생성형 AI 사용중 가장 큰 동기는 정보 검색이다.\n\n귀납적 접근방식\n\n관찰1: 사용자 발화 데이터를 보니, 정보 검색 유형이 가장 많았다.\n관찰2: 사용자가 정보 검색을 위해 쓰는 프롬프트 길이가 짧고, 좋은 답변을 받기에 내용이 충분하지 않았다.\n관찰3: 사용자가 AI와 대화를 한 번 하고 서비스를 이탈했다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#귀납적-방식의-장단점",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#귀납적-방식의-장단점",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.2 귀납적 방식의 장단점",
    "text": "2.2 귀납적 방식의 장단점\n\n2.2.1 장점\n\n실사용 데이터를 기반으로 패턴과 트렌드를 파악할 수 있다.\n실제 사용자 발화·행동 데이터를 분석하기 때문에, 사용자의 진짜 요구와 사용 맥락을 더 잘 반영할 수 있다.\n사용자 행동과 요구를 직접적으로 반영할 수 있다.\n가설이 아니라 실제 데이터를 기반으로 하기 때문에, 기능 개선과 프롬프트 튜닝의 방향성이 비교적 명확하다.\n피드백을 통해 기능을 지속적으로 개선·발전시킬 수 있다.\n사용자 피드백을 정량·정성적으로 분석하여, 어떤 프롬프트가 효과적인지, 어디서 불편함을 느끼는지 파악하고 개선할 수 있다.\n\n\n\n2.2.2 단점\n\n데이터 수집과 분석에 많은 시간과 비용이 든다.\n충분한 데이터가 쌓이기 전까지는 의미 있는 인사이트를 얻기 어렵고, 분석 과정에도 리소스가 필요하다.\n데이터 편향 가능성이 있다.\n특정 사용자 그룹, 특정 시점·상황에 치우친 데이터일 경우, 전체 사용자에 대한 잘못된 결론을 내릴 수 있다.\n과거 데이터 기반이라 새로운 트렌드 반영이 늦을 수 있다.\n이미 수집된 데이터에 기반한 예측은, 새로운 사용 패턴이나 트렌드 변화를 완전히 포착하지 못할 수 있다.\n\n\n\n2.2.3 귀납적 방법을 이용한 실무 적용 예시\n\n사용자 데이터 분석: 어떤 키워드·질문이 자주 등장하는지 로그 데이터를 수집·분석하여, 해당 결과로 프롬프트 유형별 특징을 파악하고 기획 진행\n피드백 반영: 별점, 코멘트, 실패 사례(재질문, 이탈 등)를 분석하여 프롬프트 구조를 개선하고 새로운 기능·옵션을 추가합니다.\nA/B 테스트: 서로 다른 프롬프트 버전을 동시에 운영해 전환율, 재방문율, 만족도 등을 비교하여 가장 효과적인 프롬프트 방식을 선정합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#연역적-방식의-장단점",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#연역적-방식의-장단점",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.3 연역적 방식의 장단점",
    "text": "2.3 연역적 방식의 장단점\n\n2.3.1 장점\n\n논리적인 접근을 통해 체계적으로 문제를 해결할 수 있다.\n이론과 원리를 기반으로 가설을 세우고 전개하기 때문에, 문제 정의와 해결 과정이 구조화되기 쉽다.\n명확한 가정을 설정하고 이에 따라 기능을 기획할 수 있다.\n“사용자는 어떤 정보를 필요로 한다”와 같은 가정을 선행 정의하고, 그에 맞춰 프롬프트 흐름과 기능을 설계한다.\n시간 효율적이다.\n충분한 데이터가 없더라도, 이론적 모델과 도메인 지식을 활용해 비교적 빠르게 기능을 설계·초기 버전을 출시할 수 있다.\n\n\n\n2.3.2 단점\n\n실제와의 괴리가 발생할 수 있다.\n이론이 실제 사용자 행동과 일치하지 않을 수 있어, 설계한 프롬프트가 기대만큼 사용되지 않거나 성과가 낮을 수 있다.\n제한된 유연성.\n초기 가정과 이론에 지나치게 의존하면, 예상 밖의 사용자 행동이나 새로운 요구에 유연하게 대응하기 어렵다.\n사용자 피드백 반영이 늦어질 수 있다.\n이론 중심으로 설계된 경우, 실제 사용자 피드백과 사용 로그를 설계에 적극적으로 녹여내는 데 소극적이거나 시점이 늦어질 수 있다.\n\n\n\n2.3.3 연역적 방법을 이용한 실무 적용 예시\n\n이론적 모델 수립: 사용자가 어떤 정보를 필요로 할지 서비스 목적, 도메인 지식, 기존 연구를 기반으로 이론적 모델을 만들고 가설을 세웁니다.\n기능 설계: 설정한 가설을 바탕으로 프롬프트 시나리오, 질문 구조, 응답 형식을 설계합니다.\n규칙 기반 시스템 구축: 특정 규칙이나 알고리즘(조건문, 템플릿, 추천 규칙 등)에 따라 프롬프트를 제공하는 시스템을 설계·구현합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#연역적귀납적-방법론을-사용할-때-가져야-할-프롬프트-기획-마인드",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#연역적귀납적-방법론을-사용할-때-가져야-할-프롬프트-기획-마인드",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.4 연역적/귀납적 방법론을 사용할 때 가져야 할 프롬프트 기획 마인드",
    "text": "2.4 연역적/귀납적 방법론을 사용할 때 가져야 할 프롬프트 기획 마인드\n프롬프트를 기획할 때 “사용자가 이걸 좋아할 것 같다/싫어할 것 같다”는 막연한 감(감각)에만 의존해 아이디에이션하지 않는 것이 중요합니다. 연역적·귀납적 방법론처럼 어떤 이론적 기반과 분명한 원리에 의거해 가설을 세우고, 서로 다른 접근법을 시도·혼합해 보면서 설계해야 합니다.\n즉, 프롬프트 기획도 원리·원칙을 명확히 세운 위에서 실험하고 개선해 가는 체계적인 기획 작업이라는 마인드를 가지는 것이 중요합니다.\n다음으로는 위의 방법론을 활용하여 실제 프롬프트 기획 사례 3가지를 소개합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-1-프롬프트-질문-생성기",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-1-프롬프트-질문-생성기",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "3.1 사례 1: 프롬프트 질문 생성기",
    "text": "3.1 사례 1: 프롬프트 질문 생성기\n\n3.1.1 사용자 문제 정의\n\n현황 분석: 사용자의 서비스 이용 시간이 전반적으로 짧다.\n귀납적 접근으로 본 문제: 대다수 사용자가 한 번만 질문하고 이탈하며, 멀티턴 사용자의 비율이 매우 낮다.\n\n→ 사용자가 “한 번 묻고 끝내는” 패턴에 머물지 않고, 자연스럽게 다음 대화로 이어지도록 유도하는 장치가 필요합니다.\n\n\n3.1.2 해결 방향\n\n멀티턴을 유도하는 인터페이스 설계\n사용자의 첫 질문 이후, 바로 나갈 수 있는 구조가 아니라 후속 질문을 눌러보게 만드는 UI/UX를 설계합니다.\n사용자 질문 기반 후속 질문 생성\n사용자가 입력한 질문을 분석해, 다음에 궁금해 할 만한 관련 질문을 제안합니다.\n\n위의 과정을 정리했다면, 바로 개발에 들어가기보다 아이디어와 구현 노트를 충분히 확장해 보는 단계가 한 번 더 필요합니다.\n\n\n3.1.3 프롬프트 질문 생성기 구현 아이디어 / 구현 노트\n이렇게 구체적으로 아이디어와 구현 노트를 적어 두는 이유는, 단순히 “문제를 정의하고 해결책을 하나 정하는 것”을 넘어서 여러 가지 아이디에이션을 체계적으로 쌓아가기 위함입니다.\n프롬프트는 자연어 기반이라, 누가 어떤 맥락에서 쓰느냐에 따라 유용해지기도 하고 전혀 유용하지 않을 수도 있습니다.\n따라서 충분한 실험과 탐색이 필요하다는 점을 항상 염두에 두고, 그 과정에서 떠오르는 아이디어를 적극적으로 기록·확장해 나가는 것이 중요합니다.\n\nStatic → Dynamic Question Generator\n정적인 인터페이스에서 다이나믹하게 계속 변경되는 질문 생성기를 만들어 봐야겠다!\n\n기존: 정적인 FAQ 리스트만 제공\n\n변경: 사용자의 질문 내용을 바탕으로, 바로 이어질 수 있는 후속 질문을 동적으로 생성\n\nFAQs 자동 생성 (Prediction Probability 기반)\n단순히 몇개의 사용자 질문을 토대로 세가지 후속 질문을 추천해줘 가 아니라. 확률 예측 모델을 사용해 프롬프트를 자동 생성해 봐야겠다!(?? 저 이론은 무엇인지 숙지 필요함)\n\n유사한 사용자들이 많이 물어본 질문을 확률적으로 예측해, “자주 묻는 다음 질문” 형태로 노출\n\n행동 경제학·넛지(Nudge) 이론 적용\n\n정의: 사람들의 선택 환경을 살짝 조정해, 강제하지 않고도 더 나은 선택을 하도록 유도하는 행동경제학 이론이다.\n\n핵심 개념: 선택의 자유는 그대로 두되, 사람들이 항상 이성적으로 선택하지 않고 일정한 패턴의 비합리적 선택을 반복한다는 점(예측 가능한 비합리성)을 활용해, 메뉴 구성·표현 방식·기본값 설정 등을 조금만 조정해도 더 이로운 선택을 하도록 돕는다는 것이다.\n\n연구 배경: Thaler & Sunstein(2008)의 『넛지(Nudge)』에서 대중적으로 널리 알려졌으며, 정책·서비스 설계 전반에 응용되고 있다.\n\n프롬프트 적용: 프롬프트 후속 질문에 넛지 요소를 심어, 버튼 문구·배치·레이블 등을 통해 사용자의 선택 자유는 보장하면서도 “도움이 되는 다음 질문”을 자연스럽게 클릭하도록 유도합니다. 예를 들어, 단순히 “다음 질문 보기” 대신 “사람들이 이 다음에 많이 물어본 질문 보기”와 같이 남들의 선택을 따르려는 경향(예측 가능한 비합리성)을 활용한 문구를 사용할 수 있습니다.\n\n마음이론(Theory of Mind) 적용\n\n정의: 다른 사람이 지금 무엇을 생각하고, 무엇을 믿고, 무엇을 원하는지 같은 보이지 않는 마음 상태를 추론해 보는 능력이다.\n\n핵심 개념: 타인이 나와 다른 생각·믿음·욕구·의도를 가질 수 있다고 전제하고, “저 사람 입장에서라면 어떻게 느낄까, 무엇을 기대할까?”를 상상해 보는 데 초점을 둔다.\n\n인지 배경: 인간의 사고는 보통 빠른 사고(직관, 자동적 판단)와 느린 사고(논리적 추론)로 나뉘는데, 대부분의 사용자는 서비스 이용 중에 느린 사고를 충분히 하지 못한다. 다시 말해, 스스로 복잡하게 추론해서 의도를 상세히 설명하지 않으므로, 그 빈칸을 기획자가 마음이론을 통해 채워줘야 한다.\n\n연구 배경: Premack & Woodruff(1978)의 「침팬지는 마음을 가지고 있는가?」, Wimmer & Perner(1983)의 ‘틀린 믿음 과제’, Baron-Cohen 등(1985)의 자폐와 마음이론 연구 등을 통해 심리학에서 중요한 개념으로 자리 잡았다.\n\n프롬프트 적용: 사용자의 질문 문장 그대로만 받아들이지 말고, 그 뒤에 있는 상황·목표·감정을 함께 추론합니다. “이 사용자는 빠른 사고 상태에서 대충 적었지만, 실제로는 무엇을 얻고 싶어 할까? 지금 다음으로 무엇을 궁금해할까?”를 먼저 생각한 뒤, 그 답을 기준으로 후속 질문과 추천 옵션을 설계합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-2-프롬프트-자동-완성기",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-2-프롬프트-자동-완성기",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "3.2 사례 2: 프롬프트 자동 완성기",
    "text": "3.2 사례 2: 프롬프트 자동 완성기\n\n3.2.1 사용자 문제 정의\n아무래도 구글검색이나, 네이버 검색에 문장을 쓰면서 검색을 하는 사용자들의 비율이 적은것 같다.\n\n현황 분석: 사용자가 입력하는 프롬프트가 전반적으로 구체적이지 못하고, 정보가 불완전하다.\n\n귀납적 접근으로 본 문제: 현재 제품 안에 사용자가 프롬프트를 잘 쓸 수 있도록 안내하거나 보조해 주는 장치가 거의 없다.\n\n→ 사용자가 “무엇을 어떻게 써야 할지 모르는 상태”에서 막막함을 느끼지 않도록, 프롬프트 작성 자체를 도와주는 도구가 필요합니다.\n\n\n3.2.2 해결 방향\n\n프롬프트 자동 완성기 설계\nAI와의 대화를 보다 원활하게 이어 갈 수 있도록, 사용자가 몇 단어만 입력해도 맥락에 맞는 프롬프트 후보를 자동으로 제안해 줍니다.\n사용 가치와 효율성 극대화\n사용자가 매번 처음부터 길게 프롬프트를 고민하지 않고, 자동 제안된 문장을 선택·수정하는 방식으로 시간과 인지 부담을 줄입니다.\n\n\n\n3.2.3 Claude Prompt Generator 예시\nhttps://platform.claude.com/dashboard에서 확인할 수 있는, Claude가 제공하는 프롬프트 자동 완성 기능입니다.\n  \n이 기능을 그대로 가져와 제품화하기는 어렵지만, 결과 화면만으로도 어떤 방식으로 자동 완성을 설계할지에 대한 좋은 레퍼런스·밑그림을 얻을 수 있습니다.\n다만, 현재 제공되는 기능은 영어 중심이라 한국어 프롬프트에 그대로 적용하기에는 한계가 있다는 점을 함께 고려해야 합니다.\n\n\n3.2.4 연역적 방법을 이용한 프롬프트 자동 완성기 기획\n연역적 방식에서는 먼저 “프롬프트 자동 완성기가 도입되면 사용자가 어떻게 행동할 것인가?”에 대한 전제를 설정하고, 그에 따라 기능과 지표를 설계합니다.\n전제(가정) 설정 - 가설 1: 프롬프트 자동 완성기를 통해 사용자가 더 쉽게 질문을 만들 수 있으면, 사용자 만족도가 증가할 것이다.\n- 가설 2: 질문을 만드는 부담이 줄어들면, 사용자가 서비스를 떠나지 않고 더 오래, 더 자주 이용하게 되어 리텐션이 올라갈 것이다.\n- 가설 3: 자동 완성 기능을 통해 기본적인 질문 작성이 편해지면, 사용자는 점점 더 복잡하고 정교한 질문을 시도해 볼 것이다.\n- 가설 4: 위 과정이 반복되면, 사용자는 “이 서비스가 있어야만 문제를 잘 풀 수 있다”고 느끼며 서비스에 대한 의존도도 높아질 것이다.\n이러한 가정을 바탕으로, 어떤 프롬프트 자동 완성 기능을 설계해야 할지, 또 어떤 지표(만족도, 재방문율, 질문 복잡도 등)를 트래킹해야 할지 연역적으로 기획을 전개할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-3-시스템-프롬프트-개선-작업",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/02-프롬프트_기획_사례.html#사례-3-시스템-프롬프트-개선-작업",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "3.3 사례 3: 시스템 프롬프트 개선 작업",
    "text": "3.3 사례 3: 시스템 프롬프트 개선 작업\n\n3.3.1 사용자 문제 정의\n\n현황 분석: 특정 세그먼트(예: 정보 검색형, 단발성 사용자 등)의 비율이 지나치게 높고, 다른 세그먼트는 충분히 활성화되지 못하고 있다.\n\n귀납적 접근으로 본 문제: 현 시스템 프롬프트에 다양한 문제가 존재한다.\n\n→ 특정 세그먼트에 치우치지 않고, 여러 유형의 사용자가 골고루 자신의 목적을 달성할 수 있는 범용 시스템 프롬프트가 필요하다.\n\n\n3.3.2 해결 방향\n\n범용적 목적의 시스템 프롬프트 설계\n한두 세그먼트만을 위한 최적화가 아니라, 모두가 기본적으로 “쓸 만한” 공통 베이스를 가진 시스템 프롬프트를 제작합니다.\n\n\n\n3.3.3 프롬프트 구현 아이디어 / 구현 노트\n현재 시스템 프롬프트에는 다양한 문제가 한꺼번에 섞여 있었기 때문에, 먼저 글로벌/로컬 관점에서 문제를 분리·정리한 뒤, 각 관점에 맞는 시스템 프롬프트를 따로 설계·개선해 나갔습니다.\n글로벌 관점: 상위 레벨 시스템 프롬프트 설계\n\n문제 1: 시스템 프롬프트가 너무 일반적이라, 누구나 쓸 법한 평이한 답변만 나오고 서비스만의 색이 드러나지 않는다.\n\n문제 2: 사용자에게 “이 서비스만의 특징 있는 답변”을 제공하지 못한다.\n\n문제 3: 회사의 색깔과 정체성이 답변에 충분히 반영되지 않는다.\n\n로컬 관점: 개별 서비스용 시스템 프롬프트 설계\n\n문제 1: 실시간 정보(날짜, 날씨, 시점 의존 정보 등)를 제공하지 못해 실제 상황과 어긋나는 답변이 나올 수 있다.\n\n해결 아이디어: 검색엔진·외부 API 연동을 통해, 필요한 범위 내에서 실시간 정보를 보완합니다.\n\n\n문제 2: AI가 생성하는 답변 텍스트의 가독성이 전반적으로 떨어진다.\n\n해결 아이디어: 마크다운, 리스트, 표 등 포매팅 활용을 시스템 프롬프트에 명시해 읽기 쉬운 형식을 기본값으로 만듭니다.\n\n\n문제 3: 한국어 답변이 번역체처럼 부자연스럽게 느껴지는 경우가 많다.\n\n문제 4: 부정확한 정보나 거짓 정보에 어떻게 대응할지에 대한 명확한 지침이 없다.\n\n해결 방법 요약\n\nHeavy User 20% 먼저 관찰하기 (파레토 법칙 20:80)\n전체 사용자 중 상위 20% Heavy User가 어떤 목적과 패턴으로 AI를 사용하는지 먼저 분석하고, 그 행동을 기준으로 글로벌/로컬 시스템 프롬프트의 우선순위를 정합니다.\n사용자 경험 최적화\n답변 속도를 개선하거나, 지연이 발생할 때는 “생각 중입니다”, “추가 정보를 정리하고 있습니다”처럼 진행 상태를 안내하는 문구를 시스템 프롬프트에 포함해 불필요한 이탈을 줄입니다.\n다양한 사용자 요구 수용\nHeavy User 20%에 집중하되, 정보 검색·정리·아이디어 발산·학습·코칭 등 다양한 사용 목적 카테고리를 정의하고, 각 카테고리에 맞는 예시 프롬프트와 응답 스타일을 시스템 프롬프트에 녹여 여러 유형의 사용자가 자신의 목적에 맞게 활용할 수 있도록 합니다.\n\n\n\n3.3.4 느낀점\n\n연역적/귀납적 방법론을 통해 현 상황을 논리적으로 분석하고 문제를 체계적으로 정의하는 방법을 배웠습니다. 이를 실무에 적용하면 더욱 체계적인 프롬프트 기획이 가능할 것입니다.\n효과적인 아이디에이션을 위해서는 행동심리학, 언어학 등 다양한 학문적 지식이 필요하다는 점을 깨달았습니다. 프롬프트 엔지니어링은 단순한 기술 작업을 넘어서 언어학, 심리학 등 다양한 분야의 이해가 중요한 다학제적 영역임을 알게 되었습니다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "",
    "text": "SelfQueryRetriever는 자연어 질의를 구조화된 쿼리로 자동 변환하여 검색을 수행하는 지능형 검색 도구이다. 일반적인 벡터 검색과 달리, 메타데이터 필터링과 의미적 검색을 동시에 활용한다.\n\n\n\n\n질의 분석: 사용자의 자연어 질의를 LLM이 분석\n\n쿼리 구조화: LLM이 Query-constructing LLM chain을 사용해 구조화된 질의 생성\n\n필터 추출: 질의에서 메타데이터 필터 조건 추출\n\n검색 수행: 구조화된 질의를 벡터 저장소(VectorStore)에 적용하여 검색\n\n\n\n\n\n\n\n정확도 향상: 메타데이터 필터링으로 검색 결과의 정밀도 증가\n\n복합 조건 처리: 여러 메타데이터 조건을 조합한 검색 가능\n\n자연어 인터페이스: 사용자가 복잡한 쿼리 문법을 몰라도 자연어로 질의 가능\n\n테이블 데이터 활용: 구조화된 테이블 형식 데이터 처리에 효과적\n\n\n\n\n\n메타데이터 의존성: 모든 문서에 적절한 메타데이터가 필요\n\n초기 설정 비용: AttributeInfo 정의 등 초기 구성 작업 필요\n\nLLM 호출 오버헤드: 질의마다 LLM을 통한 쿼리 변환 과정 필요\n\n\n\n\n\n대기업이나 데이터가 성숙도가 높은 기업에서는 거의 모든 데이터가 정형이기 때문에 SQL쿼리 또는 DBMS 하에 관리가 될 가능성이 높다.\n이런 table형식으로 관리되는 데이터에 이 SelfQueryRetriever는 유용하게 활용될 수 있다.\n\n전자상거래 제품 검색 (가격, 카테고리, 평점 등 필터링)\n\n문서 관리 시스템 (작성일, 저자, 문서 유형 등 조건 검색)\n\n콘텐츠 추천 시스템 (장르, 연도, 평점 기반 추천)\n\n참고 자료\n\nLangChain 공식 Self-Query Retriever 문서"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#selfqueryretriever-개요",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#selfqueryretriever-개요",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "",
    "text": "SelfQueryRetriever는 자연어 질의를 구조화된 쿼리로 자동 변환하여 검색을 수행하는 지능형 검색 도구이다. 일반적인 벡터 검색과 달리, 메타데이터 필터링과 의미적 검색을 동시에 활용한다.\n\n\n\n\n질의 분석: 사용자의 자연어 질의를 LLM이 분석\n\n쿼리 구조화: LLM이 Query-constructing LLM chain을 사용해 구조화된 질의 생성\n\n필터 추출: 질의에서 메타데이터 필터 조건 추출\n\n검색 수행: 구조화된 질의를 벡터 저장소(VectorStore)에 적용하여 검색\n\n\n\n\n\n\n\n정확도 향상: 메타데이터 필터링으로 검색 결과의 정밀도 증가\n\n복합 조건 처리: 여러 메타데이터 조건을 조합한 검색 가능\n\n자연어 인터페이스: 사용자가 복잡한 쿼리 문법을 몰라도 자연어로 질의 가능\n\n테이블 데이터 활용: 구조화된 테이블 형식 데이터 처리에 효과적\n\n\n\n\n\n메타데이터 의존성: 모든 문서에 적절한 메타데이터가 필요\n\n초기 설정 비용: AttributeInfo 정의 등 초기 구성 작업 필요\n\nLLM 호출 오버헤드: 질의마다 LLM을 통한 쿼리 변환 과정 필요\n\n\n\n\n\n대기업이나 데이터가 성숙도가 높은 기업에서는 거의 모든 데이터가 정형이기 때문에 SQL쿼리 또는 DBMS 하에 관리가 될 가능성이 높다.\n이런 table형식으로 관리되는 데이터에 이 SelfQueryRetriever는 유용하게 활용될 수 있다.\n\n전자상거래 제품 검색 (가격, 카테고리, 평점 등 필터링)\n\n문서 관리 시스템 (작성일, 저자, 문서 유형 등 조건 검색)\n\n콘텐츠 추천 시스템 (장르, 연도, 평점 기반 추천)\n\n참고 자료\n\nLangChain 공식 Self-Query Retriever 문서"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#환경-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#환경-설정",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "2 환경 설정",
    "text": "2 환경 설정\n\n2.1 API 키 로드\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일  \nfrom dotenv import load_dotenv  \n\n# API 키 정보 로드  \nload_dotenv()  \n\n\n\n\n2.2 LangSmith 추적 설정\nLangSmith를 활용하면 LLM 호출 과정과 쿼리 변환 과정을 시각적으로 추적할 수 있다.\n\n\n코드\n# LangSmith 추적을 설정 (https://smith.langchain.com)  \n# !pip install langchain-teddynote  \nfrom langchain_teddynote import logging  \n\n# 프로젝트 이름을 입력  \nlogging.langsmith(\"CH10-Retriever\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#샘플-데이터-생성",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#샘플-데이터-생성",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "3 샘플 데이터 생성",
    "text": "3 샘플 데이터 생성\n화장품 상품의 설명과 메타데이터를 기반으로 유사도 검색이 가능한 벡터 저장소를 구축한다. 각 문서는 제품 설명(page_content)과 구조화된 메타데이터(연도, 카테고리, 평점)를 포함한다.\n메타데이터 스키마\n\nyear (정수): 제품 출시 연도\n\ncategory (문자열): 제품 카테고리 (스킨케어, 메이크업, 클렌징, 선케어)\n\nuser_rating (실수): 사용자 평점 (1.0 ~ 5.0)\n\n\n\n코드\nfrom langchain_chroma import Chroma  \nfrom langchain_core.documents import Document  \nfrom langchain_openai import OpenAIEmbeddings  \n\n# 화장품 상품의 설명과 메타데이터 생성  \ndocs = [  \n    Document(  \n        page_content=\"수분 가득한 히알루론산 세럼으로 피부 속 깊은 곳까지 수분을 공급합니다.\",  \n        metadata={\"year\": 2024, \"category\": \"스킨케어\", \"user_rating\": 4.7},  \n    ),  \n    Document(  \n        page_content=\"24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.\",  \n        metadata={\"year\": 2023, \"category\": \"메이크업\", \"user_rating\": 4.5},  \n    ),  \n    Document(  \n        page_content=\"식물성 성분으로 만든 저자극 클렌징 오일, 메이크업과 노폐물을 부드럽게 제거합니다.\",  \n        metadata={\"year\": 2023, \"category\": \"클렌징\", \"user_rating\": 4.8},  \n    ),  \n    Document(  \n        page_content=\"비타민 C 함유 브라이트닝 크림, 칙칙한 피부톤을 환하게 밝혀줍니다.\",  \n        metadata={\"year\": 2023, \"category\": \"스킨케어\", \"user_rating\": 4.6},  \n    ),  \n    Document(  \n        page_content=\"롱래스팅 립스틱, 선명한 발색과 촉촉한 사용감으로 하루종일 편안하게 사용 가능합니다.\",  \n        metadata={\"year\": 2024, \"category\": \"메이크업\", \"user_rating\": 4.4},  \n    ),  \n    Document(  \n        page_content=\"자외선 차단 기능이 있는 톤업 선크림, SPF50+/PA++++ 높은 자외선 차단 지수로 피부를 보호합니다.\",  \n        metadata={\"year\": 2024, \"category\": \"선케어\", \"user_rating\": 4.9},  \n    ),  \n]  \n\n# 벡터 저장소 생성  \nvectorstore = Chroma.from_documents(  \n    docs,   \n    OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n)  \n\n\n\n3.1 메타데이터 필드 정의\nAttributeInfo 클래스를 사용하여 화장품 메타데이터 필드에 대한 정보를 정의한다. 범주형 데이터는 모든 sample space를 적어줘야하고 필요시 업데이트를 주기적으로 해줘야한다.\n\n카테고리(category): 문자열 타입, 화장품의 카테고리를 나타내며 [‘스킨케어’, ‘메이크업’, ‘클렌징’, ‘선케어’] 중 하나의 값을 가진다.\n\n연도(year): 정수 타입, 화장품이 출시된 연도를 나타낸다.\n\n사용자 평점(user_rating): 실수 타입, 1-5 범위의 사용자 평점을 나타낸다.\n\n사실, 이 부분이 SelfQueryRetriever를 좀 더 규모있게 쓰기위해선 굉장히 번거롭고 데이터 모델링 들어가야하는 부분이라 사실 난이도가 높다. 데이터 모델링이 되기 위해선 주기적으로 서비스가 제공이되는 기획이 구체적으로 만들어져야하고 이를 바탕으로 데이터 모델링이 수행되어야 하기 때문에 상당한 자원이 투입되어야한다.\nAttributeInfo 구성 요소\nAttributeInfo 클래스를 사용하여 각 메타데이터 필드의 타입, 설명, 가능한 값을 정의한다. 이 정보는 LLM이 자연어 질의를 구조화된 쿼리로 변환할 때 참조하므로 명확하고 구체적으로 작성해야 한다.\n\nname: 메타데이터 필드 이름\n\ndescription: 필드에 대한 상세 설명 (LLM이 참조하므로 명확하게 작성)\n\ntype: 데이터 타입 (string, integer, float 등)\n\n\n\n코드\nfrom langchain.chains.query_constructor.base import AttributeInfo  \n\n# 메타데이터 필드 정보 생성  \nmetadata_field_info = [  \n    AttributeInfo(  \n        name=\"category\",  \n        description=\"The category of the cosmetic product. One of ['스킨케어', '메이크업', '클렌징', '선케어']\",  \n        type=\"string\",  \n    ),  \n    AttributeInfo(  \n        name=\"year\",  \n        description=\"The year the cosmetic product was released\",  \n        type=\"integer\",  \n    ),  \n    AttributeInfo(  \n        name=\"user_rating\",  \n        description=\"A user rating for the cosmetic product, ranging from 1 to 5\",  \n        type=\"float\",  \n    ),  \n]  \n\n\n\n\n3.2 SelfQueryRetriever 생성\nSelfQueryRetriever.from_llm() 메서드를 사용하여 retriever 객체를 생성한다.\n주요 매개변수\n\nllm: 질의를 분석하고 구조화된 쿼리로 변환할 언어 모델\n\nvectorstore: 문서가 저장된 벡터 저장소\n\ndocument_contents: 문서 내용에 대한 간략한 설명 (LLM이 쿼리 생성 시 참조)\n\nmetadata_field_info: 앞서 정의한 메타데이터 필드 정보\n\n\n\n코드\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever  \nfrom langchain_openai import ChatOpenAI  \n\n# LLM 정의  \nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  \n\n# SelfQueryRetriever 생성  \nretriever = SelfQueryRetriever.from_llm(  \n    llm=llm,  \n    vectorstore=vectorstore,  \n    document_contents=\"Brief summary of a cosmetic product\",  \n    metadata_field_info=metadata_field_info,  \n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#query-test",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#query-test",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "4 Query Test",
    "text": "4 Query Test\n\n4.1 단일 필터 검색\n자연어 질의에서 메타데이터 조건을 자동으로 추출하여 필터링한다.\n\n4.1.1 평점 기반 검색\n\n\n코드\n# 평점 4.8 이상 제품 검색  \nretriever.invoke(\"평점이 4.8 이상인 제품을 추천해주세요\")  \n\n\n[Document(metadata={'category': '선케어', 'user_rating': 4.9, 'year': 2024}, page_content='자외선 차단 기능이 있는 톤업 선크림, SPF50+/PA++++ 높은 자외선 차단 지수로 피부를 보호합니다.'), Document(metadata={'category': '클렌징', 'user_rating': 4.8, 'year': 2023}, page_content='식물성 성분으로 만든 저자극 클렌징 오일, 메이크업과 노폐물을 부드럽게 제거합니다.')]  \n\n\n4.1.2 연도 기반 검색\n\n\n코드\n# 2023년 출시 제품 검색  \nretriever.invoke(\"2023년에 출시된 상품을 추천해주세요\")  \n\n\n[Document(metadata={'category': '메이크업', 'user_rating': 4.5, 'year': 2023}, page_content='24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.'), Document(metadata={'category': '스킨케어', 'user_rating': 4.6, 'year': 2023}, page_content='비타민 C 함유 브라이트닝 크림, 칙칙한 피부톤을 환하게 밝혀줍니다.'), Document(metadata={'category': '클렌징', 'user_rating': 4.8, 'year': 2023}, page_content='식물성 성분으로 만든 저자극 클렌징 오일, 메이크업과 노폐물을 부드럽게 제거합니다.')]  \n\n\n4.1.3 카테고리 기반 검색\n\n\n코드\n# 선케어 카테고리 제품 검색  \nretriever.invoke(\"카테고리가 선케어인 상품을 추천해주세요\")  \n\n\n[Document(metadata={'category': '선케어', 'user_rating': 4.9, 'year': 2024}, page_content='자외선 차단 기능이 있는 톤업 선크림, SPF50+/PA++++ 높은 자외선 차단 지수로 피부를 보호합니다.')]  \n\n\n\n4.2 복합 필터 검색\n여러 메타데이터 조건을 조합하여 정밀한 검색을 수행한다.\n\n\n코드\n# 카테고리와 평점 조건을 동시에 적용  \nretriever.invoke(  \n    \"카테고리가 메이크업인 상품 중에서 평점이 4.5 이상인 상품을 추천해주세요\"  \n)  \n\n\n[Document(metadata={'category': '메이크업', 'user_rating': 4.5, 'year': 2023}, page_content='24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.')]"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#검색-결과-제한-limit",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#검색-결과-제한-limit",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "5 검색 결과 제한 (Limit)",
    "text": "5 검색 결과 제한 (Limit)\n\n5.1 코드 기반 제한 설정\nenable_limit=True와 search_kwargs를 사용하여 반환할 문서 개수를 제한한다.\n\n\n코드\nretriever = SelfQueryRetriever.from_llm(  \n    llm=llm,  \n    vectorstore=vectorstore,  \n    document_contents=\"Brief summary of a cosmetic product\",  \n    metadata_field_info=metadata_field_info,  \n    enable_limit=True,  # 검색 결과 제한 기능 활성화  \n    search_kwargs={\"k\": 2},  # 최대 2개의 문서만 반환  \n)  \n\n\n2023년도 출시된 상품은 3개가 있지만 k=2 설정으로 인해 2개만 반환된다.\n\n\n코드\n# 검색 결과 2개로 제한  \nretriever.invoke(\"2023년에 출시된 상품을 추천해주세요\")  \n\n\n[Document(metadata={'category': '메이크업', 'user_rating': 4.5, 'year': 2023}, page_content='24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.'), Document(metadata={'category': '스킨케어', 'user_rating': 4.6, 'year': 2023}, page_content='비타민 C 함유 브라이트닝 크림, 칙칙한 피부톤을 환하게 밝혀줍니다.')]  \n\n\n5.2 자연어 기반 제한 설정\nsearch_kwargs를 명시하지 않고 질의문에 개수를 포함하여 동적으로 제한할 수 있다.\n\n\n코드\nretriever = SelfQueryRetriever.from_llm(  \n    llm=llm,  \n    vectorstore=vectorstore,  \n    document_contents=\"Brief summary of a cosmetic product\",  \n    metadata_field_info=metadata_field_info,  \n    enable_limit=True,  # 검색 결과 제한 기능 활성화  \n)  \n\n# 질의문에 \"1개\" 명시  \nretriever.invoke(\"2023년에 출시된 상품 1개를 추천해주세요\")  \n\n\n[Document(metadata={'category': '메이크업', 'user_rating': 4.5, 'year': 2023}, page_content='24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.')]  \n\n\n코드\n# 질의문에 \"2개\" 명시  \nretriever.invoke(\"2023년에 출시된 상품 2개를 추천해주세요\")  \n\n\n[Document(metadata={'category': '메이크업', 'user_rating': 4.5, 'year': 2023}, page_content='24시간 지속되는 매트한 피니시의 파운데이션, 모공을 커버하고 자연스러운 피부 표현이 가능합니다.'), Document(metadata={'category': '스킨케어', 'user_rating': 4.6, 'year': 2023}, page_content='비타민 C 함유 브라이트닝 크림, 칙칙한 피부톤을 환하게 밝혀줍니다.')]"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#고급-query-constructor-chain-커스터마이징",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#고급-query-constructor-chain-커스터마이징",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "6 고급: Query Constructor Chain 커스터마이징",
    "text": "6 고급: Query Constructor Chain 커스터마이징\n\n조건문 쿼리들을 만들어주는 chain을 생성하는 방법\n\n여기에 메타데이터 (스키마)를 기준으로 lancgChain안에 이 조건문 쿼리를 만들어주는 프롬프트가 들어가 있음\n\n핵심은 자연어를 구조화된 쿼리로 변환하는 Query Constructor Chain을 생성하는 것이다.\n\n내부에서 어떤 일이 일어나는지 확인하고 더 많은 사용자 정의 제어를 하기 위해, 우리는 retriever를 처음부터 재구성할 수 있다.\n참고 튜토리얼\n\n\n6.1 Query Constructor 생성\nget_query_constructor_prompt 함수를 사용하여 LLM이 구조화된 쿼리를 생성하도록 하는 프롬프트를 가져온다.\n\n\n코드\nfrom langchain.chains.query_constructor.base import (  \n    StructuredQueryOutputParser,  \n    get_query_constructor_prompt,  \n)  \n\n# 문서 내용 설명과 메타데이터 필드 정보를 사용하여 쿼리 생성 프롬프트 가져오기  \nprompt = get_query_constructor_prompt(  \n    \"Brief summary of a cosmetic product\",  # 문서 내용 설명  \n    metadata_field_info,  # 메타데이터 필드 정보  \n)  \n\n# 구조화된 쿼리 출력을 파싱하는 Parser 생성  \noutput_parser = StructuredQueryOutputParser.from_components()  \n\n# Query Constructor Chain 구성: 프롬프트 → LLM → 파싱  \nquery_constructor = prompt | llm | output_parser  \n\n\nprompt.format() 메서드를 사용하여 query 매개변수에 “dummy question” 문자열을 전달하고, 그 결과를 출력하여 Prompt 내용을 확인해 보겠습니다.\nlangChain에서 이 프롬프트를 스키마 형태로 작성해주는 것을 제공한다.\n\n\n코드\n# 프롬프트 템플릿 출력  \nprint(prompt.format(query=\"dummy question\"))  \n\n\n\n\n6.2 쿼리 변환 테스트\n자연어 질의가 어떻게 구조화된 쿼리로 변환되는지 확인한다.\n\n\n코드\nquery_output = query_constructor.invoke(  \n    {  \n        \"query\": \"2023년도에 출시한 상품 중 평점이 4.5 이상인 상품중에서 스킨케어 제품을 추천해주세요\"  \n    }  \n)  \n\n\n생성된 구조화된 쿼리의 필터 조건을 확인한다.\n\n\n코드\n# 추출된 필터 조건 출력  \nquery_output.filter.arguments  \n\n\n[Comparison(comparator=, attribute=‘year’, value=2023), Comparison(comparator=, attribute=‘user_rating’, value=4.5), Comparison(comparator=, attribute=‘category’, value=‘스킨케어’)]\nQuery Constructor 최적화 팁\nSelf-query retriever의 핵심은 query constructor의 품질이다. 효과적인 검색 시스템을 만들기 위해 다음을 조정해야 한다:\n\n프롬프트 튜닝: get_query_constructor_prompt의 문서 설명을 명확하게 작성\n\nFew-shot 예시 추가: 프롬프트에 질의-쿼리 변환 예시 추가\n\nAttributeInfo 정교화: 각 메타데이터 필드의 설명을 LLM이 이해하기 쉽게 작성\n\nLLM 모델 선택: 쿼리 변환 품질에 따라 더 강력한 모델 사용 고려\n\n\n\n6.3 Structured Query Translator 사용\n구조화된 쿼리를 특정 벡터 저장소의 문법에 맞게 변환하는 translator를 사용한다. 각 벡터 저장소(Chroma, Pinecone, Weaviate 등)는 고유한 필터 문법을 가지므로 적절한 translator가 필요하다.\n\n\n코드\nfrom langchain.retrievers.self_query.chroma import ChromaTranslator  \n\n# 커스텀 Query Constructor와 Translator를 사용한 Retriever 생성  \nretriever = SelfQueryRetriever(  \n    query_constructor=query_constructor,  # 앞서 생성한 커스텀 query constructor  \n    vectorstore=vectorstore,  # 벡터 저장소  \n    structured_query_translator=ChromaTranslator(),  # Chroma 전용 쿼리 변환기  \n)  \n\n\n주요 Translator 종류\n\nChromaTranslator: Chroma 벡터 저장소용\n\nPineconeTranslator: Pinecone 벡터 저장소용\n\nWeaviateTranslator: Weaviate 벡터 저장소용\n\nQdrantTranslator: Qdrant 벡터 저장소용\n\n\n\n6.4 커스텀 Retriever 테스트\n복합 조건 검색을 수행하여 커스텀 retriever의 동작을 확인한다.\n\n\n코드\nretriever.invoke(  \n    \"2023년도에 출시한 상품 중 평점이 4.5 이상인 상품중에서 스킨케어 제품을 추천해주세요\"  \n)  \n\n\n[Document(metadata={'category': '스킨케어', 'user_rating': 4.6, 'year': 2023}, page_content='비타민 C 함유 브라이트닝 크림, 칙칙한 피부톤을 환하게 밝혀줍니다.')]"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#실전-활용-가이드",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#실전-활용-가이드",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "7 실전 활용 가이드",
    "text": "7 실전 활용 가이드\n\n7.1 메타데이터 설계 원칙\n\n필터링 가능성: 사용자가 자주 질의하는 속성을 메타데이터로 설계\n\n타입 일관성: 같은 필드는 모든 문서에서 동일한 타입 유지\n\n값의 정규화: 카테고리 등은 미리 정의된 값 사용 (오타 방지)\n\n적절한 세분화: 너무 세밀하거나 너무 포괄적이지 않게 조정\n\n\n\n7.2 성능 최적화\n\nLLM 모델 선택: 쿼리 변환 품질에 따라 gpt-4o-mini vs gpt-4o 선택\n\n캐싱 활용: 동일한 질의 패턴에 대해 결과 캐싱\n\n메타데이터 인덱싱: 벡터 저장소에서 메타데이터 필드 인덱싱 설정\n\nBatch 처리: 여러 질의를 한 번에 처리하여 LLM 호출 최소화\n\n\n\n7.3 일반적인 문제와 해결책\n\n7.3.1 문제 1: LLM이 메타데이터 필터를 잘못 추출\n\n해결: AttributeInfo의 description을 더 명확하게 작성\n\n해결: Few-shot 예시를 프롬프트에 추가\n\n\n\n7.3.2 문제 2: 특정 벡터 저장소에서 필터가 작동하지 않음\n\n해결: 해당 벡터 저장소에 맞는 Translator 사용 확인\n\n해결: 벡터 저장소가 해당 필터 연산자를 지원하는지 확인\n\n\n\n7.3.3 문제 3: 검색 결과가 너무 적거나 많음\n\n해결: enable_limit=True와 함께 적절한 k 값 설정\n\n해결: 필터 조건을 완화하거나 강화\n\n\n\n7.3.4 문제 4: 메타데이터 입력에 너무 많은 자원 소모\n\n문서에서 가정하는 것처럼 “메타데이터가 이미 구조화되어 있다”는 시나리오는 실제로는 매우 제한적.\n\n다음의 경우가 실제로 마주칠 경우들인데 이 self query를 사용하기 위해 아래의 시스템을 구축해야함 (배보다 배꼽이 더큼)\n\n신규 문서 수집: 웹 크롤링, 자동 데이터 수집, 사용자 업로드 등\n\n기존 비정형 데이터: 과거에 구조화되지 않은 텍스트 문서들\n\n지속적인 유지보수: 메타데이터 스키마가 변경되면 기존 데이터도 갱신해야 함\n\n전략 1: 메타데이터를 사람이 작성하지 말고 LLM이 자동으로 추출 (가장 실용적)\n\n장점\n\n자동화됨 (사람의 개입 최소화)\n\n스케일에 관계없이 일관성 유지\n\n배치 처리로 비용 최적화 가능\n\n\n단점\n\nLLM의 추출 오류 가능성 (정확도 90~95% 정도)\n\nLLM 호출 비용 발생\n\n중요도 높은 도메인에서는 품질 관리 필요\n\n\n\n# 문서 수집 파이프라인  \nasync def process_document_with_metadata(raw_doc: str, schema: AttributeInfo) -&gt; Document:  \n    \"\"\"문서 입수 시점에 메타데이터를 자동 추출\"\"\"  \n    \n    extraction_prompt = PromptTemplate.from_template(\"\"\"  \n    다음 문서에서 메타데이터를 추출하세요.  \n    \n    메타데이터 스키마:  \n    - category: {categories}  \n    - year: 정수형 (문서가 언급하는 연도)  \n    - user_rating: 1.0~5.0 범위의 숫자 (평가가 없으면 null)  \n    \n    문서:  \n    {doc_content}  \n    \n    JSON 형식으로 반환:  \n    {{\"category\": \"...\", \"year\": ..., \"user_rating\": ...}}  \n    \"\"\")  \n    \n    extractor = extraction_prompt | llm | JsonOutputParser()  \n    metadata = await extractor.ainvoke({  \n        \"doc_content\": raw_doc,  \n        \"categories\": [\"스킨케어\", \"메이크업\", \"클렌징\", \"선케어\"]  \n    })  \n    \n    return Document(  \n        page_content=raw_doc,  \n        metadata=metadata  \n    )  \n\n전략 2: Hybrid 접근으로 자동 추출 + 사람의 검증을 조합 (가장 현실적)\n\n비율 예시 (실제 기업에서 적용):\n\n자동 추출으로 신뢰도 &gt;= 0.85: 100% 자동 반영 (80~85%)\n\n신뢰도 0.7~0.85: 샘플링해서 사람이 검증 (10~15%)\n\n신뢰도 &lt; 0.7: 수동 처리 (5~10%)\n\n\n\n# 데이터 파이프라인 단계별 처리  \nclass MetadataProcessingPipeline:  \n    \n    async def ingest_batch(self, documents: List[str]):  \n        \"\"\"배치 입수\"\"\"  \n        # 단계 1: LLM 자동 추출  \n        with_auto_metadata = [  \n            await self.extract_metadata_llm(doc)   \n            for doc in documents  \n        ]  \n        \n        # 단계 2: 신뢰도 점수 계산 (LLM에게 추출 신뢰도 함께 요청)  \n        with_confidence = [  \n            self.add_confidence_score(doc_meta)  \n            for doc_meta in with_auto_metadata  \n        ]  \n        \n        # 단계 3: 신뢰도 낮은 것만 큐에 추가 (사람 검증용)  \n        requires_review = [  \n            item for item in with_confidence   \n            if item['confidence'] &lt; 0.8  \n        ]  \n        \n        # 단계 4: 사람 검증 (선택적)  \n        reviewed_items = await self.human_review_queue(requires_review)  \n        \n        # 단계 5: DB에 저장  \n        await self.save_to_vectorstore(with_confidence + reviewed_items)  \n\n전략 3: 도메인 특화 Rule + LLM (데이터 거버넌스 맞음)\n\n이미 데이터 표준화 프레임워크(word dictionary, domain dictionary, code dictionary)가 있다면\n\n이 표준화된 용어들을 LLM에 제공하면 정확도 크게 향상\n\n데이터 거버넌스 원칙이 메타데이터 추출에도 자동으로 반영됨\n\n하지만, 그래도 데이터 모델링이라는 큰 허들이 있긴 함\n\n\nclass StructuredMetadataExtractor:  \n    \"\"\"이미 데이터 거버넌스 시스템이 있다면 활용\"\"\"  \n    \n    def __init__(self, data_catalog: DataCatalog):  \n        self.catalog = data_catalog  # 기존 메타데이터 저장소  \n    \n    async def extract_metadata(self,   \n                               doc: Document,   \n                               doc_source: str) -&gt; dict:  \n        \"\"\"  \n        1. 소스 시스템에서 구조화된 메타데이터 먼저 조회  \n        2. 없으면 LLM이 추출  \n        3. 데이터 카탈로그에 등록된 사전(dictionary) 활용  \n        \"\"\"  \n        \n        # 단계 1: 카탈로그에서 기존 메타데이터 확인  \n        known_metadata = self.catalog.lookup(doc_source)  \n        if known_metadata:  \n            return known_metadata  \n        \n        # 단계 2: 표준화된 코드/용어 사전을 LLM에 제공  \n        dictionaries = {  \n            \"category_dict\": self.catalog.get_category_dictionary(),  \n            \"year_range\": self.catalog.get_year_range(),  \n            \"valid_ratings\": self.catalog.get_rating_scale()  \n        }  \n        \n        # 단계 3: LLM이 이 사전을 기준으로 추출  \n        extracted = await self.llm_extract_with_standards(  \n            doc.page_content,  \n            dictionaries  \n        )  \n        \n        # 단계 4: 표준 메타데이터로 정규화  \n        normalized = self.catalog.normalize_metadata(extracted)  \n        \n        # 단계 5: 다시 카탈로그에 등록 (다음 번에 재사용)  \n        await self.catalog.register(doc_source, normalized)  \n        \n        return normalized  \n# 플랫폼에 적용하는 방식: 100-200명 사용자 배포 시나리오  \nclass EnterpriseMetadataRetriever:  \n    \n    def __init__(self,   \n                 standardization_framework,  # 기존 4-tier dictionary  \n                 vectorstore,  \n                 llm_model=\"gpt-4o-mini\"):  \n        self.standards = standardization_framework  \n        self.vectorstore = vectorstore  \n        self.llm = llm_model  \n    \n    async def ingest_dataset(self, dataset: DataFrame):  \n        \"\"\"데이터셋 입수 시 메타데이터 자동 생성\"\"\"  \n        \n        documents = []  \n        for row in dataset.iterrows():  \n            # 기존 데이터 거버넌스 시스템의 메타데이터 활용  \n            metadata = {  \n                \"domain\": self.standards.infer_domain(row),  # Domain dict 활용  \n                \"data_type\": self.standards.infer_type(row),  # Term dict 활용  \n                \"quality_score\": self.standards.assess_quality(row),  \n                \"standardization_status\": self._check_standard(row),  \n            }  \n            \n            documents.append(Document(  \n                page_content=row['description'],  \n                metadata=metadata  \n            ))  \n        \n        await self.vectorstore.add_documents(documents)  \n    \n    def _check_standard(self, row):  \n        \"\"\"표준화 여부 판단 (기존 시스템 활용)\"\"\"  \n        return self.standards.is_standardized(row)  \n\n각 전략의 비용-정확도 트레이드오프\n\n\n\n\n\n\n\n\n\n\n\n\n전략\n자동화율\n정확도\n유지비\n스케일\n추천 상황\n\n\n\n\n전략 1 (LLM 자동)\n100%\n90~95%\n낮음\n매우 높음\n비정형 데이터 대량, 품질 요구 낮음\n\n\n전략 2 (Hybrid)\n85~90%\n95~98%\n중간\n높음\n균형잡힌 운영\n\n\n전략 3 (거버넌스 연계)\n90~95%\n98~99%\n중간\n중간-높음\n거버넌스 상황"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#참고-자료",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#참고-자료",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "8 참고 자료",
    "text": "8 참고 자료\n\nLangChain 공식 Self-Query Retriever 통합 목록\n\nLangChain Self-Query Hotel Search 튜토리얼\n\nQuery Constructor 커스터마이징 가이드"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#요약",
    "href": "docs/blog/posts/RAG/10-Retriever/08-SelfQueryRetriever.html#요약",
    "title": "셀프 쿼리 검색기(Self-Query Retriever)",
    "section": "9 요약",
    "text": "9 요약\nSelfQueryRetriever는 메타데이터 기반 필터링과 의미적 검색을 결합한 강력한 도구이다. 초기 메타데이터 설계와 AttributeInfo 정의에 투자하면, 사용자가 자연어로 복잡한 조건 검색을 수행할 수 있는 시스템을 구축할 수 있다. 특히 전자상거래, 문서 관리, 콘텐츠 추천 등 구조화된 메타데이터를 가진 도메인에서 높은 검색 정확도를 제공한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "",
    "text": "RAG 시스템에서 벡터 검색을 수행하면 일반적으로 유사도 점수에 따라 문서가 내림차순으로 정렬된다. 그러나 검색된 문서가 10개 이상일 때, 단순히 유사도 순서대로 LLM에 전달하는 것이 최선의 전략은 아니다.\n\n\n\n성능 저하 현상\n- 모델 아키텍처와 무관하게 10개 이상의 문서를 입력할 경우 성능이 상당히 저하된다\n- 특정 훈련 방식(Instruction Tuning 등)과 관계없이 모든 LLM에서 공통적으로 나타난다\n원인: 위치 편향 (Positional Bias)\n\nLLM은 시작과 끝 토큰에 더 높은 주의 가중치(Attention Weight)를 할당하는 학습된 편향을 가지고 있다.\n이는 Transformer의 Self-Attention 메커니즘 학습 과정에서 형성된다.\n\n학습된 편향의 형성 과정\n\n초기 상태: Transformer는 초기에 단어 간의 관계만 학습한다\n편향 학습: 학습 데이터셋에서 핵심 정보가 시작과 끝에 자주 위치했기 때문에 위치적 편향이 형성된다\n파라미터 조정:\n\n시작 토큰의 \\(\\text{Q}\\) (Query)가 다른 토큰의 \\(\\text{K}\\) (Key)와 강한 유사도를 갖도록 학습\n끝 토큰의 \\(\\text{K}\\) 가 다른 토큰의 \\(\\text{Q}\\) 와 강한 유사도를 갖도록 학습\n\n결과: 중간 위치 토큰은 상대적으로 낮은 주의 가중치를 받게 되어 ‘Lost in the Middle’ 현상 발생\n\n은닉 표현 공간에서의 정보 경쟁\n중간 위치의 정보가 손실되는 이유는 벡터 공간의 제약에서 비롯된다.\n\n벡터화 한계: 모든 정보를 고정 크기 벡터로 압축해야 하는 제약\n신호 희석 (Signal Dilution):\n\n중간 위치의 관련 정보는 앞뒤 토큰들로부터의 주의 신호와 경쟁한다\n은닉 표현 공간에서 노이즈 정보에 의해 희석되거나 덮어쓰인다\n결과적으로 모델이 해당 정보를 “잊어버린” 것처럼 동작한다\n\n\n학습 데이터의 영향\n\n대부분의 잘 설계된 글은 중요한 정보를 시작(서론)이나 끝(결론)에 배치한다.\nLLM은 이러한 패턴을 내재화하여 위치 기반 휴리스틱을 학습하게 된다.\n\n\n\n\n문제 상황 - 쿼리와 관련된 핵심 문서가 컨텍스트 중간에 위치할 경우 LLM의 답변 정확도가 크게 저하된다 - 모델이 제공된 문서를 무시하고 사전 학습 지식에만 의존하는 경향이 나타난다\n해결 방안 - 문서 재정렬 (Re-ordering): 관련성 높은 문서를 시작과 끝에 배치 - 재순위화 (Re-ranking): 검색 후 추가 모델로 순위 재조정 - LongContextReorder: 중간 문서를 시작/끝으로 교대 배치하는 LangChain 기법"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#lost-in-the-middle-현상",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#lost-in-the-middle-현상",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "",
    "text": "RAG 시스템에서 벡터 검색을 수행하면 일반적으로 유사도 점수에 따라 문서가 내림차순으로 정렬된다. 그러나 검색된 문서가 10개 이상일 때, 단순히 유사도 순서대로 LLM에 전달하는 것이 최선의 전략은 아니다.\n\n\n\n성능 저하 현상\n- 모델 아키텍처와 무관하게 10개 이상의 문서를 입력할 경우 성능이 상당히 저하된다\n- 특정 훈련 방식(Instruction Tuning 등)과 관계없이 모든 LLM에서 공통적으로 나타난다\n원인: 위치 편향 (Positional Bias)\n\nLLM은 시작과 끝 토큰에 더 높은 주의 가중치(Attention Weight)를 할당하는 학습된 편향을 가지고 있다.\n이는 Transformer의 Self-Attention 메커니즘 학습 과정에서 형성된다.\n\n학습된 편향의 형성 과정\n\n초기 상태: Transformer는 초기에 단어 간의 관계만 학습한다\n편향 학습: 학습 데이터셋에서 핵심 정보가 시작과 끝에 자주 위치했기 때문에 위치적 편향이 형성된다\n파라미터 조정:\n\n시작 토큰의 \\(\\text{Q}\\) (Query)가 다른 토큰의 \\(\\text{K}\\) (Key)와 강한 유사도를 갖도록 학습\n끝 토큰의 \\(\\text{K}\\) 가 다른 토큰의 \\(\\text{Q}\\) 와 강한 유사도를 갖도록 학습\n\n결과: 중간 위치 토큰은 상대적으로 낮은 주의 가중치를 받게 되어 ‘Lost in the Middle’ 현상 발생\n\n은닉 표현 공간에서의 정보 경쟁\n중간 위치의 정보가 손실되는 이유는 벡터 공간의 제약에서 비롯된다.\n\n벡터화 한계: 모든 정보를 고정 크기 벡터로 압축해야 하는 제약\n신호 희석 (Signal Dilution):\n\n중간 위치의 관련 정보는 앞뒤 토큰들로부터의 주의 신호와 경쟁한다\n은닉 표현 공간에서 노이즈 정보에 의해 희석되거나 덮어쓰인다\n결과적으로 모델이 해당 정보를 “잊어버린” 것처럼 동작한다\n\n\n학습 데이터의 영향\n\n대부분의 잘 설계된 글은 중요한 정보를 시작(서론)이나 끝(결론)에 배치한다.\nLLM은 이러한 패턴을 내재화하여 위치 기반 휴리스틱을 학습하게 된다.\n\n\n\n\n문제 상황 - 쿼리와 관련된 핵심 문서가 컨텍스트 중간에 위치할 경우 LLM의 답변 정확도가 크게 저하된다 - 모델이 제공된 문서를 무시하고 사전 학습 지식에만 의존하는 경향이 나타난다\n해결 방안 - 문서 재정렬 (Re-ordering): 관련성 높은 문서를 시작과 끝에 배치 - 재순위화 (Re-ranking): 검색 후 추가 모델로 순위 재조정 - LongContextReorder: 중간 문서를 시작/끝으로 교대 배치하는 LangChain 기법"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#관련-연구-lost-in-the-middle",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#관련-연구-lost-in-the-middle",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "2 관련 연구: Lost in the Middle",
    "text": "2 관련 연구: Lost in the Middle\n\n2.1 논문 개요\nLost in the Middle: How Language Models Use Long Contexts (Liu et al., 2023)\n이 논문은 최근 언어 모델들이 긴 컨텍스트를 입력받을 수 있음에도 불구하고, 컨텍스트 내 정보를 얼마나 효과적으로 활용하는지를 체계적으로 분석한 연구다.\n핵심 발견: LLM이 문맥 중간에 위치한 정보를 검색할 때 성능이 현저히 저하되는 ‘Lost in the Middle’ 현상을 발견했다. ### 실험 설계\n연구팀은 멀티 문서 질의응답(Multi-Document QA)과 키-값 검색(Key-Value Retrieval) 두 가지 작업을 설계하여 모델 성능을 평가했다.\n주요 발견\n\n정보 위치에 따른 성능 저하\n\n관련 정보의 위치가 변경될 때 언어 모델의 성능이 크게 저하된다\n현재 LLM은 긴 입력 컨텍스트의 정보를 강건하게(robustly) 활용하지 못한다\n\n위치별 성능 패턴\n\n성능 순서: 앞 부분 &gt; 뒷 부분 &gt; 중간 부분\n시작(Beginning) 또는 끝(End) 위치: 최고 성능\n중간(Middle) 위치: 현저한 성능 저하\n\n범용적 현상\n\nLong Context를 명시적으로 지원하는 모델(GPT-4, Claude 등)에서도 동일하게 관찰된다\n모델 크기, 아키텍처와 무관하게 공통적으로 나타난다\n\n\n\n\n2.2 평가 방법론\n실험 설계 원칙: 관련 정보의 위치를 체계적으로 변화시키면서 성능을 측정한다.\n작업 1: 다중 문서 질의응답 (Multi-Document QA)\n목적: 여러 문서 중에서 정답을 포함한 문서를 식별하고 정확한 답변을 추출하는 능력을 평가한다.\n\n데이터셋: Natural Questions, HotpotQA 등 기존 QA 벤치마크 활용\n컨텍스트 구성:\n\n관련 문서 (Relevant Document): 질문에 대한 정답 포함\n방해 문서 (Distractor Documents): 무작위로 선택된 비관련 문서들\n두 종류를 결합하여 매우 긴 컨텍스트를 생성한다\n\n\n작업 2: 키-값 검색 (Key-Value Retrieval)\n목적: 긴 입력에서 특정 키에 해당하는 값을 정확히 추출하는 능력을 평가한다.\n\n컨텍스트 구성: 수많은 무작위 키-값 쌍으로 구성\n평가 방식:\n\n모델에게 특정 키 제시\n해당 키와 정확히 일치하는 값을 출력하는지 확인\n정확도(Exact Match) 측정\n\n\n‘Lost in the Middle’ 현상 측정 프로토콜\n두 작업 모두에서 관련 정보의 위치를 체계적으로 조작하여 성능을 측정한다.\n\n관련 정보 추출\n\nQA: 정답이 포함된 문서\n키-값 검색: 질문과 관련된 키-값 쌍\n\n위치 삽입 (Position Shifting)\n\n전체 컨텍스트의 다양한 위치에 삽입한다\n위치: 시작(Index 0), 1/4 지점, 중간(Middle), 3/4 지점, 끝(Index N)\n\n성능 측정\n\n각 위치별로 정답 정확도(Accuracy) 측정\n위치에 따른 성능 곡선 분석\n\n\n\n\n2.3 논문의 기여\n학술적 기여 1. LLM이 긴 컨텍스트를 처리하는 메커니즘에 대한 실증적 분석을 제공한다 2. Long Context LLM 성능 평가를 위한 새로운 벤치마크 프로토콜을 제시한다 3. 위치 편향이라는 LLM의 근본적 한계를 밝혔다\n실무적 시사점\n논문은 문서 재정렬(Re-ordering) 전략을 제안한다: - 중간 부분의 문서를 시작과 끝으로 교대 배치한다 - 가장 관련성 높은 문서를 시작과 끝에 위치시킨다 - LangChain의 LongContextReorder가 이 전략을 구현한 것이다\n\n\n2.4 실험 결과 요약\n위치별 성능 패턴\n\n\n\n\n\n\n\n\n\n관련 정보 위치\n예상 성능\n실험 결과\n성능 저하 정도\n\n\n\n\n시작 (Beginning)\n높음\n최고 성능 (90-95%)\n-\n\n\n1/4 지점\n중간\n중간 성능 (70-80%)\n약 15% 저하\n\n\n중간 (Middle)\n낮음\n최저 성능 (50-60%)\n최대 40% 저하\n\n\n3/4 지점\n중간\n중간 성능 (75-85%)\n약 10% 저하\n\n\n끝 (End)\n높음\n높은 성능 (85-90%)\n약 5% 저하\n\n\n\n핵심 인사이트 - 시작 위치가 가장 높은 성능을 보인다 - 중간 위치는 최대 40%의 성능 저하가 발생한다 - 끝 위치도 높은 성능을 보이지만 시작보다는 약간 낮다\n\n\n2.5 해결 방안: 문서 재정렬\n이 문제를 해결하기 위해 검색된 문서의 순서를 전략적으로 재배열하여 LLM에 입력한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#실습-longcontextreorder-적용",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#실습-longcontextreorder-적용",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "3 실습: LongContextReorder 적용",
    "text": "3 실습: LongContextReorder 적용\n\n3.1 환경 설정\n이 실습에서는 다음을 수행한다: 1. Chroma 벡터 저장소로 텍스트 데이터를 저장하고 검색한다 2. 일반 검색 결과와 재정렬된 결과를 비교한다 3. RAG 체인에 LongContextReorder를 통합한다\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n\n\n\n\n3.2 LongContextReorder 의 재순위 방식 — 핵심 로직\nLongContextReorder 의 transform_documents(docs) 함수는 내부적으로 다음과 같은 단계를 거쳐 재정렬한다.\n\nretriever가 반환한 문서 리스트를 받는다. 이 리스트는 통상적으로 유사도 기준으로 정렬되어 있다 (가장 관련 높은 문서가 앞쪽).\n\n\n원본: [D0(높음), D1, D2, D3, D4(낮음)]\n\n\n내부적으로 리스트를 역순(reverse)으로 뒤집는다.\n\n\nreverse 결과: [D4, D3, D2, D1, D0]\n\n\n그 뒤 “양끝 먼저, 중간은 나중” 식의 로직으로 다시 문서를 배치한다. 구체적으로:\n\n\n역순된 리스트에서 인덱스 i가 짝수인 문서를 결과 리스트의 맨 앞(front)에 삽입.\n인덱스 i가 홀수인 문서를 결과 리스트의 맨 뒤(back)에 삽입.\n대화 초반에 멀리 떨어진 문서를 먼저, 후반엔 가까운 문서를 배치\n이걸 i=0→4 순으로 처리하면:\n\ni=0 → D4 (짝수 → 앞에 넣기) 결과: [D4]\ni=1 → D3 (홀수 → 뒤에 넣기) 결과: [D4, D3]\ni=2 → D2 (짝수 → 앞에 넣기) 결과: [D2, D4, D3]\ni=3 → D1 (홀수 → 뒤에 넣기) 결과: [D2, D4, D3, D1]\ni=4 → D0 (짝수 → 앞에 넣기) 결과: [D0, D2, D4, D3, D1]\n\n결과적으로, 원래 유사도가 높은 문서들이 앞과 뒤(즉, 컨텍스트의 처음과 끝)에 배치되고, 덜 관련 있거나 중간 순위 문서들은 가운데 쪽에 모이게 된다.\n\n\n\n코드\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.document_transformers import LongContextReorder\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n\n# 임베딩을 가져옵니다.\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\ntexts = [\n    \"이건 그냥 내가 아무렇게나 적어본 글입니다.\",\n    \"사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\",\n    \"아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\",\n    \"챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\",\n    \"챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\",\n    \"애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\",\n    \"ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\",\n    \"비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\",\n    \"ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\",\n    \"FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\",\n]\n\n\n# 검색기를 생성합니다. (K는 10으로 설정합니다)\nretriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n    search_kwargs={\"k\": 10}\n)\n\n\n검색기에 쿼리를 입력하여 검색을 수행합니다.\n\n\n코드\nquery = \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\"\n\n# 관련성 점수에 따라 정렬된 관련 문서를 가져옵니다.\ndocs = retriever.invoke(query)\ndocs\n\n\n[Document(page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'), \nDocument(page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'), \nDocument(page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'), \nDocument(page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'), \nDocument(page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'), \nDocument(page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\nDocument(page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\nDocument(page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'), \nDocument(page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'), \nDocument(page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.')]\n검색 결과 분석\nChatGPT와 관련성이 높은 문서들이 상위에 정렬되어 있으며, 유사도 점수에 따라 내림차순으로 정렬된다.\n\n상위 3개: ChatGPT 직접 관련 문서 (높은 관련성)\n4-6번: ChatGPT 언급 문서 (중간 관련성)\n7-10번: 무관한 문서 (낮은 관련성)\n\n이제 LongContextReorder를 적용하여 문서 순서를 재배치한다.\n\n\n3.3 문서 재정렬 적용\nLongContextReorder 클래스의 인스턴스인 reordering을 생성합니다.\n\nreordering.transform_documents(docs)를 호출하여 문서 목록 docs를 재정렬\n\n덜 관련된 문서는 목록의 중간에 위치시킨다\n더 관련된 문서는 시작과 끝에 위치시킨다\n\n\n\n\n코드\n# 문서를 재정렬한다\n# 덜 관련된 문서는 목록의 중간에 위치하고 더 관련된 요소는 시작/끝에 위치한다\nreordering = LongContextReorder()\nreordered_docs = reordering.transform_documents(docs)  # 순위 재조정\n\n# ChatGPT 관련 문서가 시작과 끝에 위치하는지 확인한다\nreordered_docs\n\n\n[Document(page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'), \nDocument(page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\nDocument(page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'), \nDocument(page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'), \nDocument(page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.'), \nDocument(page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'), \nDocument(page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'), \nDocument(page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'), \nDocument(page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'), \nDocument(page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.')]\n재정렬 결과 분석\n재정렬 전후를 비교하면 다음과 같은 패턴을 확인할 수 있다:\n\n\n\n위치\n재정렬 전\n재정렬 후\n관련성\n\n\n\n\n0-1번\n높은 관련성\n중간 관련성\n시작에 중간 문서 배치\n\n\n2-7번\n중간/낮은 관련성\n낮은 관련성\n중간에 무관 문서 집중\n\n\n8-9번\n낮은 관련성\n높은 관련성\n끝에 핵심 문서 배치\n\n\n\n재정렬 전략: - 가장 관련성 높은 문서(1, 10번)를 끝에 배치한다 - 중간 관련성 문서(2-4번)를 시작에 배치한다 - 무관한 문서(5-8번)를 중간에 배치한다\n이렇게 하면 LLM이 시작과 끝에서 관련 정보를 찾을 수 있어 성능이 향상된다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#rag-체인에-context-reordering-통합",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#rag-체인에-context-reordering-통합",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "4 RAG 체인에 Context Reordering 통합",
    "text": "4 RAG 체인에 Context Reordering 통합\n\n\n코드\ndef format_docs(docs):\n    return \"\\n\".join([doc.page_content for i, doc in enumerate(docs)])\n\n\n\n\n코드\nprint(format_docs(docs))\n\n\nChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\nChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\n사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\n챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\n이건 그냥 내가 아무렇게나 적어본 글입니다.\n챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\n비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\n아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\n애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\nFIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\n\n\n코드\ndef format_docs(docs):\n    return \"\\n\".join(\n        [\n            f\"[{i}] {doc.page_content} [source: teddylee777@gmail.com]\"\n            for i, doc in enumerate(docs)\n        ]\n    )\n\n\ndef reorder_documents(docs):\n    # 재정렬\n    reordering = LongContextReorder()\n    reordered_docs = reordering.transform_documents(docs)\n    combined = format_docs(reordered_docs)\n    print(combined)\n    return combined\n\n\n재정렬된 문서를 출력합니다.\n\n\n코드\n# 재정렬된 문서를 출력\n_ = reorder_documents(docs)\n\n\n[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]\n[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]\n[2] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]\n[3] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]\n[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]\n[5] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]\n[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]\n[7] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]\n[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]\n[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]\n\n\n코드\nfrom langchain.prompts import ChatPromptTemplate\nfrom operator import itemgetter\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n# 프롬프트 템플릿\ntemplate = \"\"\"Given this text extracts:\n{context}\n\n-----\nPlease answer the following question:\n{question}\n\nAnswer in the following languages: {language}\n\"\"\"\n\n# 프롬프트 정의\nprompt = ChatPromptTemplate.from_template(template)\n\n# Chain 정의\nchain = (\n    {\n        \"context\": itemgetter(\"question\")\n        | retriever\n        | RunnableLambda(reorder_documents),  # 질문을 기반으로 문맥을 검색합니다.\n        \"question\": itemgetter(\"question\"),  # 질문을 추출합니다.\n        \"language\": itemgetter(\"language\"),  # 답변 언어를 추출합니다.\n    }\n    | prompt  # 프롬프트 템플릿에 값을 전달합니다.\n    | ChatOpenAI(model=\"gpt-4o-mini\")  # 언어 모델에 프롬프트를 전달합니다.\n    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n)\n\n\nquestion 에 쿼리를 입력하고 language 에 언어를 입력합니다.\n\n재정렬된 문서의 검색 결과도 확인합니다.\n\n\n\n코드\nanswer = chain.invoke(\n    {\"question\": \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\", \"language\": \"KOREAN\"}\n)\n\n\n[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]\n[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]\n[2] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]\n[3] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]\n[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]\n[5] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]\n[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]\n[7] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]\n[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]\n[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]\n답변을 출력합니다.\n\n\n코드\nprint(answer)\n\n\nChatGPT는 OpenAI에 의해 개발된 인공지능으로, 사용자의 질문을 이해하고 적절한 답변을 생성하는 데 대량의 데이터를 학습했습니다. 이 AI는 지속적인 학습과 업데이트를 통해 더욱 발전하고 있으며, 다양한 질문에 답하거나 복잡한 문제를 해결하는 데에도 사용될 수 있습니다. 또한 창의적인 아이디어를 제안하는 기능도 갖추고 있습니다. 사용자는 ChatGPT와 대화하는 것처럼 상호작용할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#성능-비교-및-결론",
    "href": "docs/blog/posts/RAG/10-Retriever/04-LongContextReorder.html#성능-비교-및-결론",
    "title": "긴 문맥 재정렬(LongContextReorder)",
    "section": "5 성능 비교 및 결론",
    "text": "5 성능 비교 및 결론\n\n5.1 LongContextReorder의 효과\n기대 효과 1. 답변 정확도 향상: 관련 문서를 시작과 끝에 배치하여 LLM이 핵심 정보를 놓치지 않도록 한다 2. 환각(Hallucination) 감소: 제공된 컨텍스트를 더 잘 활용하여 사실 기반 답변 생성을 촉진한다 3. 일관성 향상: 동일한 질문에 대해 더 안정적인 답변을 제공한다\n\n\n5.2 실무 적용 가이드\n언제 사용해야 하는가?\n사용 권장 상황 - 검색된 문서가 10개 이상일 때 - 긴 문서를 여러 개 처리해야 할 때 - 답변 품질이 일관되지 않을 때 - 중요 정보가 중간에 묻힐 가능성이 있을 때\n사용 불필요 상황 - 검색 문서가 3-5개 이하일 때 - 모든 문서가 고도로 관련성이 있을 때 - 짧은 컨텍스트만 다룰 때\n\n\n5.3 추가 최적화 전략\nLongContextReorder와 함께 사용하면 좋은 기법들:\n\n하이브리드 검색: BM25 + 벡터 검색 조합으로 다양한 관련 문서 확보\n재순위화(Re-ranking): Cross-encoder로 검색 결과 품질 향상 후 재정렬 적용\n청크 크기 최적화: 문서를 적절한 크기로 분할하여 관련성 높은 청크만 검색\n메타데이터 필터링: 재정렬 전에 무관한 문서를 사전 필터링\n\n\n\n5.4 핵심 요약\nLost in the Middle 문제 - LLM은 긴 컨텍스트의 중간에 위치한 정보를 잘 활용하지 못한다 - 시작과 끝 위치에 더 높은 주의를 기울이는 학습된 편향이 원인이다\n해결책: LongContextReorder - 관련성 높은 문서를 시작과 끝에 배치한다 - 무관한 문서를 중간에 집중시킨다 - RAG 시스템의 답변 정확도와 일관성을 향상시킨다\n실무 적용 - 10개 이상의 문서를 다룰 때 필수적이다 - 다른 최적화 기법(재순위화, 하이브리드 검색)과 조합하여 사용한다 - LangChain의 LongContextReorder로 간단히 구현할 수 있다"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html",
    "title": "한글 단어 리트리버 튜닝",
    "section": "",
    "text": "한글 형태소 분석기 라이브러리인 kiwipiepy 를 설치합니다. - kiwipiepy 프로젝트 링크\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n\n\nTrue\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n\n\nLangSmith 추적을 시작합니다.\n[프로젝트명]\nCH10-Retriever\n코드\n# !pip install kiwipiepy\n코드\nfrom kiwipiepy import Kiwi\n\nkiwi = Kiwi()\n토큰화를 진행합니다.\n코드\nkiwi.tokenize(\"안녕하세요? 형태소 분석기 키위입니다\")\n\n\n[Token(form='안녕', tag='NNG', start=0, len=2),\n Token(form='하', tag='XSA', start=2, len=1),\n Token(form='세요', tag='EF', start=3, len=2),\n Token(form='?', tag='SF', start=5, len=1),\n Token(form='형태소', tag='NNG', start=7, len=3),\n Token(form='분석기', tag='NNG', start=11, len=3),\n Token(form='키위', tag='NNG', start=15, len=2),\n Token(form='이', tag='VCP', start=17, len=1),\n Token(form='ᆸ니다', tag='EF', start=17, len=3)]"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html#다양한-문장으로-테스트",
    "href": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html#다양한-문장으로-테스트",
    "title": "한글 단어 리트리버 튜닝",
    "section": "1 다양한 문장으로 테스트",
    "text": "1 다양한 문장으로 테스트\n\n\n코드\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain_core.documents import Document\nfrom langchain.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\"\n    ),\n    Document(\n        page_content=\"금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\"\n    ),\n    Document(\n        page_content=\"저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\"\n    ),\n    Document(\n        page_content=\"금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\"\n    ),\n    Document(\n        page_content=\"금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\"\n    ),\n    Document(\n        page_content=\"금보험은 저축성과를 극대화합니다. 특히 노후 대비 저축에 유리하게 구성되어 있습니다.\"\n    ),\n    Document(\n        page_content=\"금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\"\n    ),\n]\n\n\n\n\n코드\nfor doc in docs:\n    print(\" \".join([token.form for token in kiwi.tokenize(doc.page_content)]))\n\n\n금융 보험 은 장기 적 이 ᆫ 자산 관리 와 위험 대비 를 목적 으로 고안 되 ᆫ 금융 상품 이 ᆸ니다 .\n금융 저축 보험 은 규칙 적 이 ᆫ 저축 을 통하 어 목돈 을 마련 하 ᆯ 수 있 으며 , 생명 보험 기능 도 겸비 하 고 있 습니다 .\n저축 금융 보험 은 저축 과 금융 을 통하 어 목돈 마련 에 도움 을 주 는 보험 이 ᆸ니다 . 또한 , 사망 보장 기능 도 제공 하 ᆸ니다 .\n금융 저 축산물 보험 은 장기 적 이 ᆫ 저축 목적 과 더불 어 , 축산물 제공 기능 을 갖추 고 있 는 특별 금융 상품 이 ᆸ니다 .\n금융 단 폭격 보험 은 저축 은 커녕 위험 대비 에 초점 을 맞추 ᆫ 상품 이 ᆸ니다 . 높 은 위험 을 감수 하 고자 하 는 고객 에게 적합 하 ᆸ니다 .\n금 보험 은 저축 성과 를 극대 화 하 ᆸ니다 . 특히 노후 대비 저축 에 유리 하 게 구성 되 어 있 습니다 .\n금융 보 씨 험하 ᆫ 말 좀 하 지 말 시 고 , 저축 이나 좀 하 시 던가요 . 뭐 가 그리 급하 시 ᆫ지 모르 겠 네요 .\n\n\n\n\n코드\n# 토큰화 함수를 생성\ndef kiwi_tokenize(text):\n    return [token.form for token in kiwi.tokenize(text)]\n\n\n\n1.1 실험: 다양한 종류의 검색기를 사용하여 검색 결과를 비교\n\n\n코드\nbm25 = BM25Retriever.from_documents(docs)\n\nkiwi_bm25 = BM25Retriever.from_documents(docs, preprocess_func=kiwi_tokenize)\n\nfaiss = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()\n\nbm25_faiss_73 = EnsembleRetriever(\n    retrievers=[bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.7, 0.3],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\nbm25_faiss_37 = EnsembleRetriever(\n    retrievers=[bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.3, 0.7],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\nkiwibm25_faiss_73 = EnsembleRetriever(\n    retrievers=[kiwi_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.7, 0.3],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\nkiwibm25_faiss_37 = EnsembleRetriever(\n    retrievers=[kiwi_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.3, 0.7],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\n\nretrievers = {\n    \"bm25\": bm25,\n    \"kiwi_bm25\": kiwi_bm25,\n    \"faiss\": faiss,\n    \"bm25_faiss_73\": bm25_faiss_73,\n    \"bm25_faiss_37\": bm25_faiss_37,\n    \"kiwi_bm25_faiss_73\": kiwibm25_faiss_73,\n    \"kiwi_bm25_faiss_37\": kiwibm25_faiss_37,\n}\n\n\n\n\n코드\ndef print_search_results(retrievers, query):\n    print(f\"Query: {query}\")\n    for name, retriever in retrievers.items():\n        print(f\"{name}    \\t: {retriever.invoke(query)[0].page_content}\")\n    print(\"===\" * 20)\n\n\n검색 결과를 출력합니다.\n\n\n코드\nprint_search_results(retrievers, \"금융보험\")\nprint_search_results(retrievers, \"금융 보험\")\nprint_search_results(retrievers, \"금융저축보험\")\nprint_search_results(retrievers, \"축산물 보험\")\nprint_search_results(retrievers, \"저축금융보험\")\nprint_search_results(retrievers, \"금융보씨 개인정보 조회\")\n\n\nQuery: 금융보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================\nQuery: 금융 보험\nbm25        : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_37       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================\nQuery: 금융저축보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkiwi_bm25_faiss_37      : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\n============================================================\nQuery: 축산물 보험\nbm25        : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nfaiss       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nbm25_faiss_73       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nbm25_faiss_37       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_73      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\n============================================================\nQuery: 저축금융보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkiwi_bm25_faiss_37      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\n============================================================\nQuery: 금융보씨 개인정보 조회\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html#konlpy",
    "href": "docs/blog/posts/RAG/10-Retriever/10-Kiwi-BM25Retriever.html#konlpy",
    "title": "한글 단어 리트리버 튜닝",
    "section": "2 Konlpy",
    "text": "2 Konlpy\n\n\n코드\n# !pip install konlpy\n\n\n\n\n코드\nfrom konlpy.tag import Kkma, Okt, Komoran, Hannanum\nfrom kiwipiepy import Kiwi\n\nkkma = Kkma()\nokt = Okt()\nkomoran = Komoran()\nhannanum = Hannanum()\nkiwi = Kiwi()\n\n\n\n\n코드\ntext = \"안녕하세요? 형태소 분석기 테스트베드입니다.\"\n\n\n\n\n코드\nprint(\"kkma    : \\t\", end=\"\")\nprint(\" \".join(kkma.morphs(text)))\nprint(\"okt     : \\t\", end=\"\")\nprint(\" \".join(okt.morphs(text)))\nprint(\"komoran : \\t\", end=\"\")\nprint(\" \".join(komoran.morphs(text)))\nprint(\"hannanum: \\t\", end=\"\")\nprint(\" \".join(hannanum.morphs(text)))\nprint(\"kiwi    : \\t\", end=\"\")\nprint(\" \".join([tok.form for tok in kiwi.tokenize(text)]))\n\n\nkkma    :   안녕 하 세요 ? 형태소 분석기 테스트 베드 이 ㅂ니다 .\nokt     :   안녕하세요 ? 형태소 분석 기 테스트 베드 입니다 .\nkomoran :   안녕하세요 ? 형태소 분석기 테스트 베드 이 ㅂ니다 .\nhannanum:   안녕 하 세 요 ? 형태소 분석기 테스트베드 이 ㅂ니다 .\nkiwi    :   안녕 하 세요 ? 형태소 분석기 테스트 베드 이 ᆸ니다 .\n\n\n\n\n코드\nkiwi.add_user_word(\"안녕하세요 반가워요\", \"NNP\", 0)\n\n\nTrue\n\n\n\n\n코드\nprint(\"kiwi    : \\t\", end=\"\")\nprint(\" \".join([tok.form for tok in kiwi.tokenize(text)]))\n\n\nkiwi    :   안녕 하 세요 ? 형태소 분석기 테스트 베드 이 ᆸ니다 .\n\n\n\n\n코드\ndef kkma_tokenize(text):\n    return [token for token in kkma.morphs(text)]\n\n\n\n\n코드\ndef okt_tokenize(text):\n    return [token for token in okt.morphs(text)]\n\n\n\n\n코드\nkkma_bm25 = BM25Retriever.from_documents(docs, preprocess_func=kkma_tokenize)\nkkma_bm25_faiss_73 = EnsembleRetriever(\n    retrievers=[kkma_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.7, 0.3],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\nkkma_bm25_faiss_37 = EnsembleRetriever(\n    retrievers=[kkma_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.3, 0.7],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\n\nokt_bm25 = BM25Retriever.from_documents(docs, preprocess_func=okt_tokenize)\nokt_bm25_faiss_73 = EnsembleRetriever(\n    retrievers=[okt_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.7, 0.3],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\nokt_bm25_faiss_37 = EnsembleRetriever(\n    retrievers=[okt_bm25, faiss],  # 사용할 검색 모델의 리스트\n    weights=[0.3, 0.7],  # 각 검색 모델의 결과에 적용할 가중치\n    search_type=\"mmr\",  # 검색 결과의 다양성을 증진시키는 MMR 방식을 사용\n)\n\nretrievers = {\n    \"bm25\": bm25,\n    \"kiwi_bm25\": kiwi_bm25,\n    \"faiss\": faiss,\n    \"bm25_faiss_73\": bm25_faiss_73,\n    \"bm25_faiss_37\": bm25_faiss_37,\n    \"kiwi_bm25_faiss_73\": kiwibm25_faiss_73,\n    \"kiwi_bm25_faiss_37\": kiwibm25_faiss_37,\n    \"kkma_bm25\": kkma_bm25,\n    \"kkma_bm25_faiss_73\": kkma_bm25_faiss_73,\n    \"kkma_bm25_faiss_37\": kkma_bm25_faiss_37,\n    \"okt_bm25\": okt_bm25,\n    \"okt_bm25_faiss_73\": okt_bm25_faiss_73,\n    \"okt_bm25_faiss_37\": okt_bm25_faiss_37,\n}\n\n\n\n\n코드\nprint_search_results(retrievers, \"금융보험\")\nprint_search_results(retrievers, \"금융 보험\")\nprint_search_results(retrievers, \"금융저축보험\")\nprint_search_results(retrievers, \"축산물 보험\")\nprint_search_results(retrievers, \"저축금융보험\")\nprint_search_results(retrievers, \"금융보씨 개인정보 조회\")\n\n\nQuery: 금융보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkkma_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkkma_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nokt_bm25        : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_73       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nokt_bm25_faiss_37       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================\nQuery: 금융 보험\nbm25        : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_37       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkkma_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_73      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkkma_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nokt_bm25        : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_73       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nokt_bm25_faiss_37       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================\nQuery: 금융저축보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkiwi_bm25_faiss_37      : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\nkkma_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_37      : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\nokt_bm25        : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_73       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_37       : 금융저축보험은 규칙적인 저축을 통해 목돈을 마련할 수 있으며, 생명보험 기능도 겸비하고 있습니다.\n============================================================\nQuery: 축산물 보험\nbm25        : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nfaiss       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nbm25_faiss_73       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nbm25_faiss_37       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_73      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkiwi_bm25_faiss_37      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkkma_bm25       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkkma_bm25_faiss_73      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nkkma_bm25_faiss_37      : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nokt_bm25        : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nokt_bm25_faiss_73       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\nokt_bm25_faiss_37       : 금융저축산물보험은 장기적인 저축 목적과 더불어, 축산물 제공 기능을 갖추고 있는 특별 금융 상품입니다.\n============================================================\nQuery: 저축금융보험\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nfaiss       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nbm25_faiss_73       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkiwi_bm25_faiss_37      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_73      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nkkma_bm25_faiss_37      : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25        : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_73       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\nokt_bm25_faiss_37       : 저축금융보험은 저축과 금융을 통해 목돈 마련에 도움을 주는 보험입니다. 또한, 사망 보장 기능도 제공합니다.\n============================================================\nQuery: 금융보씨 개인정보 조회\nbm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nfaiss       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nbm25_faiss_73       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nbm25_faiss_37       : 금융단폭격보험은 저축은 커녕 위험 대비에 초점을 맞춘 상품입니다. 높은 위험을 감수하고자 하는 고객에게 적합합니다.\nkiwi_bm25_faiss_73      : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkiwi_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nkkma_bm25       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkkma_bm25_faiss_73      : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nkkma_bm25_faiss_37      : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\nokt_bm25        : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nokt_bm25_faiss_73       : 금융보씨 험한말 좀 하지마시고, 저축이나 좀 하시던가요. 뭐가 그리 급하신지 모르겠네요.\nokt_bm25_faiss_37       : 금융보험은 장기적인 자산 관리와 위험 대비를 목적으로 고안된 금융 상품입니다.\n============================================================"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html",
    "title": "벡터스토어 기반 검색기(VectorStore-backed Retriever)",
    "section": "",
    "text": "VectorStore 지원 검색기 는 vector store를 사용하여 문서를 검색하는 retriever입니다.\nVector store에 구현된 유사도 검색(similarity search) 이나 MMR 과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리합니다.\n스플릿 다큐먼트를 확인해서 잘못나온거와 잘나온거를 확인해서 원하는 답변이 잘 나오도록 리브리버를 구현하는게 좋다.\n아래의 코드를 실행하여 VectorStore 를 생성합니다.\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n코드\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\n\n# TextLoader를 사용하여 파일을 로드합니다.\nloader = TextLoader(\"./data/appendix-keywords.txt\")\n\n# 문서를 로드합니다.\ndocuments = loader.load()\n\n# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n\n# 로드된 문서를 분할합니다.\nsplit_docs = text_splitter.split_documents(documents)\n\n# OpenAI 임베딩을 생성합니다.\nembeddings = OpenAIEmbeddings()\n\n# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\ndb = FAISS.from_documents(split_docs, embeddings)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html#동적-설정configurable",
    "href": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html#동적-설정configurable",
    "title": "벡터스토어 기반 검색기(VectorStore-backed Retriever)",
    "section": "1 동적 설정(Configurable)",
    "text": "1 동적 설정(Configurable)\n\n검색 설정을 동적으로 조정하기 위해 ConfigurableField 를 사용합니다.\nConfigurableField 는 검색 매개변수의 고유 식별자, 이름, 설명을 설정하는 역할을 합니다.\n검색 설정을 조정하기 위해 config 매개변수를 사용하여 검색 설정을 지정합니다.\n검색 설정은 config 매개변수에 전달된 딕셔너리의 configurable 키에 저장됩니다.\n검색 설정은 검색 쿼리와 함께 전달되며, 검색 쿼리에 따라 동적으로 조정됩니다.\n\n\n\n코드\nfrom langchain_core.runnables import ConfigurableField\n\n# k 설정\nretriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n    search_type=ConfigurableField(\n        id=\"search_type\",\n        name=\"Search Type\",\n        description=\"The search type to use\",\n    ),\n    search_kwargs=ConfigurableField(\n        # 검색 매개변수의 고유 식별자를 설정\n        id=\"search_kwargs\",\n        # 검색 매개변수의 이름을 설정\n        name=\"Search Kwargs\",\n        # 검색 매개변수에 대한 설명을 작성\n        description=\"The search kwargs to use\",\n    ),\n)\n\n\n아래는 동적 검색설정을 적용한 예시입니다.\n\n\n코드\n# 검색 설정을 지정. Faiss 검색에서 k=3로 설정하여 가장 유사한 문서 3개를 반환\nconfig = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n\n# 관련 문서를 검색\ndocs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\", config=config)\n\n# 관련 문서를 검색\nfor doc in docs:\n    print(doc.page_content)\n    print(\"=========================================================\")\n\n\n\n\n코드\n# 검색 설정을 지정. score_threshold 0.8 이상의 점수를 가진 문서만 반환\nconfig = {\n    \"configurable\": {\n        \"search_type\": \"similarity_score_threshold\",\n        \"search_kwargs\": {\n            \"score_threshold\": 0.8,\n        },\n    }\n}\n\n# 관련 문서를 검색\ndocs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n\n# 관련 문서를 검색\nfor doc in docs:\n    print(doc.page_content)\n    print(\"=========================================================\")\n\n\n\n\n코드\n# 검색 설정을 지정. mmr 검색 설정.\nconfig = {\n    \"configurable\": {\n        \"search_type\": \"mmr\",\n        \"search_kwargs\": {\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6},\n    }\n}\n\n# 관련 문서를 검색\ndocs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n\n# 관련 문서를 검색\nfor doc in docs:\n    print(doc.page_content)\n    print(\"=========================================================\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html#upstage-임베딩과-같이-query-passage-embedding-model-이-분리된-경우",
    "href": "docs/blog/posts/RAG/10-Retriever/01-VectorStoreRetriever.html#upstage-임베딩과-같이-query-passage-embedding-model-이-분리된-경우",
    "title": "벡터스토어 기반 검색기(VectorStore-backed Retriever)",
    "section": "2 Upstage 임베딩과 같이 Query & Passage embedding model 이 분리된 경우",
    "text": "2 Upstage 임베딩과 같이 Query & Passage embedding model 이 분리된 경우\n기본 retriever는 쿼리와 문서에 대해 동일한 임베딩 모델을 사용합니다.\n하지만 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용하는 경우가 있습니다.\n이러한 경우에는 쿼리 임베딩 모델을 사용하여 쿼리를 임베딩하고, 문서 임베딩 모델을 사용하여 문서를 임베딩합니다.\n이렇게 하면 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용할 수 있습니다.\n\n\n코드\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_upstage import UpstageEmbeddings\n\n# TextLoader를 사용하여 파일을 로드합니다.\nloader = TextLoader(\"./data/appendix-keywords.txt\")\n\n# 문서를 로드합니다.\ndocuments = loader.load()\n\n# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n\n# 로드된 문서를 분할합니다.\nsplit_docs = text_splitter.split_documents(documents)\n\n# Upstage 임베딩을 생성합니다. 문서용 모델을 사용합니다.\ndoc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n\n# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\ndb = FAISS.from_documents(split_docs, doc_embedder)\n\n\n아래는 쿼리용 Upstage 임베딩을 생성하고, 쿼리 문장을 벡터로 변환하여 벡터 유사도 검색을 수행하는 예시입니다.\n\n\n코드\n# 쿼리용 Upstage 임베딩을 생성합니다. 쿼리용 모델을 사용합니다.\nquery_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n\n# 쿼리 문장을 벡터로 변환합니다.\nquery_vector = query_embedder.embed_query(\"임베딩(Embedding)은 무엇인가요?\")\n\n# 벡터 유사도 검색을 수행하여 가장 유사한 2개의 문서를 반환합니다.\ndb.similarity_search_by_vector(query_vector, k=2)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "MultiVectorRetriever는 단일 문서에 대해 여러 벡터를 생성하여 다양한 관점에서 검색할 수 있도록 도와주는 고급 검색 시스템이다.\n\n\n이름에서 오해를 사수 있는 부분을 명확히 하자. “다중 벡터저장소”는 여러 개의 벡터 데이터베이스를 운영한다는 의미가 아니다. 실제로는 다음 두 가지 데이터 저장소를 함께 운영한다:\n\nDocStore: 원본 문서(규모가 큰 데이터)를 저장하는 공간\n\nVectorStore: 처리된 벡터 임베딩을 저장하는 공간\n\n\n\n\n계층적 맥락 보존 문제\n전통적인 RAG 시스템에서 A 페이지가 1, 2, 3번 chunk로 분할되었을 때:\n- 가장 유사도가 높은 단일 chunk만 반환됨 (예: 2번 chunk)\n- 2, 3번 chunk에도 질문과 유사한 내용이 들어있을 수 있음\n- 문제: 제한된 문맥 정보로 인해 LLM이 부정확한 답변을 생성\nMultiVectorRetriever의 해결책\n단일 chunk만 반환하는 대신, 질문 쿼리의 내용이 모두 담긴 A 페이지 전체를 검색 반환한다. 이를 통해:\n- LLM이 “Lost in the Middle” 리스크를 간접적으로 회피할 수 있음\n- 더 정확하고 포괄적인 답변 생성 가능\n- 맥락을 유지하면서도 세밀한 검색 가능\n\n\n\n\n기능적 유사성\n\n두 Retriever 모두 계층적 문서 구조를 사용하지만, MultiVectorRetriever는 용도적으로 더 세분화된 기능을 제공한다.\n\n둘 다 “작게 검색 → 크게 반환”\n\n검색: 작은 청크로 검색 (정밀도 ↑)\n\n반환: 큰 문서로 반환 (맥락 ↑)\n\n\n구조도 동일:\n\nVectorStore: 검색용 작은 청크\n\nDocStore: 반환용 큰 문서\n\nID 매핑으로 연결\n\n\n핵심 차이: “작은 청크를 어떻게 만드느냐”\n\nMultiVectorRetriever는 ParentDocumentRetriever의 일반화 버전이다.\n\nParentDocumentRetriever\n\n원본 문서를 물리적으로 쪼갬\n\n큰 문서 (Parent) -(분할)→ 작은 청크1, 청크2, 청크3 (Child)\n\nChild는 Parent의 실제 일부분 (계층적 분할): 단순히 크기만 다르게 쪼갬\n\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n\n\nMultiVectorRetriever\n\n원본 문서(Parent)는 그대로 두고, 검색용 “표현(Chunk)”을 별도 생성\n\nchild 생성 방식: 요약본 / 가상질문 / 여러 청크 (내용보전)\n\n큰 문서 (Parent) -(파생)→ 작은 청크1, 청크2, 청크3 (Child)\n\n검색용 표현은 원본과 다른 내용일 수 있음\n\n여러 방법 가능: 요약, 질문 생성, 다중 청크 등\n\nID 기반 유연한 매핑 시스템\n\n문서 A 페이지에 1, 2, 3 chunk가 있을 때, 각 문서와 chunk에 동일한 ID(예: ‘abc’)를 부여:\n\nDocStore: Parent document → ID ’abc’로 저장\n\nVectorStore: Child chunks (1, 2, 3) → 모두 ID ’abc’로 저장\n\n\nID로 parent와 child가 강제로 연결되기 때문에 child 문서를 가공 및 변형해도 parent와 연결시킬 수 있다.\n\n\n\nID 기반 유연한 매핑 시스템을 이용한 MultiVectorRetriever의 유연한 반환 전략\n\nchunk 1, 2, 3 중 하나가 검색되었을 때, ID ’abc’를 key로 사용하여 다음 옵션들을 선택적으로 제어 가능:\n\nParent Document만 반환: 전체 맥락 제공\n\nChild Chunk만 반환: 정밀한 정보만 제공\n\n둘 다 반환: 세밀한 정보 + 전체 맥락 동시 제공\n\n\n\n\n\n\n\n\n\n\n\n\nParentDocumentRetriever\nMultiVectorRetriever\n\n\n\n\n검색용 청크\n원본의 일부 (물리적 분할)\n원본의 파생 표현 (요약/질문 등)\n\n\n청크와 원본 관계\n포함 관계\n독립적 (다른 내용 가능)\n\n\n유연성\n낮음 (분할만)\n높음 (요약/질문/다중 방식)\n\n\n사용 시기\n단순한 계층 구조\n검색 품질 최적화 필요\n\n\n\n\n\nParentDocumentRetriever:\n원본: \"AI는 인공지능이다. 머신러닝을 사용한다. 딥러닝도 포함된다.\"  \n\nChild1: \"AI는 인공지능이다.\"  \nChild2: \"머신러닝을 사용한다.\"  \nChild3: \"딥러닝도 포함된다.\"  \n\n→ Child2로 검색 → 원본 전체 반환  \nMultiVectorRetriever:\n원본: \"AI는 인공지능이다. 머신러닝을 사용한다. 딥러닝도 포함된다.\"  \n\n요약: \"AI와 머신러닝, 딥러닝의 관계 설명\"  \n가상질문: \"AI란 무엇인가? 머신러닝이란?\"  \n\n→ 요약/질문으로 검색 → 원본 반환  \n\n\n\n\n\n\n\n각 chunk의 요약본을 생성하여 VectorStore에 임베딩 데이터로 저장:\n\nSemantic Match 품질 향상: 쿼리와 검색 대상의 추상화 레벨을 맞춰서 semantic match를 개선\n\n질문자 쿼리와 유사한 요약본을 검색\n\n해당 요약본에 대응되는 원본 chunk를 DocStore에서 호출\n\n\n예시\n\n원본 청크: “2023년 11월 8일 삼성전자가 삼성 AI 포럼에서 삼성 가우스를 공개했다. 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, …”\n\n사용자 질문 쿼리: “삼성의 AI 제품이 뭐야?”\n\n문제\n\n원본에는 날짜, 인명, 기술적 세부사항이 섞여있음\n\n임베딩이 이런 잡음에 영향받음\n\n핵심 의미(삼성이 AI 제품을 만들었다)가 희석됨\n\n\n해결: 요약본으로 검색\n\n요약 청크: “삼성전자가 온디바이스 생성 AI 모델 ’삼성 가우스’를 공개했으며, 언어·코드·이미지 3개 모델로 구성되어 있다.”\n\n사용자 질문 쿼리: “삼성의 AI 제품이 뭐야?”\n\n\n\n요약본 청크 생성 이점\n\n검색 품질 향상\n\n의미 밀도 증가: 핵심 정보만 압축되어 있음\n\n노이즈 제거: 불필요한 세부사항 제거됨\n\n상화 레벨 맞춤: 쿼리와 semantic 유사도가 더 높음\n\n\n요약본 청크 생성 단점\n\n비용: LLM으로 요약 생성 필요\n\n시간: 인덱싱 시간 증가\n\n정보손실: 요약 과정에서 중요한 디테일 누락 가능\n\n\n언제 사용?\n\n문서가 길고 세부사항이 많을 때\n\n사용자 질문이 고수준/추상적일 때\n\n검색 품질이 비용보다 중요할 때\n\n\n언제 비추천?\n\n정확한 수치/날짜가 중요할 때\n\n문서가 이미 간결할 때\n\n실시간 검색 속도가 중요할 때\n\n\n\n\n\nLangChain에서는 문서를 다양한 상황에서 효율적으로 쿼리할 수 있는 MultiVectorRetriever를 제공한다. 이 기능을 사용하면 문서를 여러 벡터로 저장하고 관리할 수 있어, 정보 검색의 정확도와 효율성을 대폭 향상시킬 수 있다.\n문서당 여러 벡터 생성 방법:\n\n작은 청크 생성: 문서를 더 작은 단위로 나눈 후, 각 청크에 대해 별도의 임베딩을 생성한다. 이 방식을 사용하면 문서의 특정 부분에 좋 더 세심한 주의를 기울일 수 있다. 이 과정은 ParentDocumentRetriever를 통해 구현할 수 있어, 세부 정보에 대한 탐색이 용이해진다.\n요약 임베딩: 각 문서의 요약을 생성하고, 이 요약으로부터 임베딩을 만듭니다. 이 요약 임베딩은 문서의 핵심 내용을 신속하게 파악하는 데 큰 도움이 된다. 문서 전체를 분석하는 대신 핵심적인 요약 부분만을 활용하여 효율성을 극대화할 수 있다.\n가설 질문 활용: 각 문서에 대해 적합한 가설 질문을 만들고, 이 질문에 기반한 임베딩을 생성한다. 특정 주제나 내용에 대해 깊이 있는 탐색을 원할 때 이 방법이 유용하다. 가설 질문은 문서의 내용을 다양한 관점에서 접근하게 해주며, 더 광범위한 이해를 가능하게 한다.\n수동 추가 방식: 사용자가 문서 검색 시 고려해야 할 특정 질문이나 쿼리를 직접 추가할 수 있다. 이 방법을 통해 사용자는 검색 과정에서 보다 세밀한 제어를 할 수 있으며, 자신의 요구 사항에 맞춘 맞춤형 검색이 가능해진다.\n\n\n\n\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n\n링크: https://spri.kr/posts/view/23669\n\n파일명: SPRI_AI_Brief_2023년12월호_F.pdf\n\n참고: 위의 파일은 data 폴더 내에 다운로드 받으세요\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일  \nfrom dotenv import load_dotenv  \n\n# API 키 정보 로드  \nload_dotenv()  \n\n\n\n\n코드\n# LangSmith 추적을 설정한다. https://smith.langchain.com  \n# !pip install langchain-teddynote  \nfrom langchain_teddynote import logging  \n\n# 프로젝트 이름을 입력한다.  \nlogging.langsmith(\"CH10-Retriever\")  \n\n\n텍스트 파일에서 데이터를 로드하고, 로드된 문서들을 지정된 크기로 분할하는 전처리 과정을 수행한다.\n분할된 문서들은 추후 벡터화 및 검색 등의 작업에 사용될 수 있다.\n\n\n코드\nfrom langchain_community.document_loaders import PyMuPDFLoader  \n\nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")  \ndocs = loader.load()  \nprint(len(docs))  \n\n\n데이터로부터 로드한 원본 도큐먼트는 docs 변수에 담았다.\n23  \n\n\n코드\nprint(docs[5].page_content[:500])  \n\n\n1. 정책/법제  \n2. 기업/산업   \n3. 기술/연구   \n 4. 인력/교육  \n영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언  \nn 영국 블레츨리 파크에서 개최된 AI 안전성 정상회의에 참가한 28개국들이 AI 안전 보장을   \n위한 협력 방안을 담은 블레츨리 선언을 발표  \nn 첨단 AI를 개발하는 국가와 기업들은 AI 시스템에 대한 안전 테스트 계획에 합의했으며,   \n영국의 AI 안전 연구소가 전 세계 국가와 협력해 테스트를 주도할 예정   \nKEY Contents  \n£ AI 안전성 정상회의 참가국들, 블레츨리 선언 통해 AI 안전 보장을 위한 협력에 합의  \nn 2023년 11월 1~2일 영국 블레츨리 파크에서 열린 AI 안전성 정상회의(AI Safety Summit)에   \n참가한 28개국 대표들이 AI 위험 관리를 위한 ‘블레츨리 선언’을 발표   \n∙선언은 AI 안전 보장을 위해 국가, 국제기구, 기업, 시민사회, 학계를 포함한 모든 이해관계자의 협력이   \n중요하다고 강조했으며,  \n\npdf가 23페이지로 구성되어 있어 23이 반환됨 pdf loader는 패이지 단위로 로드가 된다.\n\n\n\n\n\n대용량 정보를 검색하는 경우, 더 작은 단위로 정보를 임베딩하는 것이 유용할 수 있다.\n\nMultiVectorRetriever를 통해 문서를 여러 벡터로 저장하고 관리할 수 있다.\n\ndocstore에 원본 문서를 저장하고, vectorstore에 임베딩된 문서를 저장한다.\n\n이로써 문서를 더 작은 단위로 나누어 더 정확한 검색이 가능해진다. 때에 따라서는 원본 문서의 내용을 조회할 수 있다.\n\n\n코드\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소  \nimport uuid  \nfrom langchain.storage import InMemoryStore  \nfrom langchain_chroma import Chroma  \nfrom langchain_openai import OpenAIEmbeddings  \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter  \nfrom langchain.retrievers.multi_vector import MultiVectorRetriever  \n\nvectorstore = Chroma(  \n    collection_name=\"small_bigger_chunks\",  \n    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),  \n)  \n# 부모 문서의 저장소 계층  \nstore = InMemoryStore()  \n\nid_key = \"doc_id\"  \n\n# 검색기 (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=vectorstore,  \n    byte_store=store,  \n    id_key=id_key,  \n)  \n\n# 문서 ID를 생성합니다.  \ndoc_ids = [str(uuid.uuid4()) for _ in docs]  \n\n# 두개의 생성된 id를 확인한다.  \ndoc_ids  \n\n\n['ff65f8d8-376d-47d2-bbc0-eae1f7e3c381', 'e18fd86b-f01a-445f-bf31-eb8d16259241', 'f099589b-066f-4827-9679-3bf2d2ec28d2', '834e63d6-0dd9-4bfb-8383-ca6a06d731ef', '5a33bffb-ae85-402e-a4bf-81ab376d7744', '319396d0-cc7d-41f9-826c-71bccc8dade6', '9a38995c-94b7-4963-9605-5d27f868b5e7', 'bc32292c-0d19-452f-b970-1074c43fc864', '48de377c-4693-407b-93e6-3f0ea44b5009', '9e27fff9-15d3-4754-bb26-00779158d528', 'd73a5aef-251b-4a30-aadb-2dc973ef1607', '56535934-2a23-4e3e-b6d2-79121dd93005', 'a535c103-0cf8-4c46-b5ea-90c34aad90bc', '48aac7ed-f704-4d65-90bd-3b5b18adc127', '63b42edf-3546-4257-9046-ff3f4687a217', '749f6239-fec3-4893-8959-f641d6d551b3', '4a08aa0e-4e38-460d-90dc-34e3faca6784', '91cfa69e-7ca7-44f9-8795-bc710622af17', '817e7519-3853-450c-bb05-b3f6763d9cd4', '00e3e534-762a-477b-adf4-3d1dbe67b656', '03171c52-bf3d-4d27-bf93-bb2da4ba6d87', 'df37aeb5-0bfd-4fb1-960d-aad8a5ad11cd', '3cf6570c-25c8-4b28-82ec-6804026e844e']  \n\n총 23개 (parent document 수)의 uid가 생성됨\n\n여기서 큰 청크로 분할하기 위한 parent_text_splitter\n\n더 작은 청크로 분할하기 위한 child_text_splitter를 정의한다.\n\n\n\n코드\n# RecursiveCharacterTextSplitter 객체를 생성한다.  \nparent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=600)  \n\n# 더 작은 청크를 생성하는 데 사용할 분할기  \nchild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=200)  \n\n\n더 큰 Chunk인 Parent 문서를 생성한다.\n\n\n코드\nparent_docs = []  \n\nfor i, doc in enumerate(docs):  \n    # 현재 문서의 ID를 가져옵니다.  \n    _id = doc_ids[i]  \n    # 현재 문서를 하위 문서로 분할  \n    parent_doc = parent_text_splitter.split_documents([doc])  \n\n    for _doc in parent_doc:  \n        # metadata에 문서 ID 를 저장  \n        _doc.metadata[id_key] = _id  \n    parent_docs.extend(parent_doc)  \n\n\n\nparent_docs에 기입된 doc_id를 확인한다.\n\ndoc에는 parent doc(page단위 내용)이 담기고\n\nparent_docs에는 split chunk들이 담긴다.\n\n위의 코드 청크가 parent doc과 split chunk들에게 동일한 id를 부여하는 기능을 한다.\n\n\n\n코드\nchild_docs = []  \nfor i, doc in enumerate(docs):  \n    # 현재 문서의 ID를 가져옵니다.  \n    _id = doc_ids[i]  \n    # 현재 문서를 하위 문서로 분할  \n    child_doc = child_text_splitter.split_documents([doc])  \n    for _doc in child_doc:  \n        # metadata에 문서 ID 를 저장  \n        _doc.metadata[id_key] = _id  \n    child_docs.extend(child_doc)  \n\n\n\nchild_docs에 기입된 doc_id를 확인한다.\n\n상대적으로 더 작은 Chunk인 Child 문서를 생성한다.\n\n\n\n코드\n# 생성된 Child 문서의 메타데이터를 확인한다.  \nchild_docs[0].metadata  \n\n\n{‘source’: ‘data/SPRI_AI_Brief_2023년12월호_F.pdf’, ‘file_path’: ‘data/SPRI_AI_Brief_2023년12월호_F.pdf’, ‘page’: 0, ‘total_pages’: 23, ‘format’: ‘PDF 1.4’, ‘title’: ’‘, ’author’: ‘dj’, ‘subject’: ’‘, ’keywords’: ’‘, ’creator’: ‘Hwp 2018 10.0.0.13462’, ‘producer’: ‘Hancom PDF 1.3.0.542’, ‘creationDate’: “D:20231208132838+09’00’”, ‘modDate’: “D:20231208132838+09’00’”, ‘trapped’: ’‘, ’doc_id’: ‘ff65f8d8-376d-47d2-bbc0-eae1f7e3c381’}\n각각 분할된 청크의 수를 확인한다.\n\n\n코드\nprint(f\"분할된 parent_docs의 개수: {len(parent_docs)}\")  \nprint(f\"분할된 child_docs의 개수: {len(child_docs)}\")  \n\n\n분할된 parent_docs의 개수: 74  \n분할된 child_docs의 개수: 443  \n\n보통 표준적인 MultiVectorRetriever 사용법\n\n# 정상적인 방법  \nretriever.vectorstore.add_documents(child_docs)   # 검색용: 작은 청크만  \nretriever.docstore.mset(zip(doc_ids, parent_docs)) # 반환용: 큰 청크  \n\n# VectorStore: child_docs (443개) - 검색에 사용  \n# DocStore: parent_docs (74개) - 반환에 사용  \n\n하지만, 더 유연하게 구성을 가져갈 수도 있다.\n\n벡터저장소엔 parent_docs와 child_docs를 저장하고\n\ndocstore엔 문서 원본을 저장한다.\n\n벡터저장소에 새롭게 생성한 작게 쪼개진 하위문서 집합을 추가한다.\n\n다음으로는 상위 문서는 생성한 UUID와 맵핑하여 docstore에 추가한다.\n\nmset() 메소드를 통해 문서 ID와 문서 내용을 key-value 쌍으로 문서 저장소에 저장한다.\n\n\n\n\n코드\n# 벡터 저장소에 parent + child 문서를 추가  \nretriever.vectorstore.add_documents(parent_docs)  \nretriever.vectorstore.add_documents(child_docs)  \n\n# docstore 에 원본 문서를 저장  \nretriever.docstore.mset(list(zip(doc_ids, docs)))  \n\n\n유사도 검색을 수행한다. 가장 유사도가 높은 첫 번째 문서 조각을 출력한다.\n여기서 retriever.vectorstore.similarity_search 메소드는 child + parent 문서 chunk 내에서 검색을 수행한다.\n\n\n코드\n# vectorstore의 유사도 검색을 수행한다.   \nrelevant_chunks = retriever.vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \nprint(f\"검색된 문서의 개수: {len(relevant_chunks)}\")  \n\n\n검색된 문서의 개수: 4  \n\n잘게 쪼개진 chunk들에서 검색을 하고 싶으면 retriever.vectorstore 에서 유사도 검색 수행\n\n\n\n코드\nfor chunk in relevant_chunks:  \n    print(chunk.page_content, end=\"\\n\\n\")  \n    print(\"&gt;\" * 100, end=\"\\n\\n\")  \n\n\n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\n▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··························································· 10  \n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n이번에는 retriever.invoke() 메소드를 사용하여 쿼리를 실행한다.\nretriever.invoke() 메소드는 원본 문서의 전체 내용을 검색한다.\n\n잘게 쪼개진 chunk들의 parent_doc에서 검색을 하고 싶으면 retriever 에서 유사도 검색 수행\n\nparent와 child는 상대적인 개념으로 parent가 반드시 가공전 원본 문서를 의미하지는 않는다.\n\n\n\n코드\nrelevant_docs = retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")  \nprint(f\"검색된 문서의 개수: {len(relevant_docs)}\", end=\"\\n\\n\")  \nprint(\"=\" * 100, end=\"\\n\\n\")  \nprint(relevant_docs[0].page_content)  \n\n\n검색된 문서의 개수: 2  \n\n====================================================================================================  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n리트리버(retriever)가 벡터 데이터베이스에서 기본적으로 수행하는 검색 유형은 유사도 검색이다.\n\nLangChain Vector Stores는 Max Marginal Relevance를 통한 검색도 지원하므로, 이를 대신 사용하고 싶다면 다음과 같이 search_type 속성을 설정하면 된다.\n\nretriever 객체의 search_type 속성을 SearchType.mmr로 설정한다.\n\n이는 검색 시 MMR(Maximal Marginal Relevance) 알고리즘을 사용하도록 지정하는 것\n\n\n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 MMR(Maximal Marginal Relevance)로 설정  \nretriever.search_type = SearchType.mmr  \n\n# 관련 문서 전체를 검색  \nprint(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 similarity_score_threshold로 설정  \nretriever.search_type = SearchType.similarity_score_threshold  \nretriever.search_kwargs = {\"score_threshold\": 0.3}  \n\n# 관련 문서 전체를 검색  \nprint(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 similarity로 설정, k값을 1로 설정  \nretriever.search_type = SearchType.similarity  \nretriever.search_kwargs = {\"k\": 1}  \n\n# 관련 문서 전체를 검색  \nprint(len(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")))  \n\n\n1  \n\n\n\n\n요약은 종종 청크(chunk)의 내용을 보다 정확하게 추출할 수 있어 더 나은 검색 결과를 얻을 수 있다.\n\n요약은 핵심 내용만을 담고 있어 LLM의 할루시네이션을 방지할 수 있다.\n\n여기서는 요약을 생성하는 방법과 이를 임베딩하는 방법에 대해 설명한다.\n\n\n\n코드\n# PDF 파일을 로드하고 텍스트를 분할하기 위한 라이브러리 임포트  \nfrom langchain_community.document_loaders import PyMuPDFLoader  \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter  \n\n# PDF 파일 로더 초기화  \nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")  \n\n# 텍스트 분할  \ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)  \n\n# PDF 파일 로드 및 텍스트 분할 실행  \nsplit_docs = loader.load_and_split(text_splitter)  \n\n# 분할된 문서의 개수 출력  \nprint(f\"분할된 문서의 개수: {len(split_docs)}\")  \n\n\n분할된 문서의 개수: 61  \n\n\n코드\nfrom langchain_core.documents import Document  \nfrom langchain_core.output_parsers import StrOutputParser  \nfrom langchain_core.prompts import ChatPromptTemplate  \nfrom langchain_openai import ChatOpenAI  \n\n\nsummary_chain = (  \n    {\"doc\": lambda x: x.page_content}  \n    # 문서 요약을 위한 프롬프트 템플릿 생성  \n    | ChatPromptTemplate.from_messages(  \n        [  \n            (\"system\", \"You are an expert in summarizing documents in Korean.\"),  \n            (  \n                \"user\",  \n                \"Summarize the following documents in 3 sentences in bullet points format.\\n\\n{doc}\",  \n            ),  \n        ]  \n    )  \n    # OpenAI의 ChatGPT 모델을 사용하여 요약 생성  \n    | ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  \n    | StrOutputParser()  \n)  \n\n\n\nchain.batch 메소드를 사용하여 docs 리스트의 문서들을 일괄 요약한다.\n\n여기서 max_concurrency 매개변수를 10으로 설정하여 최대 10개의 문서를 동시에 처리할 수 있도록 한다.\n\n\n\n코드\n# 문서 배치 처리  \nsummaries = summary_chain.batch(split_docs, {\"max_concurrency\": 10})  \n\n\n\n\n코드\nlen(summaries)  \n\n\n61  \n\n61개의 요약본이 생김\n\n요약된 내용을 출력하여 결과를 확인한다.\n\n\n\n코드\n# 원본 문서의 내용을 출력한다.  \nprint(split_docs[33].page_content, end=\"\\n\\n\")  \n# 요약을 출력한다.  \nprint(\"[요약]\")  \nprint(summaries[33])  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에  \n\n[요약]  \n- 삼성전자가 온디바이스에서 작동 가능한 생성 AI 모델 '삼성 가우스'를 공개하였으며, 이 모델은 언어, 코드, 이미지의 3개 모델로 구성되어 있다.  \n- '삼성 가우스'는 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, 다양한 상황에 최적화된 모델 선택이 가능하다.  \n- 삼성전자는 이 AI 모델이 사용자 정보를 외부로 유출하지 않도록 설계되었으며, 향후 다양한 제품에 단계적으로 탑재할 계획이다.  \n\nChroma 벡터 저장소를 초기화하여 자식 청크(child chunks)를 인덱싱한다. 이때 OpenAIEmbeddings를 임베딩 함수로 사용한다.\n\n문서 ID를 나타내는 키로 \"doc_id\"를 사용한다.\n\n\n\n코드\nimport uuid  \n\n# 요약 정보를 저장할 벡터 저장소를 생성한다.  \nsummary_vectorstore = Chroma(  \n    collection_name=\"summaries\",  \n    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),  \n)  \n\n# 부모 문서를 저장할 저장소를 생성한다.  \nstore = InMemoryStore()  \n\n# 문서 ID를 저장할 키 이름을 지정한다.  \nid_key = \"doc_id\"  \n\n# 검색기를 초기화한다. (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=summary_vectorstore,  # 벡터 저장소  \n    byte_store=store,  # 바이트 저장소  \n    id_key=id_key,  # 문서 ID 키  \n)  \n# 문서 ID를 생성합니다.  \ndoc_ids = [str(uuid.uuid4()) for _ in split_docs]  \n\n\n요약된 문서와 메타데이터(여기서는 생성한 요약본에 대한 Document ID이다)를 저장한다.\n\n\n코드\nsummary_docs = [  \n    # 요약된 내용을 페이지 콘텐츠로 하고, 문서 ID를 메타데이터로 포함하는 Document 객체를 생성한다.  \n    Document(page_content=s, metadata={id_key: doc_ids[i]})  \n    for i, s in enumerate(summaries)  \n]  \n\n\n요약본의 문서의 개수는 원본 문서의 개수와 일치한다.\n\n\n코드\n# 요약본의 문서의 개수  \nlen(summary_docs)  \n\n\n61  \n\nretriever.vectorstore.add_documents(summary_docs)를 통해 summary_docs를 벡터 저장소에 추가한다.\n\nretriever.docstore.mset(list(zip(doc_ids, docs)))를 사용하여 doc_ids와 docs를 매핑하여 문서 저장소에 저장한다.\n\n\n\n코드\nretriever.vectorstore.add_documents(  \n    summary_docs  \n)  # 요약된 문서를 벡터 저장소에 추가한다.  \n\n# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.  \nretriever.docstore.mset(list(zip(doc_ids, split_docs)))  \n\n\nvectorstore 객체의 similarity_search 메소드를 사용하여 유사도 검색을 수행한다.\n\n\n코드\n# 유사도 검색을 수행한다.  \nresult_docs = summary_vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \n\n\n\n\n코드\n# 1개의 결과 문서를 출력한다. (요약본)  \nprint(result_docs[0].page_content)  \n\n\n- 삼성전자가 온디바이스에서 작동 가능한 생성 AI 모델 '삼성 가우스'를 공개하였으며, 이 모델은 언어, 코드, 이미지의 3개 모델로 구성되어 있다.  \n- '삼성 가우스'는 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, 다양한 상황에 최적화된 모델 선택이 가능하다.  \n- 삼성전자는 이 AI 모델이 사용자 정보를 외부로 유출하지 않도록 설계되었으며, 향후 다양한 제품에 단계적으로 탑재할 계획이다.  \nretriever 객체의 invoke() 사용하여 질문과 관련된 문서를 검색한다.\n\n\n코드\n# 관련된 원본 문서를 검색하여 가져옵니다.  \nretrieved_docs = retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")  \nprint(retrieved_docs[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에  \n\n\n\n\n\n사용자가 물어볼 만한 질문을 미리 생성해서 검색 인덱스로 활용하자는 전략이다.\n\n\n\n1. 짧은 텍스트 간 매칭의 우수성\n일반적으로 다음과 같은 검색 성능 순서가 관찰된다:\n- 질문 ↔︎ 질문 (최고) &gt; 질문 ↔︎ 문단 &gt; 질문 ↔︎ 긴 문서 (최저)\n2. 임베딩 정보 손실 문제\n긴 문서를 임베딩할 때:\n1. 문서 → 임베딩 과정에서 1차 정보 손실\n2. 검색 매칭 과정에서 2차 정보 손실\n따라서 임베딩 대상을 짧게 유지하는 것이 정보 손실을 최소화한다.\n3. 가설 질문 방식의 장점\n전통적 방식:  \n사용자 질문: \"삼성의 AI 제품은?\"  \n    ↓ (의미 공간에서 멀리 떨어짐)  \n원본 문서: \"2023년 11월 8일 삼성전자가 삼성 AI 포럼에서...\"  \n\n가설 질문 방식:  \n사용자 질문: \"삼성의 AI 제품은?\"  \n    ↓ (의미 공간에서 매우 가까움)  \n미리 생성한 질문: \"삼성전자가 개발한 AI 기술은?\"  \n    ↓ (ID로 연결)  \n원본 문서: \"2023년 11월 8일 삼성전자가...\"  \n질문 ↔︎ 질문 매칭이므로 의미적 유사도가 훨씬 높다.\n\n\n\n\n문서마다 가상 질문 생성 (LLM 활용)\n\n문서 내용을 읽고 “이 문서로 답할 수 있는 질문”을 3~5개 생성\n\n질문을 VectorStore에 저장\n\n원본 문서는 DocStore에 보관\n\n가상 질문들만 임베딩하여 검색 대상으로 활용\n\n검색 시\n\n사용자 질문 → 가상 질문들과 유사도 검색\n\n매칭된 가상 질문의 ID로 원본 문서 반환\n\n\n\n\n\n장점:\n- 검색 정확도 향상: 질문-질문 매칭의 높은 의미 유사도\n- 다양한 관점 포착: 한 문서에 대해 여러 각도의 질문 생성 가능\n- 사용자 의도 파악 개선: 실제 사용자 질문 패턴과 유사\n단점:\n- 비용 증가: LLM으로 질문 생성 필요 (문서당 3~5개 질문)\n- 시간 소요: 인덱싱 시간 증가\n- 질문 품질 의존: LLM이 생성한 질문의 품질에 성능 좌우\n언제 사용할까?\n- 사용자 질문 패턴이 명확할 때 (FAQ, 고객 지원 등)\n- 검색 품질이 매우 중요할 때\n- 인덱싱 비용보다 검색 성능이 우선일 때\n언제 비추천?\n- 문서가 자주 변경될 때 (재생성 비용)\n- 실시간 인덱싱이 필요할 때\n- 비용이 제약 조건일 때\n\n\n\n아래는 Function Calling을 활용하여 가설 질문을 생성하는 예제다.\n\n\n코드\nfunctions = [  \n    {  \n        \"name\": \"hypothetical_questions\",  # 함수의 이름을 지정한다.  \n        \"description\": \"Generate hypothetical questions\",  # 함수에 대한 설명을 작성한다.  \n        \"parameters\": {  # 함수의 매개변수를 정의한다.  \n            \"type\": \"object\",  # 매개변수의 타입을 객체로 지정한다.  \n            \"properties\": {  # 객체의 속성을 정의한다.  \n                \"questions\": {  # 'questions' 속성을 정의한다.  \n                    \"type\": \"array\",  # 'questions'의 타입을 배열로 지정합니다.  \n                    \"items\": {  \n                        \"type\": \"string\"  \n                    },  # 배열의 요소 타입을 문자열로 지정합니다.  \n                },  \n            },  \n            \"required\": [\"questions\"],  # 필수 매개변수로 'questions'를 지정합니다.  \n        },  \n    }  \n]  \n\n\nChatPromptTemplate을 사용하여 주어진 문서를 기반으로 3개의 가상 질문을 생성하는 프롬프트 템플릿을 정의한다.\n\nfunctions와 function_call을 설정하여 가상 질문 생성 함수를 호출한다.\n\nJsonKeyOutputFunctionsParser를 사용하여 생성된 가상 질문을 파싱하고, questions 키에 해당하는 값을 추출한다.\n\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate  \nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser  \nfrom langchain_openai import ChatOpenAI  \n\nhypothetical_query_chain = (  \n    {\"doc\": lambda x: x.page_content}  \n    # 아래 문서를 사용하여 답변할 수 있는 가상의 질문을 정확히 3개 생성하도록 요청합니다. 이 숫자는 조정될 수 있습니다.  \n    | ChatPromptTemplate.from_template(  \n        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer. \"  \n        \"Potential users are those interested in the AI industry. Create questions that they would be interested in. \"  \n        \"Output should be written in Korean:\\n\\n{doc}\"  \n    )  \n    | ChatOpenAI(max_retries=0, model=\"gpt-4o-mini\").bind(  \n        functions=functions, function_call={\"name\": \"hypothetical_questions\"}  \n    )  \n    # 출력에서 \"questions\" 키에 해당하는 값을 추출합니다.  \n    | JsonKeyOutputFunctionsParser(key_name=\"questions\")  \n)  \n\n\n문서에 대한 답변을 출력\n- 출력은 생성한 3개의 가설 쿼리(Hypothetical Queries) 가 담겨 있다.\n\n\n코드\n# 주어진 문서에 대해 체인을 실행합니다.  \nhypothetical_query_chain.invoke(split_docs[33])  \n\n\n['삼성 가우스가 다른 생성 AI 모델에 비해 어떤 경쟁력을 가질 수 있을까?', '온디바이스 작동이 가능한 삼성 가우스가 개인 정보 보호에 미치는 영향은 무엇일까?', '삼성전자가 삼성 가우스를 다양한 제품에 단계적으로 탑재하려는 이유는 무엇일까?']  \nchain.batch 메소드를 사용하여 split_docs 데이터에 대해 동시에 여러 개의 요청을 처리한다.\n\n\n코드\n# 문서 목록에 대해 가설 질문을 배치 생성  \nhypothetical_questions = hypothetical_query_chain.batch(  \n    split_docs, {\"max_concurrency\": 10}  \n)  \n\n\n\n\n코드\nhypothetical_questions[33]  \n\n\n['삼성 가우스가 다른 생성 AI 모델과 비교했을 때, 사용자 개인 정보 보호에 있어 어떤 장점을 제공할까요?', '삼성전자가 삼성 가우스를 다양한 제품에 탑재하기로 결정한 이유는 무엇일까요?', '온디바이스에서 작동하는 삼성 가우스의 기능이 AI 산업의 미래에 어떤 영향을 미칠까요?']  \n생성한 가설 쿼리(Hypothetical Queries)를 벡터저장소에 저장하는 과정이다.\n\n\n코드\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소  \nhypothetical_vectorstore = Chroma(  \n    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()  \n)  \n# 부모 문서의 저장소 계층  \nstore = InMemoryStore()  \n\nid_key = \"doc_id\"  \n# 검색기 (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=hypothetical_vectorstore,  \n    byte_store=store,  \n    id_key=id_key,  \n)  \ndoc_ids = [str(uuid.uuid4()) for _ in split_docs]  # 문서 ID 생성  \n\n\nquestion_docs 리스트에 메타데이터(문서 ID)를 추가한다.\n\n\n코드\nquestion_docs = []  \n# hypothetical_questions 저장  \nfor i, question_list in enumerate(hypothetical_questions):  \n    question_docs.extend(  \n        # 질문 리스트의 각 질문에 대해 Document 객체를 생성하고, 메타데이터에 해당 질문의 문서 ID를 포함시킵니다.  \n        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]  \n    )  \n\n\n가설 쿼리를 문서에 추가하고, 원본 문서를 docstore에 추가한다.\n\n\n코드\n# hypothetical_questions 문서를 벡터 저장소에 추가합니다.  \nretriever.vectorstore.add_documents(question_docs)  \n\n# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.  \nretriever.docstore.mset(list(zip(doc_ids, split_docs)))  \n\n\nvectorstore 객체의 similarity_search 메소드를 사용하여 유사도 검색을 수행한다.\n\n\n코드\n# 유사한 문서를 벡터 저장소에서 검색합니다.  \nresult_docs = hypothetical_vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \n\n\n아래는 유사도 검색 결과를 확인한다.\n여기서는 생성한 가설 쿼리만 추가해 놓은 상태이므로, 생성한 가설 쿼리 중 유사도가 가장 높은 문서를 반환한다.\n\n\n코드\n# 유사도 검색 결과를 출력한다.  \nfor doc in result_docs:  \n    print(doc.page_content)  \n    print(doc.metadata)  \n\n\n삼성전자가 AI 기술을 활용하여 개발한 새로운 제품이나 서비스는 무엇일까요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n삼성의 Generative AI 기술이 향후 AI 시장에서 어떤 경쟁력을 가질 것으로 예상되나요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n삼성 개발자 콘퍼런스 코리아 2023에서 발표된 내용이 AI 산업에 어떤 영향을 미칠까요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n온디바이스에서 작동하는 삼성 가우스의 기능이 AI 산업의 미래에 어떤 영향을 미칠까요?  \n{'doc_id': 'b7e88e61-8c28-400e-a4fb-f6bd25e5e40b'}  \nretriever 객체의 invoke 메소드를 사용하여 쿼리와 관련된 문서를 검색한다.\n\n\n코드\n# 관련된 문서를 검색하여 가져옵니다.  \nretrieved_docs = retriever.invoke(result_docs[1].page_content)  \n\n# 검색된 문서를 출력합니다.  \nfor doc in retrieved_docs:  \n    print(doc.page_content)  \n\n\n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#핵심-개념",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#핵심-개념",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "MultiVectorRetriever는 단일 문서에 대해 여러 벡터를 생성하여 다양한 관점에서 검색할 수 있도록 도와주는 고급 검색 시스템이다.\n\n\n이름에서 오해를 사수 있는 부분을 명확히 하자. “다중 벡터저장소”는 여러 개의 벡터 데이터베이스를 운영한다는 의미가 아니다. 실제로는 다음 두 가지 데이터 저장소를 함께 운영한다:\n\nDocStore: 원본 문서(규모가 큰 데이터)를 저장하는 공간\n\nVectorStore: 처리된 벡터 임베딩을 저장하는 공간\n\n\n\n\n계층적 맥락 보존 문제\n전통적인 RAG 시스템에서 A 페이지가 1, 2, 3번 chunk로 분할되었을 때:\n- 가장 유사도가 높은 단일 chunk만 반환됨 (예: 2번 chunk)\n- 2, 3번 chunk에도 질문과 유사한 내용이 들어있을 수 있음\n- 문제: 제한된 문맥 정보로 인해 LLM이 부정확한 답변을 생성\nMultiVectorRetriever의 해결책\n단일 chunk만 반환하는 대신, 질문 쿼리의 내용이 모두 담긴 A 페이지 전체를 검색 반환한다. 이를 통해:\n- LLM이 “Lost in the Middle” 리스크를 간접적으로 회피할 수 있음\n- 더 정확하고 포괄적인 답변 생성 가능\n- 맥락을 유지하면서도 세밀한 검색 가능"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#parentdocumentretriever와의-차이점",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#parentdocumentretriever와의-차이점",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "기능적 유사성\n\n두 Retriever 모두 계층적 문서 구조를 사용하지만, MultiVectorRetriever는 용도적으로 더 세분화된 기능을 제공한다.\n\n둘 다 “작게 검색 → 크게 반환”\n\n검색: 작은 청크로 검색 (정밀도 ↑)\n\n반환: 큰 문서로 반환 (맥락 ↑)\n\n\n구조도 동일:\n\nVectorStore: 검색용 작은 청크\n\nDocStore: 반환용 큰 문서\n\nID 매핑으로 연결\n\n\n핵심 차이: “작은 청크를 어떻게 만드느냐”\n\nMultiVectorRetriever는 ParentDocumentRetriever의 일반화 버전이다.\n\nParentDocumentRetriever\n\n원본 문서를 물리적으로 쪼갬\n\n큰 문서 (Parent) -(분할)→ 작은 청크1, 청크2, 청크3 (Child)\n\nChild는 Parent의 실제 일부분 (계층적 분할): 단순히 크기만 다르게 쪼갬\n\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n\n\nMultiVectorRetriever\n\n원본 문서(Parent)는 그대로 두고, 검색용 “표현(Chunk)”을 별도 생성\n\nchild 생성 방식: 요약본 / 가상질문 / 여러 청크 (내용보전)\n\n큰 문서 (Parent) -(파생)→ 작은 청크1, 청크2, 청크3 (Child)\n\n검색용 표현은 원본과 다른 내용일 수 있음\n\n여러 방법 가능: 요약, 질문 생성, 다중 청크 등\n\nID 기반 유연한 매핑 시스템\n\n문서 A 페이지에 1, 2, 3 chunk가 있을 때, 각 문서와 chunk에 동일한 ID(예: ‘abc’)를 부여:\n\nDocStore: Parent document → ID ’abc’로 저장\n\nVectorStore: Child chunks (1, 2, 3) → 모두 ID ’abc’로 저장\n\n\nID로 parent와 child가 강제로 연결되기 때문에 child 문서를 가공 및 변형해도 parent와 연결시킬 수 있다.\n\n\n\nID 기반 유연한 매핑 시스템을 이용한 MultiVectorRetriever의 유연한 반환 전략\n\nchunk 1, 2, 3 중 하나가 검색되었을 때, ID ’abc’를 key로 사용하여 다음 옵션들을 선택적으로 제어 가능:\n\nParent Document만 반환: 전체 맥락 제공\n\nChild Chunk만 반환: 정밀한 정보만 제공\n\n둘 다 반환: 세밀한 정보 + 전체 맥락 동시 제공\n\n\n\n\n\n\n\n\n\n\n\n\nParentDocumentRetriever\nMultiVectorRetriever\n\n\n\n\n검색용 청크\n원본의 일부 (물리적 분할)\n원본의 파생 표현 (요약/질문 등)\n\n\n청크와 원본 관계\n포함 관계\n독립적 (다른 내용 가능)\n\n\n유연성\n낮음 (분할만)\n높음 (요약/질문/다중 방식)\n\n\n사용 시기\n단순한 계층 구조\n검색 품질 최적화 필요\n\n\n\n\n\nParentDocumentRetriever:\n원본: \"AI는 인공지능이다. 머신러닝을 사용한다. 딥러닝도 포함된다.\"  \n\nChild1: \"AI는 인공지능이다.\"  \nChild2: \"머신러닝을 사용한다.\"  \nChild3: \"딥러닝도 포함된다.\"  \n\n→ Child2로 검색 → 원본 전체 반환  \nMultiVectorRetriever:\n원본: \"AI는 인공지능이다. 머신러닝을 사용한다. 딥러닝도 포함된다.\"  \n\n요약: \"AI와 머신러닝, 딥러닝의 관계 설명\"  \n가상질문: \"AI란 무엇인가? 머신러닝이란?\"  \n\n→ 요약/질문으로 검색 → 원본 반환"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#고급-활용-전략",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#고급-활용-전략",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "각 chunk의 요약본을 생성하여 VectorStore에 임베딩 데이터로 저장:\n\nSemantic Match 품질 향상: 쿼리와 검색 대상의 추상화 레벨을 맞춰서 semantic match를 개선\n\n질문자 쿼리와 유사한 요약본을 검색\n\n해당 요약본에 대응되는 원본 chunk를 DocStore에서 호출\n\n\n예시\n\n원본 청크: “2023년 11월 8일 삼성전자가 삼성 AI 포럼에서 삼성 가우스를 공개했다. 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, …”\n\n사용자 질문 쿼리: “삼성의 AI 제품이 뭐야?”\n\n문제\n\n원본에는 날짜, 인명, 기술적 세부사항이 섞여있음\n\n임베딩이 이런 잡음에 영향받음\n\n핵심 의미(삼성이 AI 제품을 만들었다)가 희석됨\n\n\n해결: 요약본으로 검색\n\n요약 청크: “삼성전자가 온디바이스 생성 AI 모델 ’삼성 가우스’를 공개했으며, 언어·코드·이미지 3개 모델로 구성되어 있다.”\n\n사용자 질문 쿼리: “삼성의 AI 제품이 뭐야?”\n\n\n\n요약본 청크 생성 이점\n\n검색 품질 향상\n\n의미 밀도 증가: 핵심 정보만 압축되어 있음\n\n노이즈 제거: 불필요한 세부사항 제거됨\n\n상화 레벨 맞춤: 쿼리와 semantic 유사도가 더 높음\n\n\n요약본 청크 생성 단점\n\n비용: LLM으로 요약 생성 필요\n\n시간: 인덱싱 시간 증가\n\n정보손실: 요약 과정에서 중요한 디테일 누락 가능\n\n\n언제 사용?\n\n문서가 길고 세부사항이 많을 때\n\n사용자 질문이 고수준/추상적일 때\n\n검색 품질이 비용보다 중요할 때\n\n\n언제 비추천?\n\n정확한 수치/날짜가 중요할 때\n\n문서가 이미 간결할 때\n\n실시간 검색 속도가 중요할 때\n\n\n\n\n\nLangChain에서는 문서를 다양한 상황에서 효율적으로 쿼리할 수 있는 MultiVectorRetriever를 제공한다. 이 기능을 사용하면 문서를 여러 벡터로 저장하고 관리할 수 있어, 정보 검색의 정확도와 효율성을 대폭 향상시킬 수 있다.\n문서당 여러 벡터 생성 방법:\n\n작은 청크 생성: 문서를 더 작은 단위로 나눈 후, 각 청크에 대해 별도의 임베딩을 생성한다. 이 방식을 사용하면 문서의 특정 부분에 좋 더 세심한 주의를 기울일 수 있다. 이 과정은 ParentDocumentRetriever를 통해 구현할 수 있어, 세부 정보에 대한 탐색이 용이해진다.\n요약 임베딩: 각 문서의 요약을 생성하고, 이 요약으로부터 임베딩을 만듭니다. 이 요약 임베딩은 문서의 핵심 내용을 신속하게 파악하는 데 큰 도움이 된다. 문서 전체를 분석하는 대신 핵심적인 요약 부분만을 활용하여 효율성을 극대화할 수 있다.\n가설 질문 활용: 각 문서에 대해 적합한 가설 질문을 만들고, 이 질문에 기반한 임베딩을 생성한다. 특정 주제나 내용에 대해 깊이 있는 탐색을 원할 때 이 방법이 유용하다. 가설 질문은 문서의 내용을 다양한 관점에서 접근하게 해주며, 더 광범위한 이해를 가능하게 한다.\n수동 추가 방식: 사용자가 문서 검색 시 고려해야 할 특정 질문이나 쿼리를 직접 추가할 수 있다. 이 방법을 통해 사용자는 검색 과정에서 보다 세밀한 제어를 할 수 있으며, 자신의 요구 사항에 맞춘 맞춤형 검색이 가능해진다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#실습에-활용한-문서",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#실습에-활용한-문서",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n\n링크: https://spri.kr/posts/view/23669\n\n파일명: SPRI_AI_Brief_2023년12월호_F.pdf\n\n참고: 위의 파일은 data 폴더 내에 다운로드 받으세요\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일  \nfrom dotenv import load_dotenv  \n\n# API 키 정보 로드  \nload_dotenv()  \n\n\n\n\n코드\n# LangSmith 추적을 설정한다. https://smith.langchain.com  \n# !pip install langchain-teddynote  \nfrom langchain_teddynote import logging  \n\n# 프로젝트 이름을 입력한다.  \nlogging.langsmith(\"CH10-Retriever\")  \n\n\n텍스트 파일에서 데이터를 로드하고, 로드된 문서들을 지정된 크기로 분할하는 전처리 과정을 수행한다.\n분할된 문서들은 추후 벡터화 및 검색 등의 작업에 사용될 수 있다.\n\n\n코드\nfrom langchain_community.document_loaders import PyMuPDFLoader  \n\nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")  \ndocs = loader.load()  \nprint(len(docs))  \n\n\n데이터로부터 로드한 원본 도큐먼트는 docs 변수에 담았다.\n23  \n\n\n코드\nprint(docs[5].page_content[:500])  \n\n\n1. 정책/법제  \n2. 기업/산업   \n3. 기술/연구   \n 4. 인력/교육  \n영국 AI 안전성 정상회의에 참가한 28개국, AI 위험에 공동 대응 선언  \nn 영국 블레츨리 파크에서 개최된 AI 안전성 정상회의에 참가한 28개국들이 AI 안전 보장을   \n위한 협력 방안을 담은 블레츨리 선언을 발표  \nn 첨단 AI를 개발하는 국가와 기업들은 AI 시스템에 대한 안전 테스트 계획에 합의했으며,   \n영국의 AI 안전 연구소가 전 세계 국가와 협력해 테스트를 주도할 예정   \nKEY Contents  \n£ AI 안전성 정상회의 참가국들, 블레츨리 선언 통해 AI 안전 보장을 위한 협력에 합의  \nn 2023년 11월 1~2일 영국 블레츨리 파크에서 열린 AI 안전성 정상회의(AI Safety Summit)에   \n참가한 28개국 대표들이 AI 위험 관리를 위한 ‘블레츨리 선언’을 발표   \n∙선언은 AI 안전 보장을 위해 국가, 국제기구, 기업, 시민사회, 학계를 포함한 모든 이해관계자의 협력이   \n중요하다고 강조했으며,  \n\npdf가 23페이지로 구성되어 있어 23이 반환됨 pdf loader는 패이지 단위로 로드가 된다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#chunk-원본-문서-검색",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#chunk-원본-문서-검색",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "대용량 정보를 검색하는 경우, 더 작은 단위로 정보를 임베딩하는 것이 유용할 수 있다.\n\nMultiVectorRetriever를 통해 문서를 여러 벡터로 저장하고 관리할 수 있다.\n\ndocstore에 원본 문서를 저장하고, vectorstore에 임베딩된 문서를 저장한다.\n\n이로써 문서를 더 작은 단위로 나누어 더 정확한 검색이 가능해진다. 때에 따라서는 원본 문서의 내용을 조회할 수 있다.\n\n\n코드\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소  \nimport uuid  \nfrom langchain.storage import InMemoryStore  \nfrom langchain_chroma import Chroma  \nfrom langchain_openai import OpenAIEmbeddings  \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter  \nfrom langchain.retrievers.multi_vector import MultiVectorRetriever  \n\nvectorstore = Chroma(  \n    collection_name=\"small_bigger_chunks\",  \n    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),  \n)  \n# 부모 문서의 저장소 계층  \nstore = InMemoryStore()  \n\nid_key = \"doc_id\"  \n\n# 검색기 (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=vectorstore,  \n    byte_store=store,  \n    id_key=id_key,  \n)  \n\n# 문서 ID를 생성합니다.  \ndoc_ids = [str(uuid.uuid4()) for _ in docs]  \n\n# 두개의 생성된 id를 확인한다.  \ndoc_ids  \n\n\n['ff65f8d8-376d-47d2-bbc0-eae1f7e3c381', 'e18fd86b-f01a-445f-bf31-eb8d16259241', 'f099589b-066f-4827-9679-3bf2d2ec28d2', '834e63d6-0dd9-4bfb-8383-ca6a06d731ef', '5a33bffb-ae85-402e-a4bf-81ab376d7744', '319396d0-cc7d-41f9-826c-71bccc8dade6', '9a38995c-94b7-4963-9605-5d27f868b5e7', 'bc32292c-0d19-452f-b970-1074c43fc864', '48de377c-4693-407b-93e6-3f0ea44b5009', '9e27fff9-15d3-4754-bb26-00779158d528', 'd73a5aef-251b-4a30-aadb-2dc973ef1607', '56535934-2a23-4e3e-b6d2-79121dd93005', 'a535c103-0cf8-4c46-b5ea-90c34aad90bc', '48aac7ed-f704-4d65-90bd-3b5b18adc127', '63b42edf-3546-4257-9046-ff3f4687a217', '749f6239-fec3-4893-8959-f641d6d551b3', '4a08aa0e-4e38-460d-90dc-34e3faca6784', '91cfa69e-7ca7-44f9-8795-bc710622af17', '817e7519-3853-450c-bb05-b3f6763d9cd4', '00e3e534-762a-477b-adf4-3d1dbe67b656', '03171c52-bf3d-4d27-bf93-bb2da4ba6d87', 'df37aeb5-0bfd-4fb1-960d-aad8a5ad11cd', '3cf6570c-25c8-4b28-82ec-6804026e844e']  \n\n총 23개 (parent document 수)의 uid가 생성됨\n\n여기서 큰 청크로 분할하기 위한 parent_text_splitter\n\n더 작은 청크로 분할하기 위한 child_text_splitter를 정의한다.\n\n\n\n코드\n# RecursiveCharacterTextSplitter 객체를 생성한다.  \nparent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=600)  \n\n# 더 작은 청크를 생성하는 데 사용할 분할기  \nchild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=200)  \n\n\n더 큰 Chunk인 Parent 문서를 생성한다.\n\n\n코드\nparent_docs = []  \n\nfor i, doc in enumerate(docs):  \n    # 현재 문서의 ID를 가져옵니다.  \n    _id = doc_ids[i]  \n    # 현재 문서를 하위 문서로 분할  \n    parent_doc = parent_text_splitter.split_documents([doc])  \n\n    for _doc in parent_doc:  \n        # metadata에 문서 ID 를 저장  \n        _doc.metadata[id_key] = _id  \n    parent_docs.extend(parent_doc)  \n\n\n\nparent_docs에 기입된 doc_id를 확인한다.\n\ndoc에는 parent doc(page단위 내용)이 담기고\n\nparent_docs에는 split chunk들이 담긴다.\n\n위의 코드 청크가 parent doc과 split chunk들에게 동일한 id를 부여하는 기능을 한다.\n\n\n\n코드\nchild_docs = []  \nfor i, doc in enumerate(docs):  \n    # 현재 문서의 ID를 가져옵니다.  \n    _id = doc_ids[i]  \n    # 현재 문서를 하위 문서로 분할  \n    child_doc = child_text_splitter.split_documents([doc])  \n    for _doc in child_doc:  \n        # metadata에 문서 ID 를 저장  \n        _doc.metadata[id_key] = _id  \n    child_docs.extend(child_doc)  \n\n\n\nchild_docs에 기입된 doc_id를 확인한다.\n\n상대적으로 더 작은 Chunk인 Child 문서를 생성한다.\n\n\n\n코드\n# 생성된 Child 문서의 메타데이터를 확인한다.  \nchild_docs[0].metadata  \n\n\n{‘source’: ‘data/SPRI_AI_Brief_2023년12월호_F.pdf’, ‘file_path’: ‘data/SPRI_AI_Brief_2023년12월호_F.pdf’, ‘page’: 0, ‘total_pages’: 23, ‘format’: ‘PDF 1.4’, ‘title’: ’‘, ’author’: ‘dj’, ‘subject’: ’‘, ’keywords’: ’‘, ’creator’: ‘Hwp 2018 10.0.0.13462’, ‘producer’: ‘Hancom PDF 1.3.0.542’, ‘creationDate’: “D:20231208132838+09’00’”, ‘modDate’: “D:20231208132838+09’00’”, ‘trapped’: ’‘, ’doc_id’: ‘ff65f8d8-376d-47d2-bbc0-eae1f7e3c381’}\n각각 분할된 청크의 수를 확인한다.\n\n\n코드\nprint(f\"분할된 parent_docs의 개수: {len(parent_docs)}\")  \nprint(f\"분할된 child_docs의 개수: {len(child_docs)}\")  \n\n\n분할된 parent_docs의 개수: 74  \n분할된 child_docs의 개수: 443  \n\n보통 표준적인 MultiVectorRetriever 사용법\n\n# 정상적인 방법  \nretriever.vectorstore.add_documents(child_docs)   # 검색용: 작은 청크만  \nretriever.docstore.mset(zip(doc_ids, parent_docs)) # 반환용: 큰 청크  \n\n# VectorStore: child_docs (443개) - 검색에 사용  \n# DocStore: parent_docs (74개) - 반환에 사용  \n\n하지만, 더 유연하게 구성을 가져갈 수도 있다.\n\n벡터저장소엔 parent_docs와 child_docs를 저장하고\n\ndocstore엔 문서 원본을 저장한다.\n\n벡터저장소에 새롭게 생성한 작게 쪼개진 하위문서 집합을 추가한다.\n\n다음으로는 상위 문서는 생성한 UUID와 맵핑하여 docstore에 추가한다.\n\nmset() 메소드를 통해 문서 ID와 문서 내용을 key-value 쌍으로 문서 저장소에 저장한다.\n\n\n\n\n코드\n# 벡터 저장소에 parent + child 문서를 추가  \nretriever.vectorstore.add_documents(parent_docs)  \nretriever.vectorstore.add_documents(child_docs)  \n\n# docstore 에 원본 문서를 저장  \nretriever.docstore.mset(list(zip(doc_ids, docs)))  \n\n\n유사도 검색을 수행한다. 가장 유사도가 높은 첫 번째 문서 조각을 출력한다.\n여기서 retriever.vectorstore.similarity_search 메소드는 child + parent 문서 chunk 내에서 검색을 수행한다.\n\n\n코드\n# vectorstore의 유사도 검색을 수행한다.   \nrelevant_chunks = retriever.vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \nprint(f\"검색된 문서의 개수: {len(relevant_chunks)}\")  \n\n\n검색된 문서의 개수: 4  \n\n잘게 쪼개진 chunk들에서 검색을 하고 싶으면 retriever.vectorstore 에서 유사도 검색 수행\n\n\n\n코드\nfor chunk in relevant_chunks:  \n    print(chunk.page_content, end=\"\\n\\n\")  \n    print(\"&gt;\" * 100, end=\"\\n\\n\")  \n\n\n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\n▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··························································· 10  \n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  \n이번에는 retriever.invoke() 메소드를 사용하여 쿼리를 실행한다.\nretriever.invoke() 메소드는 원본 문서의 전체 내용을 검색한다.\n\n잘게 쪼개진 chunk들의 parent_doc에서 검색을 하고 싶으면 retriever 에서 유사도 검색 수행\n\nparent와 child는 상대적인 개념으로 parent가 반드시 가공전 원본 문서를 의미하지는 않는다.\n\n\n\n코드\nrelevant_docs = retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")  \nprint(f\"검색된 문서의 개수: {len(relevant_docs)}\", end=\"\\n\\n\")  \nprint(\"=\" * 100, end=\"\\n\\n\")  \nprint(relevant_docs[0].page_content)  \n\n\n검색된 문서의 개수: 2  \n\n====================================================================================================  \n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n리트리버(retriever)가 벡터 데이터베이스에서 기본적으로 수행하는 검색 유형은 유사도 검색이다.\n\nLangChain Vector Stores는 Max Marginal Relevance를 통한 검색도 지원하므로, 이를 대신 사용하고 싶다면 다음과 같이 search_type 속성을 설정하면 된다.\n\nretriever 객체의 search_type 속성을 SearchType.mmr로 설정한다.\n\n이는 검색 시 MMR(Maximal Marginal Relevance) 알고리즘을 사용하도록 지정하는 것\n\n\n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 MMR(Maximal Marginal Relevance)로 설정  \nretriever.search_type = SearchType.mmr  \n\n# 관련 문서 전체를 검색  \nprint(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 similarity_score_threshold로 설정  \nretriever.search_type = SearchType.similarity_score_threshold  \nretriever.search_kwargs = {\"score_threshold\": 0.3}  \n\n# 관련 문서 전체를 검색  \nprint(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에   \n단계적으로 탑재할 계획  \nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는   \n이미지 모델의 3개 모델로 구성  \n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의   \n처리를 지원  \n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며   \n사내 소프트웨어 개발에 최적화  \n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며   \n저해상도 이미지의 고해상도 전환도 지원  \nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,   \n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글   \n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상  \n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.  \n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \n\n\n코드\nfrom langchain.retrievers.multi_vector import SearchType  \n\n# 검색 유형을 similarity로 설정, k값을 1로 설정  \nretriever.search_type = SearchType.similarity  \nretriever.search_kwargs = {\"k\": 1}  \n\n# 관련 문서 전체를 검색  \nprint(len(retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")))  \n\n\n1"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#요약본summary을-벡터저장소에-저장",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#요약본summary을-벡터저장소에-저장",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "요약은 종종 청크(chunk)의 내용을 보다 정확하게 추출할 수 있어 더 나은 검색 결과를 얻을 수 있다.\n\n요약은 핵심 내용만을 담고 있어 LLM의 할루시네이션을 방지할 수 있다.\n\n여기서는 요약을 생성하는 방법과 이를 임베딩하는 방법에 대해 설명한다.\n\n\n\n코드\n# PDF 파일을 로드하고 텍스트를 분할하기 위한 라이브러리 임포트  \nfrom langchain_community.document_loaders import PyMuPDFLoader  \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter  \n\n# PDF 파일 로더 초기화  \nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")  \n\n# 텍스트 분할  \ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)  \n\n# PDF 파일 로드 및 텍스트 분할 실행  \nsplit_docs = loader.load_and_split(text_splitter)  \n\n# 분할된 문서의 개수 출력  \nprint(f\"분할된 문서의 개수: {len(split_docs)}\")  \n\n\n분할된 문서의 개수: 61  \n\n\n코드\nfrom langchain_core.documents import Document  \nfrom langchain_core.output_parsers import StrOutputParser  \nfrom langchain_core.prompts import ChatPromptTemplate  \nfrom langchain_openai import ChatOpenAI  \n\n\nsummary_chain = (  \n    {\"doc\": lambda x: x.page_content}  \n    # 문서 요약을 위한 프롬프트 템플릿 생성  \n    | ChatPromptTemplate.from_messages(  \n        [  \n            (\"system\", \"You are an expert in summarizing documents in Korean.\"),  \n            (  \n                \"user\",  \n                \"Summarize the following documents in 3 sentences in bullet points format.\\n\\n{doc}\",  \n            ),  \n        ]  \n    )  \n    # OpenAI의 ChatGPT 모델을 사용하여 요약 생성  \n    | ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  \n    | StrOutputParser()  \n)  \n\n\n\nchain.batch 메소드를 사용하여 docs 리스트의 문서들을 일괄 요약한다.\n\n여기서 max_concurrency 매개변수를 10으로 설정하여 최대 10개의 문서를 동시에 처리할 수 있도록 한다.\n\n\n\n코드\n# 문서 배치 처리  \nsummaries = summary_chain.batch(split_docs, {\"max_concurrency\": 10})  \n\n\n\n\n코드\nlen(summaries)  \n\n\n61  \n\n61개의 요약본이 생김\n\n요약된 내용을 출력하여 결과를 확인한다.\n\n\n\n코드\n# 원본 문서의 내용을 출력한다.  \nprint(split_docs[33].page_content, end=\"\\n\\n\")  \n# 요약을 출력한다.  \nprint(\"[요약]\")  \nprint(summaries[33])  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에  \n\n[요약]  \n- 삼성전자가 온디바이스에서 작동 가능한 생성 AI 모델 '삼성 가우스'를 공개하였으며, 이 모델은 언어, 코드, 이미지의 3개 모델로 구성되어 있다.  \n- '삼성 가우스'는 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, 다양한 상황에 최적화된 모델 선택이 가능하다.  \n- 삼성전자는 이 AI 모델이 사용자 정보를 외부로 유출하지 않도록 설계되었으며, 향후 다양한 제품에 단계적으로 탑재할 계획이다.  \n\nChroma 벡터 저장소를 초기화하여 자식 청크(child chunks)를 인덱싱한다. 이때 OpenAIEmbeddings를 임베딩 함수로 사용한다.\n\n문서 ID를 나타내는 키로 \"doc_id\"를 사용한다.\n\n\n\n코드\nimport uuid  \n\n# 요약 정보를 저장할 벡터 저장소를 생성한다.  \nsummary_vectorstore = Chroma(  \n    collection_name=\"summaries\",  \n    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),  \n)  \n\n# 부모 문서를 저장할 저장소를 생성한다.  \nstore = InMemoryStore()  \n\n# 문서 ID를 저장할 키 이름을 지정한다.  \nid_key = \"doc_id\"  \n\n# 검색기를 초기화한다. (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=summary_vectorstore,  # 벡터 저장소  \n    byte_store=store,  # 바이트 저장소  \n    id_key=id_key,  # 문서 ID 키  \n)  \n# 문서 ID를 생성합니다.  \ndoc_ids = [str(uuid.uuid4()) for _ in split_docs]  \n\n\n요약된 문서와 메타데이터(여기서는 생성한 요약본에 대한 Document ID이다)를 저장한다.\n\n\n코드\nsummary_docs = [  \n    # 요약된 내용을 페이지 콘텐츠로 하고, 문서 ID를 메타데이터로 포함하는 Document 객체를 생성한다.  \n    Document(page_content=s, metadata={id_key: doc_ids[i]})  \n    for i, s in enumerate(summaries)  \n]  \n\n\n요약본의 문서의 개수는 원본 문서의 개수와 일치한다.\n\n\n코드\n# 요약본의 문서의 개수  \nlen(summary_docs)  \n\n\n61  \n\nretriever.vectorstore.add_documents(summary_docs)를 통해 summary_docs를 벡터 저장소에 추가한다.\n\nretriever.docstore.mset(list(zip(doc_ids, docs)))를 사용하여 doc_ids와 docs를 매핑하여 문서 저장소에 저장한다.\n\n\n\n코드\nretriever.vectorstore.add_documents(  \n    summary_docs  \n)  # 요약된 문서를 벡터 저장소에 추가한다.  \n\n# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.  \nretriever.docstore.mset(list(zip(doc_ids, split_docs)))  \n\n\nvectorstore 객체의 similarity_search 메소드를 사용하여 유사도 검색을 수행한다.\n\n\n코드\n# 유사도 검색을 수행한다.  \nresult_docs = summary_vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \n\n\n\n\n코드\n# 1개의 결과 문서를 출력한다. (요약본)  \nprint(result_docs[0].page_content)  \n\n\n- 삼성전자가 온디바이스에서 작동 가능한 생성 AI 모델 '삼성 가우스'를 공개하였으며, 이 모델은 언어, 코드, 이미지의 3개 모델로 구성되어 있다.  \n- '삼성 가우스'는 정규분포 이론을 정립한 수학자 가우스의 이름을 따왔으며, 다양한 상황에 최적화된 모델 선택이 가능하다.  \n- 삼성전자는 이 AI 모델이 사용자 정보를 외부로 유출하지 않도록 설계되었으며, 향후 다양한 제품에 단계적으로 탑재할 계획이다.  \nretriever 객체의 invoke() 사용하여 질문과 관련된 문서를 검색한다.\n\n\n코드\n# 관련된 원본 문서를 검색하여 가져옵니다.  \nretrieved_docs = retriever.invoke(\"삼성전자가 만든 생성형 AI 의 이름은?\")  \nprint(retrieved_docs[0].page_content)  \n\n\nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#가설-쿼리hypothetical-queries를-활용한-문서-탐색",
    "href": "docs/blog/posts/RAG/10-Retriever/07-MultiVectorRetriever.html#가설-쿼리hypothetical-queries를-활용한-문서-탐색",
    "title": "MultiVectorRetriever",
    "section": "",
    "text": "사용자가 물어볼 만한 질문을 미리 생성해서 검색 인덱스로 활용하자는 전략이다.\n\n\n\n1. 짧은 텍스트 간 매칭의 우수성\n일반적으로 다음과 같은 검색 성능 순서가 관찰된다:\n- 질문 ↔︎ 질문 (최고) &gt; 질문 ↔︎ 문단 &gt; 질문 ↔︎ 긴 문서 (최저)\n2. 임베딩 정보 손실 문제\n긴 문서를 임베딩할 때:\n1. 문서 → 임베딩 과정에서 1차 정보 손실\n2. 검색 매칭 과정에서 2차 정보 손실\n따라서 임베딩 대상을 짧게 유지하는 것이 정보 손실을 최소화한다.\n3. 가설 질문 방식의 장점\n전통적 방식:  \n사용자 질문: \"삼성의 AI 제품은?\"  \n    ↓ (의미 공간에서 멀리 떨어짐)  \n원본 문서: \"2023년 11월 8일 삼성전자가 삼성 AI 포럼에서...\"  \n\n가설 질문 방식:  \n사용자 질문: \"삼성의 AI 제품은?\"  \n    ↓ (의미 공간에서 매우 가까움)  \n미리 생성한 질문: \"삼성전자가 개발한 AI 기술은?\"  \n    ↓ (ID로 연결)  \n원본 문서: \"2023년 11월 8일 삼성전자가...\"  \n질문 ↔︎ 질문 매칭이므로 의미적 유사도가 훨씬 높다.\n\n\n\n\n문서마다 가상 질문 생성 (LLM 활용)\n\n문서 내용을 읽고 “이 문서로 답할 수 있는 질문”을 3~5개 생성\n\n질문을 VectorStore에 저장\n\n원본 문서는 DocStore에 보관\n\n가상 질문들만 임베딩하여 검색 대상으로 활용\n\n검색 시\n\n사용자 질문 → 가상 질문들과 유사도 검색\n\n매칭된 가상 질문의 ID로 원본 문서 반환\n\n\n\n\n\n장점:\n- 검색 정확도 향상: 질문-질문 매칭의 높은 의미 유사도\n- 다양한 관점 포착: 한 문서에 대해 여러 각도의 질문 생성 가능\n- 사용자 의도 파악 개선: 실제 사용자 질문 패턴과 유사\n단점:\n- 비용 증가: LLM으로 질문 생성 필요 (문서당 3~5개 질문)\n- 시간 소요: 인덱싱 시간 증가\n- 질문 품질 의존: LLM이 생성한 질문의 품질에 성능 좌우\n언제 사용할까?\n- 사용자 질문 패턴이 명확할 때 (FAQ, 고객 지원 등)\n- 검색 품질이 매우 중요할 때\n- 인덱싱 비용보다 검색 성능이 우선일 때\n언제 비추천?\n- 문서가 자주 변경될 때 (재생성 비용)\n- 실시간 인덱싱이 필요할 때\n- 비용이 제약 조건일 때\n\n\n\n아래는 Function Calling을 활용하여 가설 질문을 생성하는 예제다.\n\n\n코드\nfunctions = [  \n    {  \n        \"name\": \"hypothetical_questions\",  # 함수의 이름을 지정한다.  \n        \"description\": \"Generate hypothetical questions\",  # 함수에 대한 설명을 작성한다.  \n        \"parameters\": {  # 함수의 매개변수를 정의한다.  \n            \"type\": \"object\",  # 매개변수의 타입을 객체로 지정한다.  \n            \"properties\": {  # 객체의 속성을 정의한다.  \n                \"questions\": {  # 'questions' 속성을 정의한다.  \n                    \"type\": \"array\",  # 'questions'의 타입을 배열로 지정합니다.  \n                    \"items\": {  \n                        \"type\": \"string\"  \n                    },  # 배열의 요소 타입을 문자열로 지정합니다.  \n                },  \n            },  \n            \"required\": [\"questions\"],  # 필수 매개변수로 'questions'를 지정합니다.  \n        },  \n    }  \n]  \n\n\nChatPromptTemplate을 사용하여 주어진 문서를 기반으로 3개의 가상 질문을 생성하는 프롬프트 템플릿을 정의한다.\n\nfunctions와 function_call을 설정하여 가상 질문 생성 함수를 호출한다.\n\nJsonKeyOutputFunctionsParser를 사용하여 생성된 가상 질문을 파싱하고, questions 키에 해당하는 값을 추출한다.\n\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate  \nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser  \nfrom langchain_openai import ChatOpenAI  \n\nhypothetical_query_chain = (  \n    {\"doc\": lambda x: x.page_content}  \n    # 아래 문서를 사용하여 답변할 수 있는 가상의 질문을 정확히 3개 생성하도록 요청합니다. 이 숫자는 조정될 수 있습니다.  \n    | ChatPromptTemplate.from_template(  \n        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer. \"  \n        \"Potential users are those interested in the AI industry. Create questions that they would be interested in. \"  \n        \"Output should be written in Korean:\\n\\n{doc}\"  \n    )  \n    | ChatOpenAI(max_retries=0, model=\"gpt-4o-mini\").bind(  \n        functions=functions, function_call={\"name\": \"hypothetical_questions\"}  \n    )  \n    # 출력에서 \"questions\" 키에 해당하는 값을 추출합니다.  \n    | JsonKeyOutputFunctionsParser(key_name=\"questions\")  \n)  \n\n\n문서에 대한 답변을 출력\n- 출력은 생성한 3개의 가설 쿼리(Hypothetical Queries) 가 담겨 있다.\n\n\n코드\n# 주어진 문서에 대해 체인을 실행합니다.  \nhypothetical_query_chain.invoke(split_docs[33])  \n\n\n['삼성 가우스가 다른 생성 AI 모델에 비해 어떤 경쟁력을 가질 수 있을까?', '온디바이스 작동이 가능한 삼성 가우스가 개인 정보 보호에 미치는 영향은 무엇일까?', '삼성전자가 삼성 가우스를 다양한 제품에 단계적으로 탑재하려는 이유는 무엇일까?']  \nchain.batch 메소드를 사용하여 split_docs 데이터에 대해 동시에 여러 개의 요청을 처리한다.\n\n\n코드\n# 문서 목록에 대해 가설 질문을 배치 생성  \nhypothetical_questions = hypothetical_query_chain.batch(  \n    split_docs, {\"max_concurrency\": 10}  \n)  \n\n\n\n\n코드\nhypothetical_questions[33]  \n\n\n['삼성 가우스가 다른 생성 AI 모델과 비교했을 때, 사용자 개인 정보 보호에 있어 어떤 장점을 제공할까요?', '삼성전자가 삼성 가우스를 다양한 제품에 탑재하기로 결정한 이유는 무엇일까요?', '온디바이스에서 작동하는 삼성 가우스의 기능이 AI 산업의 미래에 어떤 영향을 미칠까요?']  \n생성한 가설 쿼리(Hypothetical Queries)를 벡터저장소에 저장하는 과정이다.\n\n\n코드\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소  \nhypothetical_vectorstore = Chroma(  \n    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()  \n)  \n# 부모 문서의 저장소 계층  \nstore = InMemoryStore()  \n\nid_key = \"doc_id\"  \n# 검색기 (시작 시 비어 있음)  \nretriever = MultiVectorRetriever(  \n    vectorstore=hypothetical_vectorstore,  \n    byte_store=store,  \n    id_key=id_key,  \n)  \ndoc_ids = [str(uuid.uuid4()) for _ in split_docs]  # 문서 ID 생성  \n\n\nquestion_docs 리스트에 메타데이터(문서 ID)를 추가한다.\n\n\n코드\nquestion_docs = []  \n# hypothetical_questions 저장  \nfor i, question_list in enumerate(hypothetical_questions):  \n    question_docs.extend(  \n        # 질문 리스트의 각 질문에 대해 Document 객체를 생성하고, 메타데이터에 해당 질문의 문서 ID를 포함시킵니다.  \n        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]  \n    )  \n\n\n가설 쿼리를 문서에 추가하고, 원본 문서를 docstore에 추가한다.\n\n\n코드\n# hypothetical_questions 문서를 벡터 저장소에 추가합니다.  \nretriever.vectorstore.add_documents(question_docs)  \n\n# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.  \nretriever.docstore.mset(list(zip(doc_ids, split_docs)))  \n\n\nvectorstore 객체의 similarity_search 메소드를 사용하여 유사도 검색을 수행한다.\n\n\n코드\n# 유사한 문서를 벡터 저장소에서 검색합니다.  \nresult_docs = hypothetical_vectorstore.similarity_search(  \n    \"삼성전자가 만든 생성형 AI 의 이름은?\"  \n)  \n\n\n아래는 유사도 검색 결과를 확인한다.\n여기서는 생성한 가설 쿼리만 추가해 놓은 상태이므로, 생성한 가설 쿼리 중 유사도가 가장 높은 문서를 반환한다.\n\n\n코드\n# 유사도 검색 결과를 출력한다.  \nfor doc in result_docs:  \n    print(doc.page_content)  \n    print(doc.metadata)  \n\n\n삼성전자가 AI 기술을 활용하여 개발한 새로운 제품이나 서비스는 무엇일까요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n삼성의 Generative AI 기술이 향후 AI 시장에서 어떤 경쟁력을 가질 것으로 예상되나요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n삼성 개발자 콘퍼런스 코리아 2023에서 발표된 내용이 AI 산업에 어떤 영향을 미칠까요?  \n{'doc_id': '61bcf671-1f5c-4d75-850e-42d4c88c2c87'}  \n온디바이스에서 작동하는 삼성 가우스의 기능이 AI 산업의 미래에 어떤 영향을 미칠까요?  \n{'doc_id': 'b7e88e61-8c28-400e-a4fb-f6bd25e5e40b'}  \nretriever 객체의 invoke 메소드를 사용하여 쿼리와 관련된 문서를 검색한다.\n\n\n코드\n# 관련된 문서를 검색하여 가져옵니다.  \nretrieved_docs = retriever.invoke(result_docs[1].page_content)  \n\n# 검색된 문서를 출력합니다.  \nfor doc in retrieved_docs:  \n    print(doc.page_content)  \n\n\n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.  \nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.  \nSPRi AI Brief |  \n2023-12월호  \n10  \n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개  \nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성   \nAI 모델 ‘삼성 가우스’를 공개  \nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한   \n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유  \nKEY Contents  \n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원  \nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델   \n‘삼성 가우스’를 최초 공개  \n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에   \n최적화된 크기의 모델 선택이 가능  \n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,   \n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유  \n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html",
    "title": "Parent Document Retriever",
    "section": "",
    "text": "Child Document: Document, Sub Document, Split, Split Document, Chunk 등으로도 불리는 작은 단위의 문서 조각\nParent Document: Child Document들이 생성된 원본 문서 또는 더 큰 단위의 상위 문서\n\n\n\n\n\n\n전통적인 RAG 시스템에서는 Child Document만을 사용하여 검색과 응답 생성을 수행한다. 하지만 특정 상황에서는 원본 문서(Parent Document)의 맥락이 필요한 경우가 있다.\n\n\n\n단순히 검색된 Child Document를 LLM에 직접 입력하는 것보다, 다음과 같은 고도화된 프로세스를 통해 더 나은 결과를 얻을 수 있다:\n\nChild Document로 정확한 검색 수행\n해당 Child Document가 포함된 Parent Document에서 앞뒤 맥락 추출\n추출된 맥락을 별도 LLM으로 실시간 요약\n요약된 결과를 최종 RAG LLM에 입력\n\n이러한 접근법은 할루시네이션 감소와 답변 품질 향상에 크게 기여한다.\n\n\n\nParentDocumentRetriever는 이러한 계층적 문서 구조를 효과적으로 활용할 수 있도록 설계된 특화된 Retriever이다. 원본 문서의 맥락이 필요한 모든 상황에서 사용할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#parent-document-retriever-개요",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#parent-document-retriever-개요",
    "title": "Parent Document Retriever",
    "section": "",
    "text": "Child Document: Document, Sub Document, Split, Split Document, Chunk 등으로도 불리는 작은 단위의 문서 조각\nParent Document: Child Document들이 생성된 원본 문서 또는 더 큰 단위의 상위 문서\n\n\n\n\n\n\n전통적인 RAG 시스템에서는 Child Document만을 사용하여 검색과 응답 생성을 수행한다. 하지만 특정 상황에서는 원본 문서(Parent Document)의 맥락이 필요한 경우가 있다.\n\n\n\n단순히 검색된 Child Document를 LLM에 직접 입력하는 것보다, 다음과 같은 고도화된 프로세스를 통해 더 나은 결과를 얻을 수 있다:\n\nChild Document로 정확한 검색 수행\n해당 Child Document가 포함된 Parent Document에서 앞뒤 맥락 추출\n추출된 맥락을 별도 LLM으로 실시간 요약\n요약된 결과를 최종 RAG LLM에 입력\n\n이러한 접근법은 할루시네이션 감소와 답변 품질 향상에 크게 기여한다.\n\n\n\nParentDocumentRetriever는 이러한 계층적 문서 구조를 효과적으로 활용할 수 있도록 설계된 특화된 Retriever이다. 원본 문서의 맥락이 필요한 모든 상황에서 사용할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#rag의-청킹-딜레마",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#rag의-청킹-딜레마",
    "title": "Parent Document Retriever",
    "section": "2 RAG의 청킹 딜레마",
    "text": "2 RAG의 청킹 딜레마\n\n2.1 문제: 상충되는 두 가지 요구사항\nRAG 시스템에서 문서를 청크(chunk)로 분할할 때 다음의 트레이드오프 (정확한 임베딩 vs 넓은 범위의 맥락)에 직면하게 된다.\n요구사항 1: 작은 청크 (정확한 임베딩)\n- 목적: 임베딩이 청크의 의미를 정확하게 표현\n- 이유: 긴 텍스트의 임베딩은 다양한 주제가 섞여 의미가 희석된다\n- 장점: 검색 정확도 향상\n- 단점: 맥락 손실\n요구사항 2: 큰 청크 (충분한 맥락)\n- 목적: LLM이 답변 생성 시 필요한 맥락 제공\n- 이유: 작은 청크만으로는 질문에 답하기 어려울 수 있다\n- 장점: 풍부한 맥락 정보\n- 단점: 임베딩 품질 저하\n청킹 딜레마 예시\n\n\n\n청크 크기\n임베딩 품질\n맥락 유지\n검색 정확도\nLLM 답변 품질\n\n\n\n\n작음 (200자)\n높음\n낮음\n높음\n낮음\n\n\n중간 (500자)\n중간\n중간\n중간\n중간\n\n\n큼 (1000자)\n낮음\n높음\n낮음\n높음\n\n\n\n\n\n2.2 해결책: ParentDocumentRetriever\n핵심 아이디어: “작게 검색하고, 크게 반환한다”\nParentDocumentRetriever는 계층적 문서 구조를 활용하여 이 딜레마를 해결한다.\n작동 원리\n\n문서 분할 (Splitting)\n\n원본 문서를 큰 청크(Parent)로 분할\n\n큰 청크를 다시 작은 청크(Child)로 분할\n\n인덱싱 (Indexing)\n\n작은 청크(Child)만 벡터 DB에 임베딩\n\n큰 청크(Parent)는 별도 문서 저장소(Docstore)에 보관\n\nChild와 Parent 간 ID로 연결\n\n검색 (Retrieval)\n\n작은 청크로 유사도 검색 (정확한 매칭)\n\n검색된 작은 청크의 Parent ID 확인\n\n큰 청크(Parent)를 LLM에 전달\n\n\n계층 구조 예시\n원본 문서 (5000자) ← 문서 DB에 저장\n├── Parent Chunk 1 (1000자) ← 문서 DB 또는 벡터 DB에 저장\n│   ├── Child Chunk 1-1 (200자) ← 벡터 DB에 저장\n│   ├── Child Chunk 1-2 (200자) ← 벡터 DB에 저장\n│   └── Child Chunk 1-3 (200자) ← 벡터 DB에 저장\n├── Parent Chunk 2 (1000자)\n│   ├── Child Chunk 2-1 (200자)\n│   └── Child Chunk 2-2 (200자)\n\n검색 프로세스\n사용자 질문: \"Word2Vec이란?\"\n         ↓\n벡터 검색 (Child Chunk에서)\n         ↓\nChild Chunk 1-2 발견 (유사도: 0.95)\n         ↓\nParent ID 확인 → Parent Chunk 1\n         ↓\nParent Chunk 1 (1000자) 반환\n         ↓\nLLM에 충분한 맥락 제공\n\n\n2.3 핵심 장점\n1. 검색 정확도 향상\n- 작은 청크로 검색하므로 임베딩이 명확한 의미 표현\n- 노이즈가 적어 관련 문서를 정확히 찾음\n2. 맥락 보존\n- 큰 청크를 반환하므로 충분한 맥락 제공\n- LLM이 더 정확한 답변 생성 가능\n3. 유연한 구조\n- Parent 크기를 조절하여 맥락 범위 조정 가능\n- Child 크기를 조절하여 검색 정밀도 조정 가능\n4. 저장 공간 효율\n- Child만 벡터화하므로 벡터 DB 크기 절감\n- Parent는 일반 문서 저장소에 보관"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#환경-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#환경-설정",
    "title": "Parent Document Retriever",
    "section": "3 환경 설정",
    "text": "3 환경 설정\nParentDocumentRetriever 실습을 위한 환경을 설정한다. 여러 개의 텍스트 파일을 로드하기 위해 TextLoader 객체를 생성하고 데이터를 로드한다.\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n\n\n\n\n코드\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.retrievers import ParentDocumentRetriever\n\n\n\n\n코드\nloaders = [\n    # 파일을 로드합니다.\n    TextLoader(\"./data/appendix-keywords.txt\"),\n]\n\ndocs = [] # embeding을 위해 docs에 text 데이터 저장\nfor loader in loaders:\n    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n    docs.extend(loader.load())\n\n\n\n예시는 .txt 파일로 로드되어 있기 때문에 파일 내에 있는 전체 내용이 parent document가 된다.\n\nlen(docs)를 출력하면 parent document의 수 1이 출력된다.\n\n\n단, .pdf의 경우 업로드 시 페이지 단위로 문서가 업로드되어 parent document는 PDF의 page가 된다.\n\nlen(docs)를 출력하면 parent document의 수는 페이지수가 출력된다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#전체-문서-검색",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#전체-문서-검색",
    "title": "Parent Document Retriever",
    "section": "4 전체 문서 검색",
    "text": "4 전체 문서 검색\n\n4.1 전략: Child만 분할, Parent는 원본\nparent_splitter를 지정하지 않고 child_splitter만 사용하는 기본적인 방식을 살펴본다.\n동작 방식\n- Child: 200자 청크로 분할 → 벡터 DB 저장\n- Parent: 원본 문서 그대로 → Docstore 저장\n- 검색: Child로 찾고 → 원본 문서 전체 반환\n장점: 전체 문서 맥락 유지\n단점: 문서가 너무 길면 LLM 컨텍스트 낭비\n\n\n4.2 코드 구현\n\n\n코드\n# 자식 분할기를 생성: 200 token 단위 chunk 생성\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n\n# DB를 생성\nvectorstore = Chroma(\n    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n)\n\nstore = InMemoryStore()\n\n# Retriever 를 생성\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store, # Parent Document (원본 Document) 저장 \n    child_splitter=child_splitter,\n)\n\n\nretriever.add_documents(docs, ids=None) 함수로 문서목록을 추가한다.\n\nids가 None이면 자동으로 생성된다.\nadd_to_docstore=False로 설정 시 document를 중복으로 추가하지 않는다. 단, 중복을 체크하기 위한 ids 값이 필수적으로 요구된다.\n\n\n\n코드\n# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.\nretriever.add_documents(docs, ids=None, add_to_docstore=True)\n\n\n이 코드는 추가된 문서 수만큼 키를 반환한다.\n\nstore 객체의 yield_keys() 메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환한다.\n\n\n\n코드\n# 저장소의 모든 키를 리스트로 반환합니다.\nlist(store.yield_keys())\n\n\n['c2a89a0f-a690-4915-af68-2ea432fb6e51']\n\n1개가 출력되었다. 이는 parent document가 1개 있다는 의미이다. 하지만 chunk는 여러 개다.\n\n이제 벡터 스토어의 검색 기능을 테스트해본다.\n\n작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환되는 것을 확인할 수 있다.\n\nvectorstore 객체의 similarity_search 메서드를 사용하여 Word2Vec와 관련성이 높은 문서를 찾아내는 유사도 검색을 수행한다.\n\n\n\n코드\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n\n\n\nsub_docs[0].page_content, 즉 chunk를 출력한다.\n\n\n\n코드\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n\n이제 전체 retriever에서 검색해본다.\n이 과정에서는 작은 청크(chunk)들이 위치한 문서를 반환하기 때문에 상대적으로 큰 문서들이 반환된다.\nretriever 객체의 invoke() 메서드를 사용하여 쿼리와 관련된 문서를 검색한다.\n\n\n\n코드\n# 전체 문서 (parent document)를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n\n\n\nParent Document (retrieved_docs[0])의 일부 내용을 출력한다.\n\n\n\n코드\n# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.\nprint(\n    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n    end=\"\\n\\n=====================\\n\\n\",\n)\n\n# 문서의 일부를 출력합니다.\nprint(retrieved_docs[0].page_content[2000:2500])\n\n\n문서의 길이: 5733\n\n=====================\n\n 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n연관키워드: 혁신, 기술, 비즈니스 모델\n\nCrawling\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)\n\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을\n결과 분석\n\n\n\n항목\nChild 검색\nParent 반환\n\n\n\n\n크기\n200자 내외\n5733자 (원본 전체)\n\n\n내용\nWord2Vec 정의만\n전체 키워드 사전\n\n\n장점\n정확한 매칭\n풍부한 맥락\n\n\n단점\n맥락 부족\n불필요한 정보 포함\n\n\n\n문제점\n\n원본 문서가 5733자로 너무 길어 LLM에 전달 시 비효율적이다.\n\nParent 크기를 조절하여 이를 해결한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#계층적-청킹-권장-방식",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#계층적-청킹-권장-방식",
    "title": "Parent Document Retriever",
    "section": "5 계층적 청킹 (권장 방식)",
    "text": "5 계층적 청킹 (권장 방식)\n\n5.1 전략: Parent와 Child 모두 분할\n\n이전 방식의 문제점(맥락부족 & 불필요한 정보 포함)을 해결하기 위해 2단계 청킹을 적용한다.\n즉, parent의 크기를 조절하는 방식이다.\n\n분할 전략\n1. 1단계: 원본 문서 → Parent Chunk (1000자)\n2. 2단계: Parent Chunk → Child Chunk (200자)\n3. 저장: Child만 벡터화, Parent는 Docstore\n4. 검색: Child로 찾고 → Parent 반환 (1000자)\n크기 비교\n\n\n\n방식\nChild 크기\nParent 크기\nParent 개수\n장점\n\n\n\n\n실습 1\n200자\n5733자 (원본)\n1개\n전체 맥락\n\n\n실습 2\n200자\n1000자\n7개\n적절한 맥락 + 효율성\n\n\n\n왜 1000자인가?\n- 대부분의 LLM은 입력으로 4-8K 토큰 컨텍스트 사용\n- 1000자 ≈ 250 토큰 (영문 기준)\n- 4-5개 청크 = 1000-1250 토큰 (적정 범위)\n- 충분한 맥락 + 효율적인 토큰 사용\n\n\n5.2 코드 구현\n\nRecursiveCharacterTextSplitter를 사용하여 부모 문서와 자식 문서를 생성한다.\n\n부모 문서는 chunk_size가 1000으로 설정되어 있다\n\n자식 문서는 chunk_size가 200으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성된다\n\n\n\n\n코드\n# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n# 부모보다 작은 문서를 생성해야 합니다.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.\nvectorstore = Chroma(\n    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n)\n# 부모 문서의 저장 계층입니다.\nstore = InMemoryStore()\n\n\nParentDocumentRetriever를 초기화하는 코드이다.\n\nvectorstore 매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정한다.\ndocstore 매개변수는 문서 데이터를 저장하는 문서 저장소를 지정한다.\nchild_splitter 매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정한다.\nparent_splitter 매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정한다.\n\nParentDocumentRetriever는 계층적 문서 구조를 처리하며, 상위 문서와 하위 문서를 별도로 분할하고 저장한다. 이를 통해 검색 시 상위 문서와 하위 문서를 효과적으로 활용할 수 있다.\n\n\n코드\nretriever = ParentDocumentRetriever(\n    # 벡터 저장소를 지정한다.\n    vectorstore=vectorstore,\n    # 문서 저장소를 지정한다.\n    docstore=store,\n    # 하위 문서 분할기를 지정한다.\n    child_splitter=child_splitter,\n    # 상위 문서 분할기를 지정한다.\n    parent_splitter=parent_splitter, # 1000글자 단위 spliter\n)\n\n\nretriever 객체에 docs를 추가한다. retriever가 검색할 수 있는 문서 집합에 새로운 문서들을 추가하는 역할을 한다.\n\n\n코드\nretriever.add_documents(docs)  # 문서를 retriever에 추가한다.\n\n\n이제 문서의 수가 훨씬 더 많아진 것을 볼 수 있다. 이는 더 큰 크기의 상위 청크(chunk)들이다.\n\n\n코드\n# 저장소에서 키를 생성하고 리스트로 변환한 후 길이를 반환합니다.\nlen(list(store.yield_keys()))\n\n\n7\n\n1000자 단위로 쪼개진 상위 chunk가 7개라는 의미이다\n\n기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인해본다.\nvectorstore 객체의 similarity_search 메서드를 사용하여 유사도 검색을 수행한다.\n\n\n코드\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n이번에는 retriever 객체의 invoke() 메서드를 사용하여 문서를 검색한다.\n\n\n코드\n# 문서를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n\n# 검색된 문서의 첫 번째 문서의 페이지 내용을 반환한다.\nprint(retrieved_docs[0].page_content)\n\n\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n연관키워드: 딥러닝, 자연어 처리, Attention\n\nHuggingFace\n\n정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\n\nDigital Transformation\n\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n연관키워드: 혁신, 기술, 비즈니스 모델\n\nCrawling\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#두-가지-방식-비교",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#두-가지-방식-비교",
    "title": "Parent Document Retriever",
    "section": "6 두 가지 방식 비교",
    "text": "6 두 가지 방식 비교\n\n6.1 검색 결과 분석\n실습 1: Parent = 원본 문서\n- Parent 크기: 5733자 (원본 전체)\n- Parent 개수: 1개\n- 반환 내용: 전체 키워드 사전\n- 문제점: 불필요한 정보 과다 포함\n실습 2: Parent = 1000자 청크\n- Parent 크기: 약 1000자\n- Parent 개수: 7개\n- 반환 내용: Word2Vec 관련 키워드 및 문맥 집합\n- 장점: 관련성 높은 정보만 포함\n\n\n6.2 성능 비교\n\n\n\n지표\n실습 1 (원본)\n실습 2 (1000자)\n개선\n\n\n\n\n검색 정확도\n높음\n높음\n동일\n\n\n맥락 충분성\n과다\n적절\n개선\n\n\n토큰 효율성\n낮음\n높음\n5배 개선\n\n\n답변 품질\n중간\n높음\n개선\n\n\n처리 속도\n느림\n빠름\n개선"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#실무-적용-가이드",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#실무-적용-가이드",
    "title": "Parent Document Retriever",
    "section": "7 실무 적용 가이드",
    "text": "7 실무 적용 가이드\n\n7.1 청크 크기 설정 전략\nChild Chunk 크기 (검색용)\n\n\n\n크기\n권장 상황\n장점\n단점\n\n\n\n\n100-200자\n정확한 검색 필요\n정밀한 매칭\n맥락 부족\n\n\n200-400자\n일반적인 경우\n균형 잡힘\n-\n\n\n400-600자\n넓은 검색 필요\n다양한 매칭\n정확도 저하\n\n\n\nParent Chunk 크기 (반환용)\n\n\n\n크기\n권장 상황\n토큰 (영문 기준)\n예상 비용\n\n\n\n\n500-1000자\n짧은 답변\n125-250\n낮음\n\n\n1000-2000자\n일반적인 경우\n250-500\n중간\n\n\n2000-4000자\n긴 맥락 필요\n500-1000\n높음\n\n\n\n\n\n7.2 ParentDocumentRetriever 사용 권장 시나리오\n\n긴 문서 처리\n\n논문, 보고서, 매뉴얼 등\n\n전체 문서는 너무 크지만 맥락은 필요한 경우\n\n토큰 비용 절감\n\nLLM API 비용이 중요한 경우\n\n대량의 문서 처리 시\n\n계층적 정보 구조\n\n섹션별로 나뉜 문서\n\n챕터/절 구조가 있는 경우\n\n\n\n\n7.3 일반 Retriever 사용 권장\n\n짧은 문서\n\n이미 400자 이하의 작은 단위\n\n추가 분할이 불필요한 경우\n\n단순 구조\n\nFAQ, 짧은 뉴스 기사\n\n계층 구조가 없는 경우\n\n전체 맥락 필수\n\n문서 전체를 봐야 하는 경우\n\n코드 전체, 계약서 전문 등\n\n\n\n\n7.4 최적화 팁\n\n동적 크기 조정\n\n# 문서 길이에 따라 Parent 크기 조정\nif avg_doc_length &lt; 2000:\n    parent_size = 1000\nelif avg_doc_length &lt; 5000:\n    parent_size = 2000\nelse:\n    parent_size = 3000\n\nOverlap 설정\n\nparent_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200  # 20% 오버랩으로 맥락 연결\n)\n\n메타데이터 활용\n\n# 섹션 정보 추가로 검색 정확도 향상\nchild_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    add_start_index=True  # 위치 정보 추가\n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#핵심-요약",
    "href": "docs/blog/posts/RAG/10-Retriever/05-ParentDocumentRetriever.html#핵심-요약",
    "title": "Parent Document Retriever",
    "section": "8 핵심 요약",
    "text": "8 핵심 요약\n\n8.1 ParentDocumentRetriever의 핵심 원리\n“작게 검색하고, 크게 반환한다”\n\n정확한 검색: 작은 청크로 정밀한 임베딩과 유사도 검색 수행\n\n충분한 맥락: 큰 청크를 반환하여 LLM에 필요한 맥락 정보 제공\n\n계층적 구조: 두 단계 청킹으로 효율성과 정확도를 동시에 달성\n\n\n\n8.2 주요 장점과 해결 문제\n해결하는 문제\n- 기존 RAG의 청킹 딜레마 (정확한 임베딩 vs 충분한 맥락)\n- 토큰 비용 과다 문제\n- 검색 정확도와 답변 품질 사이의 트레이드오프\n제공하는 장점\n1. 검색 정확도 향상: 작은 청크로 정밀한 매칭\n2. 맥락 정보 보존: 큰 청크 반환으로 풍부한 맥락\n3. 토큰 비용 절감: 적절한 크기로 효율성 증대\n4. 유연한 구조: 도메인별 크기 조절 가능\n\n\n8.3 실무 적용 가이드\n권장 설정\n- Child 청크: 200-400자 (검색 정밀도 최적화)\n- Parent 청크: 1000-2000자 (맥락과 효율성 균형)\n- Overlap: 10-20% (맥락 연결성 향상)\n적용 시나리오\n- ✅ 긴 문서 처리 (논문, 매뉴얼, 보고서)\n- ✅ 토큰 비용이 중요한 대용량 처리\n- ✅ 계층적 정보 구조가 있는 문서\n- ❌ 이미 작은 단위의 문서 (FAQ, 짧은 기사)\n최적화 전략\n- 문서 특성에 따른 동적 크기 조정\n- 메타데이터 활용으로 검색 정확도 향상\n- 도메인별 청킹 전략 수립\n- A/B 테스트를 통한 최적 크기 탐색\nParentDocumentRetriever는 RAG 시스템의 성능을 크게 향상시킬 수 있는 강력한 도구이다. 특히 긴 문서를 다루는 환경에서 검색 정확도와 답변 품질을 동시에 개선할 수 있는 효과적인 솔루션이다."
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html",
    "href": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html",
    "title": "LLM 에 도구 바인딩(Binding Tools)",
    "section": "",
    "text": "LLM 모델이 도구(tool) 를 호출할 수 있으려면 chat 요청을 할 때 모델에 도구 스키마(tool schema) 를 전달해야 합니다.\n도구 호출(tool calling) 기능을 지원하는 LangChain Chat Model 은 .bind_tools() 메서드를 구현하여 LangChain 도구 객체, Pydantic 클래스 또는 JSON 스키마 목록을 수신하고 공급자별 예상 형식으로 채팅 모델에 바인딩(binding) 합니다.\n바인딩된 Chat Model 의 후속 호출은 모델 API에 대한 모든 호출에 도구 스키마를 포함합니다.\n코드\n# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv()\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH15-Bind-Tools\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#llm에-바인딩할-tool-정의",
    "href": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#llm에-바인딩할-tool-정의",
    "title": "LLM 에 도구 바인딩(Binding Tools)",
    "section": "1 LLM에 바인딩할 Tool 정의",
    "text": "1 LLM에 바인딩할 Tool 정의\n실험을 위한 도구(tool) 를 정의합니다.\n\nget_word_length : 단어의 길이를 반환하는 함수\nadd_function : 두 숫자를 더하는 함수\nnaver_news_crawl : 네이버 뉴스 기사를 크롤링하여 본문 내용을 반환하는 함수\n\n참고 - 도구를 정의할 때 @tool 데코레이터를 사용하여 도구를 정의합니다. - docstring 은 가급적 영어로 작성하는 것을 권장합니다.\n\n\n코드\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.agents import tool\n\n\n# 도구를 정의합니다.\n@tool\ndef get_word_length(word: str) -&gt; int:\n    \"\"\"Returns the length of a word.\"\"\"\n    return len(word)\n\n\n@tool\ndef add_function(a: float, b: float) -&gt; float:\n    \"\"\"Adds two numbers together.\"\"\"\n    return a + b\n\n\n@tool\ndef naver_news_crawl(news_url: str) -&gt; str:\n    \"\"\"Crawls a 네이버 (naver.com) news article and returns the body content.\"\"\"\n    # HTTP GET 요청 보내기\n    response = requests.get(news_url)\n\n    # 요청이 성공했는지 확인\n    if response.status_code == 200:\n        # BeautifulSoup을 사용하여 HTML 파싱\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        # 원하는 정보 추출\n        title = soup.find(\"h2\", id=\"title_area\").get_text()\n        content = soup.find(\"div\", id=\"contents\").get_text()\n        cleaned_title = re.sub(r\"\\n{2,}\", \"\\n\", title)\n        cleaned_content = re.sub(r\"\\n{2,}\", \"\\n\", content)\n    else:\n        print(f\"HTTP 요청 실패. 응답 코드: {response.status_code}\")\n\n    return f\"{cleaned_title}\\n{cleaned_content}\"\n\n\ntools = [get_word_length, add_function, naver_news_crawl]"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-로-llm-에-도구-바인딩",
    "href": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-로-llm-에-도구-바인딩",
    "title": "LLM 에 도구 바인딩(Binding Tools)",
    "section": "2 bind_tools() 로 LLM 에 도구 바인딩",
    "text": "2 bind_tools() 로 LLM 에 도구 바인딩\nllm 모델에 bind_tools() 를 사용하여 도구를 바인딩합니다.\n\n\n코드\nfrom langchain_openai import ChatOpenAI\n\n# 모델 생성\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# 도구 바인딩\nllm_with_tools = llm.bind_tools(tools)\n\n\n실행결과를 확인합니다.\n결과는 tool_calls 에 저장됩니다. 따라서, .tool_calls 를 확인하여 도구 호출 결과를 확인할 수 있습니다.\n참고 - name 은 도구의 이름을 의미합니다. - args 는 도구에 전달되는 인자를 의미합니다.\n\n\n코드\n# 실행 결과\nllm_with_tools.invoke(\"What is the length of the word 'teddynote'?\").tool_calls\n\n\n다음으로는 llm_with_tools 와 JsonOutputToolsParser 를 연결하여 tool_calls 를 parsing 하여 결과를 확인합니다.\n\n\n코드\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n\n# 도구 바인딩 + 도구 파서\nchain = llm_with_tools | JsonOutputToolsParser(tools=tools)\n\n# 실행 결과\ntool_call_results = chain.invoke(\"What is the length of the word 'teddynote'?\")\n\n\n\n\n코드\nprint(tool_call_results)\n\n\n실행 결과는 다음과 같습니다.\n참고 - type: 도구의 이름 - args: 도구에 전달되는 인자\n\n\n코드\nprint(tool_call_results, end=\"\\n\\n==========\\n\\n\")\n# 첫 번째 도구 호출 결과\nsingle_result = tool_call_results[0]\n# 도구 이름\nprint(single_result[\"type\"])\n# 도구 인자\nprint(single_result[\"args\"])\n\n\n도구 이름과 일치하는 도구를 찾아 실행합니다.\n\n\n코드\ntool_call_results[0][\"type\"], tools[0].name\n\n\nexecute_tool_calls 함수는 도구를 찾아 args 를 전달하여 도구를 실행합니다.\n즉, type 은 도구의 이름을 의미하고 args 는 도구에 전달되는 인자를 의미합니다.\n\n\n코드\ndef execute_tool_calls(tool_call_results):\n    \"\"\"\n    도구 호출 결과를 실행하는 함수\n\n    :param tool_call_results: 도구 호출 결과 리스트\n    :param tools: 사용 가능한 도구 리스트\n    \"\"\"\n    # 도구 호출 결과 리스트를 순회합니다.\n    for tool_call_result in tool_call_results:\n        # 도구의 이름과 인자를 추출합니다.\n        tool_name = tool_call_result[\"type\"]  # 도구의 이름(함수명)\n        tool_args = tool_call_result[\"args\"]  # 도구에 전달되는 인자\n\n        # 도구 이름과 일치하는 도구를 찾아 실행합니다.\n        # next() 함수를 사용하여 일치하는 첫 번째 도구를 찾습니다.\n        matching_tool = next((tool for tool in tools if tool.name == tool_name), None)\n\n        if matching_tool:\n            # 일치하는 도구를 찾았다면 해당 도구를 실행합니다.\n            result = matching_tool.invoke(tool_args)\n            # 실행 결과를 출력합니다.\n            print(f\"[실행도구] {tool_name} [Argument] {tool_args}\\n[실행결과] {result}\")\n        else:\n            # 일치하는 도구를 찾지 못했다면 경고 메시지를 출력합니다.\n            print(f\"경고: {tool_name}에 해당하는 도구를 찾을 수 없습니다.\")\n\n\n# 도구 호출 실행\n# 이전에 얻은 tool_call_results를 인자로 전달하여 함수를 실행합니다.\nexecute_tool_calls(tool_call_results)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-parser-execution",
    "href": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-parser-execution",
    "title": "LLM 에 도구 바인딩(Binding Tools)",
    "section": "3 bind_tools + Parser + Execution",
    "text": "3 bind_tools + Parser + Execution\n이번에는 일련의 과정을 한 번에 실행합니다.\n\nllm_with_tools : 도구를 바인딩한 모델\nJsonOutputToolsParser : 도구 호출 결과를 파싱하는 파서\nexecute_tool_calls : 도구 호출 결과를 실행하는 함수\n\n흐름 정리 1. 모델에 도구를 바인딩 2. 도구 호출 결과를 파싱 3. 도구 호출 결과를 실행\n\n\n코드\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n\n# bind_tools + Parser + Execution\nchain = llm_with_tools | JsonOutputToolsParser(tools=tools) | execute_tool_calls\n\n\n\n\n코드\n# 실행 결과\nchain.invoke(\"What is the length of the word 'teddynote'?\")\n\n\n\n\n코드\n# 실행 결과\nchain.invoke(\"114.5 + 121.2\")\nprint(114.5 + 121.2)\n\n\n\n\n코드\n# 실행 결과\nchain.invoke(\n    \"뉴스 기사 내용을 크롤링해줘: https://n.news.naver.com/mnews/hotissue/article/092/0002347672?type=series&cid=2000065\"\n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-agent-agentexecutor-로-대체",
    "href": "docs/blog/posts/RAG/15-Agent/02-Bind-Tools.html#bind_tools-agent-agentexecutor-로-대체",
    "title": "LLM 에 도구 바인딩(Binding Tools)",
    "section": "4 bind_tools > Agent & AgentExecutor 로 대체",
    "text": "4 bind_tools &gt; Agent & AgentExecutor 로 대체\nbind_tools() 는 모델에 사용할 수 있는 스키마(도구)를 제공합니다.\nAgentExecutor 는 실제로 llm 호출, 올바른 도구로 라우팅, 실행, 모델 재호출 등을 위한 실행 루프를 생성합니다.\n참고 - Agent 와 AgentExecutor 에 대해서는 다음 장에서 자세히 다룹니다.\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n\n# Agent 프롬프트 생성\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are very powerful assistant, but don't know current events\",\n        ),\n        (\"user\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n\n# 모델 생성\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n\n\n\n코드\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain.agents import AgentExecutor\n\n# 이전에 정의한 도구 사용\ntools = [get_word_length, add_function, naver_news_crawl]\n\n# Agent 생성\nagent = create_tool_calling_agent(llm, tools, prompt)\n\n# AgentExecutor 생성\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    handle_parsing_errors=True,\n)\n\n\n\n\n코드\n# Agent 실행\nresult = agent_executor.invoke({\"input\": \"How many letters in the word `teddynote`?\"})\n\n# 결과 확인\nprint(result[\"output\"])\n\n\n\n\n코드\n# Agent 실행\nresult = agent_executor.invoke({\"input\": \"114.5 + 121.2 의 계산 결과는?\"})\n\n# 결과 확인\nprint(result[\"output\"])\n\n\n한 번의 실행으로 끝나는 것이 아닌, 모델이 자신의 결과를 확인하고 다시 자신을 호출하는 과정을 거칩니다.\n\n\n코드\n# Agent 실행\nresult = agent_executor.invoke(\n    {\"input\": \"114.5 + 121.2 + 34.2 + 110.1 의 계산 결과는?\"}\n)\n\n# 결과 확인\nprint(result[\"output\"])\nprint(\"==========\\n\")\nprint(114.5 + 121.2 + 34.2 + 110.1)\n\n\n이번에는 뉴스 결과를 크롤링 해서 요약 해달라는 요청을 수행합니다.\n\n\n코드\nresult = agent_executor.invoke(\n    {\n        \"input\": \"뉴스 기사를 요약해 줘: https://n.news.naver.com/mnews/hotissue/article/092/0002347672?type=series&cid=2000065\"\n    }\n)\nprint(result[\"output\"])"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/01-Tools.html",
    "href": "docs/blog/posts/RAG/15-Agent/01-Tools.html",
    "title": "도구 (Tools)",
    "section": "",
    "text": "도구(Tool)는 에이전트, 체인 또는 LLM이 외부 세계와 상호작용하기 위한 인터페이스입니다.\nLangChain 에서 기본 제공하는 도구를 사용하여 쉽게 도구를 활용할 수 있으며, 사용자 정의 도구(Custom Tool) 를 쉽게 구축하는 것도 가능합니다.\nLangChain 에 통합된 도구 리스트는 아래 링크에서 확인할 수 있습니다.\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH15-Tools\")\n코드\nimport warnings\n\n# 경고 메시지 무시\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/01-Tools.html#빌트인-도구built-in-tools",
    "href": "docs/blog/posts/RAG/15-Agent/01-Tools.html#빌트인-도구built-in-tools",
    "title": "도구 (Tools)",
    "section": "1 빌트인 도구(built-in tools)",
    "text": "1 빌트인 도구(built-in tools)\n랭체인에서 제공하는 사전에 정의된 도구(tool) 와 툴킷(toolkit) 을 사용할 수 있습니다.\ntool 은 단일 도구를 의미하며, toolkit 은 여러 도구를 묶어서 하나의 도구로 사용할 수 있습니다.\n관련 도구는 아래의 링크에서 참고하실 수 있습니다.\n참고 - LangChain Tools/Toolkits\n\n1.1 Python REPL 도구\n이 도구는 Python 코드를 REPL(Read-Eval-Print Loop) 환경에서 실행하기 위한 클래스를 제공합니다 어떤 코드를 만들어서 실행하는 것이다. - PythonREPLTool\n설명\n\nPython 셸 환경을 제공합니다.\n유효한 Python 명령어를 입력으로 받아 실행합니다.\n결과를 보려면 print(…) 함수를 사용해야 합니다.\n\n주요 특징\n\nsanitize_input: 입력을 정제하는 옵션 (기본값: True)\npython_repl: PythonREPL 인스턴스 (기본값: 전역 범위에서 실행)\n\n사용 방법\n\nPythonREPLTool 인스턴스 생성\nrun 또는 arun, invoke 메서드를 사용하여 Python 코드 실행\n\n입력 정제\n\n입력 문자열에서 불필요한 공백, 백틱, ‘python’ 키워드 등을 제거합니다.\n\n\n\n코드\nfrom langchain_experimental.tools import PythonREPLTool\n\n# 파이썬 코드를 실행하는 도구를 생성합니다.\npython_tool = PythonREPLTool()\n\n\n\n\n코드\n# 파이썬 코드를 실행하고 결과를 반환합니다.\nprint(python_tool.invoke(\"print(100 + 200)\"))\n\n\n아래는 LLM 에게 파이썬 코드를 작성하도록 요청하고 결과를 반환하는 예제입니다.\n흐름 정리 1. LLM 모델에게 특정 작업을 수행하는 Python 코드를 작성하도록 요청합니다. 2. 작성된 코드를 실행하여 결과를 얻습니다. 3. 결과를 출력합니다.\n\n\n코드\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n\n# 파이썬 코드를 실행하고 중간 과정을 출력하고 도구 실행 결과를 반환하는 함수\ndef print_and_execute(code, debug=False):\n    if debug:\n        print(\"CODE:\")\n        print(code)\n    return python_tool.invoke(code)\n\n\n# 파이썬 코드를 작성하도록 요청하는 프롬프트\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \"\n            \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\",\n        ),\n        (\"human\", \"{input}\"),\n    ]\n)\n# LLM 모델 생성\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# 프롬프트와 LLM 모델을 사용하여 체인 생성\nchain = prompt | llm | StrOutputParser() | RunnableLambda(print_and_execute)\n\n\n\n\n코드\n# 결과 출력\nprint(chain.invoke(\"로또 번호 생성기를 출력하는 코드를 작성하세요.\"))\n\n\n\n\n1.2 검색 API 도구\nTavily 검색 API를 활용하여 검색 기능을 구현하는 도구입니다. 이 도구는 두 가지 주요 클래스를 제공합니다: TavilySearchResults와 TavilyAnswer.\nAPI 키 발급 주소 - https://app.tavily.com/\n발급한 API 키를 환경변수에 설정합니다.\n.env 파일에 아래와 같이 설정합니다.\nTAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz\n\n\n1.3 TavilySearchResults\n설명 - Tavily 검색 API를 쿼리하고 JSON 형식의 결과를 반환합니다. - 포괄적이고 정확하며 신뢰할 수 있는 결과에 최적화된 검색 엔진입니다. - 현재 이벤트에 대한 질문에 답변할 때 유용합니다. - 사용자가 LLM/RAG 에서도 없는 최신 정보가 불만족스럽거나, 그럴때 사용하는것.\n주요 매개변수 - max_results (int): 반환할 최대 검색 결과 수 (기본값: 5) - search_depth (str): 검색 깊이 (“basic” 또는 “advanced”) - include_domains (List[str]): 검색 결과에 포함할 도메인 목록 - exclude_domains (List[str]): 검색 결과에서 제외할 도메인 목록 - include_answer (bool): 원본 쿼리에 대한 짧은 답변 포함 여부 - include_raw_content (bool): 각 사이트의 정제된 HTML 콘텐츠 포함 여부 - include_images (bool): 쿼리 관련 이미지 목록 포함 여부\n반환 값 - 검색 결과를 포함하는 JSON 형식의 문자열(url, content)\n혹은 아래의 주석을 해제하고 발급받은 API 키를 입력합니다.\n\n\n코드\n# import os\n\n# os.environ[\"TAVILY_API_KEY\"] = \"TAVILY API 키 입력\"\n\n\n\n\n코드\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n# 도구 생성\ntool = TavilySearchResults(\n    max_results=6,\n    include_answer=True,\n    include_raw_content=True,\n    # include_images=True,\n    # search_depth=\"advanced\", # or \"basic\"\n    include_domains=[\"github.io\", \"wikidocs.net\"],\n    # exclude_domains = []\n)\n\n\n\n\n코드\n# 도구 실행\ntool.invoke({\"query\": \"LangChain Tools 에 대해서 알려주세요\"})\n\n\n\n\n1.4 Image 생성 도구 (DALL-E)\n\nDallEAPIWrapper 클래스: OpenAI의 DALL-E 이미지 생성기를 위한 래퍼(wrapper)입니다.\n\n이 도구를 사용하면 DALL-E API를 쉽게 통합하여 텍스트 기반 이미지 생성 기능을 구현할 수 있습니다. 다양한 설정 옵션을 통해 유연하고 강력한 이미지 생성 도구로 활용할 수 있습니다.\n주요 속성\n\nmodel: 사용할 DALL-E 모델 이름 (기본값: “dall-e-2”, “dall-e-3”)\nn: 생성할 이미지 수 (기본값: 1)\nsize: 생성할 이미지 크기\n\n“dall-e-2”: “1024x1024”, “512x512”, “256x256”\n“dall-e-3”: “1024x1024”, “1792x1024”, “1024x1792”\n\nstyle: 생성될 이미지의 스타일 (기본값: “natural”, “vivid”)\nquality: 생성될 이미지의 품질 (기본값: “standard”, “hd”)\nmax_retries: 생성 시 최대 재시도 횟수\n\n주요 기능 - DALL-E API를 사용하여 텍스트 설명에 기반한 이미지 생성\n흐름 정리\n다음은 DALL-E Image Generator 를 사용하여 이미지를 생성하는 예제입니다.\n이번에는 DallEAPIWrapper 를 사용하여 이미지를 생성해 보겠습니다.\n이때 입력 프롬프트는 LLM 모델에게 이미지를 생성하는 프롬프트를 작성하도록 요청합니다.\n\n\n코드\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# ChatOpenAI 모델 초기화\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9, max_tokens=1000)\n\n# DALL-E 이미지 생성을 위한 프롬프트 템플릿 정의\nprompt = PromptTemplate.from_template(\n    \"Generate a detailed IMAGE GENERATION prompt for DALL-E based on the following description. \"\n    \"Return only the prompt, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the prompt\"\n    \"Output should be less than 1000 characters. Write in English only.\"\n    \"Image Description: \\n{image_desc}\",\n)\n\n# 프롬프트, LLM, 출력 파서를 연결하는 체인 생성\nchain = prompt | llm | StrOutputParser()\n\n# 체인 실행\nimage_prompt = chain.invoke(\n    {\"image_desc\": \"스마트폰을 바라보는 사람들을 풍자한 neo-classicism painting\"}\n)\n\n# 이미지 프롬프트 출력\nprint(image_prompt)\n\n\n그럼, 이전에 생성한 이미지 프롬프트를 DallEAPIWrapper 에 입력하여 이미지를 생성해 보겠습니다.\nDallEAPIWrapper 에 대한 임시 버그 안내사항 (작성일: 2024-10-13)\n\n현재 langchain 0.3.x 이상 버전에서 DallEAPIWrapper 에 대한 임시 버그가 있습니다. (401 오류: invalid API key)\n\n따라서, 아래의 코드를 오류 없이 실행하기 위해서는 LangChain 버전을 0.2.16 으로 변경해야 합니다.\n아래의 주석을 해제하고 실행하면 LangChain 버전을 0.2.16 으로 변경됩니다.\n하지만, 이후 내용에서는 LangChain 버전을 0.3.x 이상으로 변경하여 사용하기 때문에\npoetry shell 명령어를 통해 다시 최신 langchain 버전으로 변경해야 합니다.\n이 과정이 번거로운 분들은 일단 DallEAPIWrapper 를 사용하지 않고 진행하셔도 무방합니다.\n업그레이드/다운그레이드 후에는 반드시 상단 메뉴의 “Restart” 버튼을 클릭한 뒤 진행해야 합니다.\n\n\n코드\n# 임시 버전 다운그레이드 명령어 (실행 후 restart)\n# !pip install langchain==0.2.16 langchain-community==0.2.16 langchain-text-splitters==0.2.4 langchain-experimental==0.0.65 langchain-openai==0.1.20\n\n\n\n\n코드\n# DALL-E API 래퍼 가져오기\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\nfrom IPython.display import Image\nimport os\n\n# DALL-E API 래퍼 초기화\n# model: 사용할 DALL-E 모델 버전\n# size: 생성할 이미지 크기\n# quality: 이미지 품질\n# n: 생성할 이미지 수\ndalle = DallEAPIWrapper(\n    model=\"dall-e-3\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\n# 질문\nquery = \"스마트폰을 바라보는 사람들을 풍자한 neo-classicism painting\"\n\n# 이미지 생성 및 URL 받기\n# chain.invoke()를 사용하여 이미지 설명을 DALL-E 프롬프트로 변환\n# dalle.run()을 사용하여 실제 이미지 생성\nimage_url = dalle.run(chain.invoke({\"image_desc\": query}))\n\n# 생성된 이미지를 표시합니다.\nImage(url=image_url, width=500)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/01-Tools.html#사용자-정의-도구custom-tool",
    "href": "docs/blog/posts/RAG/15-Agent/01-Tools.html#사용자-정의-도구custom-tool",
    "title": "도구 (Tools)",
    "section": "2 사용자 정의 도구(Custom Tool)",
    "text": "2 사용자 정의 도구(Custom Tool)\nLangChain 에서 제공하는 빌트인 도구 외에도 사용자가 직접 도구를 정의하여 사용할 수 있습니다.\n이를 위해서는 langchain.tools 모듈에서 제공하는 tool 데코레이터를 사용하여 함수를 도구로 변환합니다.\n\n2.1 @tool 데코레이터\n이 데코레이터는 함수를 도구로 변환하는 기능을 제공합니다. 다양한 옵션을 통해 도구의 동작을 커스터마이즈할 수 있습니다.\n사용 방법 1. 함수 위에 @tool 데코레이터 적용 2. 필요에 따라 데코레이터 매개변수 설정\n이 데코레이터를 사용하면 일반 Python 함수를 강력한 도구로 쉽게 변환할 수 있으며, 자동화된 문서화와 유연한 인터페이스 생성이 가능합니다.\n\n\n코드\nfrom langchain.tools import tool\n\n\n# 데코레이터를 사용하여 함수를 도구로 변환합니다.\n@tool\ndef add_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n@tool\ndef multiply_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\n\n\n\n코드\n# 도구 실행\nadd_numbers.invoke({\"a\": 3, \"b\": 4})\n\n\n\n\n코드\n# 도구 실행\nmultiply_numbers.invoke({\"a\": 3, \"b\": 4})\n\n\n\n\n2.2 구글 뉴스기사 검색 도구\nlangchain-teddynote 패키지에서 제공하는 GoogleNews 도구를 사용하여 구글 뉴스기사를 검색하는 도구입니다.\n참고 - API 키가 필요하지 않습니다. (RSS 피드를 사용하기 때문)\nnews.google.com 에서 제공하는 뉴스기사를 검색하는 도구입니다.\n설명 - 구글 뉴스 검색 API를 사용하여 최신 뉴스를 검색합니다. - 키워드를 기반으로 뉴스를 검색할 수 있습니다. - 최신 뉴스를 검색할 수 있습니다.\n주요 매개변수 - k (int): 반환할 최대 검색 결과 수 (기본값: 5)\n사용하기 전 패키지를 업데이트 해주세요.\n\n\n코드\n# !pip install -qU langchain-teddynote\n\n\n\n\n코드\nfrom langchain_teddynote.tools import GoogleNews\n\n# 도구 생성\nnews_tool = GoogleNews()\n\n\n\n\n코드\n# 최신 뉴스 검색\nnews_tool.search_latest(k=5)\n\n\n\n\n코드\n# 키워드로 뉴스 검색\nnews_tool.search_by_keyword(\"AI 투자\", k=3)\n\n\n\n\n코드\nfrom langchain_teddynote.tools import GoogleNews\nfrom langchain.tools import tool\nfrom typing import List, Dict\n\n\n# 키워드로 뉴스 검색하는 도구 생성\n@tool\ndef search_keyword(query: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Look up news by keyword\"\"\"\n    print(query)\n    news_tool = GoogleNews()\n    return news_tool.search_by_keyword(query, k=5)\n\n\n\n\n코드\n# 실행 결과\nsearch_keyword.invoke({\"query\": \"LangChain AI\"})"
  },
  {
    "objectID": "docs/blog/posts/RAG/index.html",
    "href": "docs/blog/posts/RAG/index.html",
    "title": "RAG",
    "section": "",
    "text": "01-VectorStoreRetriever\n02-ContextualCompressionRetriever\n03-EnsembleRetriever\n04-LongContextReorder.qmd\n05-ParentDocumentRetriever\n06-MultiQueryRetriever\n07-MultiVectorRetriever\n08-SelfQueryRetriever\n09-TimeWeightedVectorStoreRetriever\n\n\n\n\n\n01-Tools\n02-Bind-Tools\n03-Agent\n04-Agent-More-LLMs\n05-Iter-Human-In-the-Loop",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "RAG"
    ]
  },
  {
    "objectID": "docs/blog/posts/RAG/index.html#rag",
    "href": "docs/blog/posts/RAG/index.html#rag",
    "title": "RAG",
    "section": "",
    "text": "01-VectorStoreRetriever\n02-ContextualCompressionRetriever\n03-EnsembleRetriever\n04-LongContextReorder.qmd\n05-ParentDocumentRetriever\n06-MultiQueryRetriever\n07-MultiVectorRetriever\n08-SelfQueryRetriever\n09-TimeWeightedVectorStoreRetriever\n\n\n\n\n\n01-Tools\n02-Bind-Tools\n03-Agent\n04-Agent-More-LLMs\n05-Iter-Human-In-the-Loop",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "RAG"
    ]
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "이전 글에서 4번의 시행착오를 통해 발견한 순수 LLM 접근의 한계:\n\n\n\n\n\n\n\n문제\n영향\n\n\n\n\n비결정성\n같은 코드로 실행해도 항목 수, 순서, 논리명이 매번 달라짐 (재현성 ~70%)\n\n\n높은 비용\n7 에이전트 x ~500K tokens, 매 실행마다 전체 재처리\n\n\n느린 속도\n~50분/레포 (32개 레포 = ~27시간)\n\n\n디테일 소실\n읽기-합성 방식에서 원자적 항목의 93%가 누락될 수 있음\n\n\n\n이 문제들은 프롬프트 개선만으로는 해결할 수 없습니다. 아키텍처 변경이 필요합니다.\n\n\n\n\n메타데이터 테이블의 8열을 분석하면:\n| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |\n     +-------------------------------------+  +------------------+\n           코드 파서로 100% 결정적 추출             LLM 해석 필요\n\n\n\n컬럼\n추출 방법\n결정적?\n\n\n\n\n#\n자동 번호\n100%\n\n\n파일\nglob(\"**/*.py\")\n100%\n\n\n클래스/함수/변수\nPython AST\n100%\n\n\n물리명\nPython AST\n100%\n\n\n논리명\nLLM\n비결정적\n\n\n데이터 타입\nAST + type hints\n95%\n\n\n값/기본값\nAST\n100%\n\n\n의미\nLLM\n비결정적\n\n\n\n8열 중 6열은 코드 파서, 2열만 LLM으로 채우면 됩니다.\n\n\n\n\nPhase 1: 결정적 추출 (Python AST 파서)\n--------------------------------------------\n  glob + ast.parse -&gt; 모든 클래스/함수/변수/상수 추출\n  |\n  규칙 기반 카테고리 분류 (rules.yaml)\n  |\n  skeleton.json (8열 중 6열 완성, 논리명/의미는 빈 칸)\n\nPhase 2: LLM 해석\n--------------------------------------------\n  skeleton.json + 소스코드 컨텍스트 -&gt; Claude API\n  |\n  논리명 + 의미 + N.1/N.4 해석 메타데이터 생성\n  |\n  enriched.json (이전 실행 결과 참조하여 안정성 확보)\n\nPhase 3: 렌더링\n--------------------------------------------\n  enriched.json + Jinja2 템플릿\n  |\n  METADATA_INVESTIGATION.md (항상 동일 구조)\n\nPhase 4: Diff 검증\n--------------------------------------------\n  이전 버전과 비교 -&gt; 변경 리포트\n  |\n  안정성 점수, 신규/삭제/변경 항목 식별\n\n\n\n\n\n\nPython ast 모듈로 코드의 사실(fact)을 결정적으로 추출합니다:\nimport ast, glob, os\n\nclass PythonASTExtractor:\n    def extract_all(self, repo_path: str) -&gt; list[dict]:\n        items = []\n        # sorted()로 파일 순서 고정\n        for py_file in sorted(glob.glob(f\"{repo_path}/**/*.py\", recursive=True)):\n            tree = ast.parse(open(py_file).read())\n            for node in ast.walk(tree):\n                if isinstance(node, ast.ClassDef):\n                    for item in node.body:\n                        if isinstance(item, ast.AnnAssign):\n                            items.append({\n                                # id = 파일:줄번호:물리명 (실행 간 앵커)\n                                \"id\": f\"{rel_path}:{item.lineno}:{item.target.id}\",\n                                \"file\": rel_path,\n                                \"line_number\": item.lineno,\n                                \"structure\": f\"{node.name} field\",\n                                \"physical_name\": item.target.id,\n                                \"data_type\": ast.unparse(item.annotation),\n                                \"default_value\": ast.unparse(item.value) if item.value else \"-\",\n                                \"logical_name\": \"\",   # LLM이 채움\n                                \"meaning\": \"\",        # LLM이 채움\n                            })\n        return sorted(items, key=lambda x: (x[\"file\"], x[\"line_number\"]))\n핵심: id가 파일:줄번호:물리명으로 구성되어, 코드가 동일하면 항상 동일한 ID가 생성됩니다. 이 ID가 실행 간 일관성의 앵커 역할을 합니다.\n\n\n\n# rules.yaml\ncategories:\n  - id: 3\n    name: \"중간 산출물\"\n    rules:\n      - type: \"file_path_match\"\n        file_patterns: [\"*/dto/*.py\"]\n        structure_types: [\"class_field\"]\n        priority: 20  # DTO 필드는 무조건 Cat 3\n\n  - id: 6\n    name: \"DB 스키마 및 쿼리\"\n    rules:\n      - type: \"file_path_match\"\n        file_patterns: [\"*/repository/*.py\", \"*/entity/**/*.py\"]\n        priority: 18\n\n# 매칭되지 않은 항목은 Cat 14 (기타)\nfallback_category: 14\n파일 경로, 구조 유형, 물리명 패턴으로 카테고리를 규칙 기반으로 분류합니다. LLM이 개입하지 않으므로 항상 동일한 분류 결과가 나옵니다.\n\n\n\n\n\nLLM이 담당하는 논리명과 의미의 안정성을 확보하는 핵심 메커니즘:\ndef build_prompt(self, skeleton_items, previous_items):\n    prompt = \"\"\"\n    ## 기존 항목 검증 (변경 불필요 시 그대로 유지)\n\n    아래 항목들은 이전 실행에서 이미 해석되었습니다.\n    코드 변경으로 인해 의미가 달라진 항목만 수정하세요.\n    **변경이 없으면 반드시 그대로 유지하세요.**\n\n    ## 신규 항목 해석 (새로 작성)\n\n    아래 항목들은 이전 실행에 없던 신규 항목입니다.\n    logical_name(영문 Title Case)과 meaning(한글 1~2문장)을 작성하세요.\n    \"\"\"\n이전 실행 결과를 명시적으로 제공하고 “변경 없으면 유지”를 지시하면, LLM이 담당하는 2열도 95%+ 안정성을 확보합니다.\n\n\n\n\n{% for cat in categories %}\n## {{ cat.number }}. {{ cat.name }}\n\n### {{ cat.number }}.1 추출 가능 이유\n{{ cat.rationale }}\n\n### {{ cat.number }}.2 조사 대상 파일 목록\n{% for f in cat.files %}\n- `{ f }`\n{% endfor %}\n\n### {{ cat.number }}.3 상세 내용\n\n| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |\n|---|------|------------------|--------|--------|-------------|-----------|------|\n{% for item in cat.items %}\n| {{ item.number }} | `{ item.file }` | {{ item.structure }} | `{ item.physical_name }` | {{ item.logical_name }} | `{ item.data_type }` | {{ item.default }} | {{ item.meaning }} |\n{% endfor %}\n\n### {{ cat.number }}.4 해석 메타데이터\n{{ cat.interpretation }}\n{% endfor %}\n같은 JSON이 들어오면 항상 동일한 Markdown이 출력됩니다. 병합 과정의 ad-hoc 작업이 완전히 제거됩니다.\n\n\n\n\n=== 메타데이터 변경 리포트 ===\n이전: 1,411개 -&gt; 현재: 1,423개 (+12)\n안정성 점수: 0.9915 (99.15%)\n\n[신규 추가] 12개:\n  + automsa/common/module/new_feature.py:15:threshold  (Cat 5)\n  ...\n\n[삭제] 0개\n[의미 변경] 0개\n[카테고리 이동] 0개\n\n\n\n\n\n\n\n\n\n\n\n\n항목\n순수 LLM (7 에이전트)\n하이브리드\n\n\n\n\nPhase 1 (추출)\n-\n30초 (AST)\n\n\nPhase 2 (해석)\n7 에이전트 x 5분 = 35분\n14 API 호출 x 30초 = 7분\n\n\nPhase 3 (렌더링)\n수동 병합 10분\n1초 (Jinja2)\n\n\nPhase 4 (검증)\n수동 확인\n1초 (자동)\n\n\n총 시간\n~50분\n~8분\n\n\n토큰 비용\n~500K tokens\n~80K tokens (1/6)\n\n\n재현성\n~70%\n~99%\n\n\n코드 변경 시\n전체 재실행\n변경분만 재실행\n\n\n\n\n\n\n\n한 번에 전체를 구축할 필요 없습니다:\n\n\npython extractor.py phase1 /path/to/repo --output skeleton.json\n이것만으로도 행 수, 행 순서, 카테고리 분류가 100% 고정됩니다. skeleton.json을 에이전트에게 전달하면, 에이전트는 빈 칸(논리명, 의미)만 채우면 됩니다.\n\nStep 1만으로 가장 큰 문제(비결정성)의 80%가 해결됩니다.\n\n\n\n\nClaude API로 논리명/의미를 자동 채우고, previous.json 참조 메커니즘을 추가합니다.\n\n\n\nJinja2 렌더링, Diff 검증을 완성하고, 코드 변경 시 자동 실행되도록 CI/CD에 통합합니다.\n\n\n\n\n\n하이브리드 파이프라인에서도 LLM이 여전히 필수적인 영역이 있습니다:\n\n\nTSV/FASTA 파일의 스키마(구분자, 컬럼 순서, 생산/소비 함수)는 AST로 함수 시그니처는 추출할 수 있지만, 실제 write 로직 (f-string 안의 컬럼 순서, 조건부 컬럼 등)은 코드 흐름 분석이 필요합니다.\n\n\n\n항목 간 관계, 암묵적 규칙, 상태 전이 조건 등은 코드의 의미론적 이해가 필요하며, AST의 구문론적 분석만으로는 도출할 수 없습니다.\n\n\n\n규칙 기반 분류로 90%는 해결되지만, 여러 카테고리에 걸치는 항목의 “주 카테고리” 결정은 LLM의 판단이 필요합니다.\n\n\n\n\n\n대규모 코드베이스의 메타데이터를 주기적으로 추출해야 하는 상황에서:\n\n코드 파서(AST)가 사실을 고정하고, LLM은 해석만 담당하는 하이브리드가 정답이다\n이전 실행 결과를 참조하면 LLM 해석의 안정성도 95%+로 올릴 수 있다\nJinja2 템플릿으로 렌더링하면 출력 형식이 100% 결정적이다\nDiff 검증으로 변경을 추적하면 운영 환경에서의 안정성이 보장된다\nStep 1(AST Skeleton)만으로도 비결정성 문제의 80%가 즉시 해결된다\n\n한마디로: “무엇을 추출할 것인가”는 프롬프트의 영역이지만, “어떻게 안정적으로 반복할 것인가”는 아키텍처의 영역이다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#문제-정의-왜-순수-llm이-안-되는가",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#문제-정의-왜-순수-llm이-안-되는가",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "이전 글에서 4번의 시행착오를 통해 발견한 순수 LLM 접근의 한계:\n\n\n\n\n\n\n\n문제\n영향\n\n\n\n\n비결정성\n같은 코드로 실행해도 항목 수, 순서, 논리명이 매번 달라짐 (재현성 ~70%)\n\n\n높은 비용\n7 에이전트 x ~500K tokens, 매 실행마다 전체 재처리\n\n\n느린 속도\n~50분/레포 (32개 레포 = ~27시간)\n\n\n디테일 소실\n읽기-합성 방식에서 원자적 항목의 93%가 누락될 수 있음\n\n\n\n이 문제들은 프롬프트 개선만으로는 해결할 수 없습니다. 아키텍처 변경이 필요합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#핵심-발견-8열-테이블의-75는-결정적으로-추출-가능하다",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#핵심-발견-8열-테이블의-75는-결정적으로-추출-가능하다",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "메타데이터 테이블의 8열을 분석하면:\n| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |\n     +-------------------------------------+  +------------------+\n           코드 파서로 100% 결정적 추출             LLM 해석 필요\n\n\n\n컬럼\n추출 방법\n결정적?\n\n\n\n\n#\n자동 번호\n100%\n\n\n파일\nglob(\"**/*.py\")\n100%\n\n\n클래스/함수/변수\nPython AST\n100%\n\n\n물리명\nPython AST\n100%\n\n\n논리명\nLLM\n비결정적\n\n\n데이터 타입\nAST + type hints\n95%\n\n\n값/기본값\nAST\n100%\n\n\n의미\nLLM\n비결정적\n\n\n\n8열 중 6열은 코드 파서, 2열만 LLM으로 채우면 됩니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#하이브리드-아키텍처",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#하이브리드-아키텍처",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "Phase 1: 결정적 추출 (Python AST 파서)\n--------------------------------------------\n  glob + ast.parse -&gt; 모든 클래스/함수/변수/상수 추출\n  |\n  규칙 기반 카테고리 분류 (rules.yaml)\n  |\n  skeleton.json (8열 중 6열 완성, 논리명/의미는 빈 칸)\n\nPhase 2: LLM 해석\n--------------------------------------------\n  skeleton.json + 소스코드 컨텍스트 -&gt; Claude API\n  |\n  논리명 + 의미 + N.1/N.4 해석 메타데이터 생성\n  |\n  enriched.json (이전 실행 결과 참조하여 안정성 확보)\n\nPhase 3: 렌더링\n--------------------------------------------\n  enriched.json + Jinja2 템플릿\n  |\n  METADATA_INVESTIGATION.md (항상 동일 구조)\n\nPhase 4: Diff 검증\n--------------------------------------------\n  이전 버전과 비교 -&gt; 변경 리포트\n  |\n  안정성 점수, 신규/삭제/변경 항목 식별"
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-1-ast-추출기",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-1-ast-추출기",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "Python ast 모듈로 코드의 사실(fact)을 결정적으로 추출합니다:\nimport ast, glob, os\n\nclass PythonASTExtractor:\n    def extract_all(self, repo_path: str) -&gt; list[dict]:\n        items = []\n        # sorted()로 파일 순서 고정\n        for py_file in sorted(glob.glob(f\"{repo_path}/**/*.py\", recursive=True)):\n            tree = ast.parse(open(py_file).read())\n            for node in ast.walk(tree):\n                if isinstance(node, ast.ClassDef):\n                    for item in node.body:\n                        if isinstance(item, ast.AnnAssign):\n                            items.append({\n                                # id = 파일:줄번호:물리명 (실행 간 앵커)\n                                \"id\": f\"{rel_path}:{item.lineno}:{item.target.id}\",\n                                \"file\": rel_path,\n                                \"line_number\": item.lineno,\n                                \"structure\": f\"{node.name} field\",\n                                \"physical_name\": item.target.id,\n                                \"data_type\": ast.unparse(item.annotation),\n                                \"default_value\": ast.unparse(item.value) if item.value else \"-\",\n                                \"logical_name\": \"\",   # LLM이 채움\n                                \"meaning\": \"\",        # LLM이 채움\n                            })\n        return sorted(items, key=lambda x: (x[\"file\"], x[\"line_number\"]))\n핵심: id가 파일:줄번호:물리명으로 구성되어, 코드가 동일하면 항상 동일한 ID가 생성됩니다. 이 ID가 실행 간 일관성의 앵커 역할을 합니다.\n\n\n\n# rules.yaml\ncategories:\n  - id: 3\n    name: \"중간 산출물\"\n    rules:\n      - type: \"file_path_match\"\n        file_patterns: [\"*/dto/*.py\"]\n        structure_types: [\"class_field\"]\n        priority: 20  # DTO 필드는 무조건 Cat 3\n\n  - id: 6\n    name: \"DB 스키마 및 쿼리\"\n    rules:\n      - type: \"file_path_match\"\n        file_patterns: [\"*/repository/*.py\", \"*/entity/**/*.py\"]\n        priority: 18\n\n# 매칭되지 않은 항목은 Cat 14 (기타)\nfallback_category: 14\n파일 경로, 구조 유형, 물리명 패턴으로 카테고리를 규칙 기반으로 분류합니다. LLM이 개입하지 않으므로 항상 동일한 분류 결과가 나옵니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-2-llm-해석-이전-결과-참조",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-2-llm-해석-이전-결과-참조",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "LLM이 담당하는 논리명과 의미의 안정성을 확보하는 핵심 메커니즘:\ndef build_prompt(self, skeleton_items, previous_items):\n    prompt = \"\"\"\n    ## 기존 항목 검증 (변경 불필요 시 그대로 유지)\n\n    아래 항목들은 이전 실행에서 이미 해석되었습니다.\n    코드 변경으로 인해 의미가 달라진 항목만 수정하세요.\n    **변경이 없으면 반드시 그대로 유지하세요.**\n\n    ## 신규 항목 해석 (새로 작성)\n\n    아래 항목들은 이전 실행에 없던 신규 항목입니다.\n    logical_name(영문 Title Case)과 meaning(한글 1~2문장)을 작성하세요.\n    \"\"\"\n이전 실행 결과를 명시적으로 제공하고 “변경 없으면 유지”를 지시하면, LLM이 담당하는 2열도 95%+ 안정성을 확보합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-3-jinja2-렌더링",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-3-jinja2-렌더링",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "{% for cat in categories %}\n## {{ cat.number }}. {{ cat.name }}\n\n### {{ cat.number }}.1 추출 가능 이유\n{{ cat.rationale }}\n\n### {{ cat.number }}.2 조사 대상 파일 목록\n{% for f in cat.files %}\n- `{ f }`\n{% endfor %}\n\n### {{ cat.number }}.3 상세 내용\n\n| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |\n|---|------|------------------|--------|--------|-------------|-----------|------|\n{% for item in cat.items %}\n| {{ item.number }} | `{ item.file }` | {{ item.structure }} | `{ item.physical_name }` | {{ item.logical_name }} | `{ item.data_type }` | {{ item.default }} | {{ item.meaning }} |\n{% endfor %}\n\n### {{ cat.number }}.4 해석 메타데이터\n{{ cat.interpretation }}\n{% endfor %}\n같은 JSON이 들어오면 항상 동일한 Markdown이 출력됩니다. 병합 과정의 ad-hoc 작업이 완전히 제거됩니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-4-diff-검증",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#phase-4-diff-검증",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "=== 메타데이터 변경 리포트 ===\n이전: 1,411개 -&gt; 현재: 1,423개 (+12)\n안정성 점수: 0.9915 (99.15%)\n\n[신규 추가] 12개:\n  + automsa/common/module/new_feature.py:15:threshold  (Cat 5)\n  ...\n\n[삭제] 0개\n[의미 변경] 0개\n[카테고리 이동] 0개"
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#비용시간재현성-비교",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#비용시간재현성-비교",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "항목\n순수 LLM (7 에이전트)\n하이브리드\n\n\n\n\nPhase 1 (추출)\n-\n30초 (AST)\n\n\nPhase 2 (해석)\n7 에이전트 x 5분 = 35분\n14 API 호출 x 30초 = 7분\n\n\nPhase 3 (렌더링)\n수동 병합 10분\n1초 (Jinja2)\n\n\nPhase 4 (검증)\n수동 확인\n1초 (자동)\n\n\n총 시간\n~50분\n~8분\n\n\n토큰 비용\n~500K tokens\n~80K tokens (1/6)\n\n\n재현성\n~70%\n~99%\n\n\n코드 변경 시\n전체 재실행\n변경분만 재실행"
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#점진적-도입-경로",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#점진적-도입-경로",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "한 번에 전체를 구축할 필요 없습니다:\n\n\npython extractor.py phase1 /path/to/repo --output skeleton.json\n이것만으로도 행 수, 행 순서, 카테고리 분류가 100% 고정됩니다. skeleton.json을 에이전트에게 전달하면, 에이전트는 빈 칸(논리명, 의미)만 채우면 됩니다.\n\nStep 1만으로 가장 큰 문제(비결정성)의 80%가 해결됩니다.\n\n\n\n\nClaude API로 논리명/의미를 자동 채우고, previous.json 참조 메커니즘을 추가합니다.\n\n\n\nJinja2 렌더링, Diff 검증을 완성하고, 코드 변경 시 자동 실행되도록 CI/CD에 통합합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#ast만으로는-부족한-영역",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#ast만으로는-부족한-영역",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "하이브리드 파이프라인에서도 LLM이 여전히 필수적인 영역이 있습니다:\n\n\nTSV/FASTA 파일의 스키마(구분자, 컬럼 순서, 생산/소비 함수)는 AST로 함수 시그니처는 추출할 수 있지만, 실제 write 로직 (f-string 안의 컬럼 순서, 조건부 컬럼 등)은 코드 흐름 분석이 필요합니다.\n\n\n\n항목 간 관계, 암묵적 규칙, 상태 전이 조건 등은 코드의 의미론적 이해가 필요하며, AST의 구문론적 분석만으로는 도출할 수 없습니다.\n\n\n\n규칙 기반 분류로 90%는 해결되지만, 여러 카테고리에 걸치는 항목의 “주 카테고리” 결정은 LLM의 판단이 필요합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#결론",
    "href": "docs/blog/posts/Agent/01-코드분석/02-하이브리드_파이프라인_설계.html#결론",
    "title": "하이브리드 메타데이터 추출 파이프라인: AST + LLM 자동화 설계",
    "section": "",
    "text": "대규모 코드베이스의 메타데이터를 주기적으로 추출해야 하는 상황에서:\n\n코드 파서(AST)가 사실을 고정하고, LLM은 해석만 담당하는 하이브리드가 정답이다\n이전 실행 결과를 참조하면 LLM 해석의 안정성도 95%+로 올릴 수 있다\nJinja2 템플릿으로 렌더링하면 출력 형식이 100% 결정적이다\nDiff 검증으로 변경을 추적하면 운영 환경에서의 안정성이 보장된다\nStep 1(AST Skeleton)만으로도 비결정성 문제의 80%가 즉시 해결된다\n\n한마디로: “무엇을 추출할 것인가”는 프롬프트의 영역이지만, “어떻게 안정적으로 반복할 것인가”는 아키텍처의 영역이다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "Claude Code는 Anthropic에서 만든 CLI 기반 AI 코딩 어시스턴트입니다. 터미널에서 직접 실행되며, 코드 작성/수정, 디버깅, Git 작업, 테스트 등 소프트웨어 엔지니어링 전반을 지원합니다.\n이 포스트에서는 팀 전체가 동일한 Claude Code 환경을 사용할 수 있도록 팀 설정 레포지토리를 클론하고, 모든 도구를 설치하는 과정을 기록합니다.\n\n\n\n\n팀 설정 레포는 GitHub 조직의 private 레포이므로 SSH 인증이 필요합니다.\n\n\n~/.ssh/config에 조직용 호스트를 등록합니다:\nHost seegene\n  User git\n  HostName github.com\n  Port 22\n  IdentityFile ~/.ssh/seegene\n\n\n\ngit clone git@seegene:seegene/claude-code-team-config.git\nSSH 호스트 별칭(seegene)을 사용하면 HTTPS 인증 없이도 private 레포에 접근할 수 있습니다.\n\n\n\n\n\ncd claude-code-team-config\n./install.sh\n이 스크립트가 ~/.claude/에 설치하는 항목:\n\n\n\n항목\n설명\n\n\n\n\nCLAUDE.md\n팀 공통 글로벌 지침 (한국어 응답, 코딩 스타일 등)\n\n\nsettings.json\n훅, 권한, Agent Teams 환경변수\n\n\nrules/ (9개)\n보안, 테스팅, Git 워크플로우 등 코딩 규칙\n\n\nagents/ (9개)\nplanner, architect, tdd-guide 등 전문 에이전트\n\n\ncommands/ (26개)\n/plan, /tdd, /code-review 등 슬래시 명령어\n\n\nhooks/\nPrettier 자동 포맷, TypeScript 체크, Codiens 데이터 수집\n\n\n\n기존 설정이 있으면 ~/.claude/backup-YYYYMMDD-HHMMSS/에 자동 백업됩니다.\n\n\n\n\nbrew install tmux bat lsd fzf\n\n\n\n도구\n용도\n\n\n\n\ntmux\nAgent Teams 분할 창 모드에 필수\n\n\nbat\n문법 강조가 되는 cat 대체\n\n\nlsd\n아이콘이 있는 ls 대체\n\n\nfzf\n퍼지 검색 (Ctrl+R로 명령어 히스토리 검색)\n\n\n\n\n\n# 마우스 지원\nset -g mouse on\n\n# 256색 지원\nset -g default-terminal \"screen-256color\"\nset -ga terminal-overrides \",xterm-256color:Tc\"\n\n# 패널 분할 (직관적)\nbind | split-window -h\nbind - split-window -v\n\n# vim 스타일 패널 이동\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n# 히스토리 크기\nset -g history-limit 50000\n\n# 인덱스 1부터 시작\nset -g base-index 1\nsetw -g pane-base-index 1\ntmux는 Claude Code의 Agent Teams 기능에 필수입니다. 여러 에이전트가 각각 독립된 tmux 패널에서 동시에 작업하는 것을 시각적으로 확인할 수 있습니다.\n\n\n\n\n\n./install-bioskills.sh\nGPTomics/bioSkills 레포에서 바이오인포매틱스 전문 스킬을 설치합니다. 56개 카테고리, 425개 스킬이 ~/.claude/skills/에 설치됩니다.\n\n\n\n\n\n카테고리\n스킬 예시\n\n\n\n\nvariant-calling\nGATK, DeepVariant, VCF 처리\n\n\nsingle-cell\nscRNA-seq, trajectory, perturb-seq\n\n\nalignment\nSAM/BAM 처리, MSA, pairwise alignment\n\n\nchip-seq\npeak calling, motif analysis\n\n\nspatial-transcriptomics\n공간 전사체 분석\n\n\nworkflows\nNextflow, Snakemake, WDL 파이프라인\n\n\n\n설치 후 Claude Code에 자연어로 요청하면 해당 스킬이 자동 활성화됩니다:\n\"RNA-seq 카운트 데이터에서 차등 발현 유전자를 찾아줘\"\n\"BAM 파일에서 변이를 호출해줘\"\n\n\n\n\n\n./install-speckit.sh\nGitHub의 spec-kit 기반 Spec-Driven Development 도구를 설치합니다.\n\n\n\n\n\n명령어\n기능\n\n\n\n\n/speckit-constitution\n프로젝트 원칙 수립\n\n\n/speckit-specify\n기능 명세 작성\n\n\n/speckit-plan\n구현 계획 생성\n\n\n/speckit-tasks\n태스크 분해\n\n\n/speckit-taskstoissues\n태스크를 이슈로 변환\n\n\n/speckit-implement\n명세 기반 구현\n\n\n/speckit-checklist\nQA 체크리스트 생성\n\n\n/speckit-analyze\n기존 코드 분석\n\n\n/speckit-clarify\n요구사항 명확화\n\n\n\n\n\n\n\n\n팀 설정에는 14개의 Hook이 포함되어 있습니다. Hook은 Claude Code의 도구 실행 전후에 자동으로 실행되는 스크립트입니다.\n\n\n\n\n\n\n\n\n\n\n타이밍\n수량\n주요 동작\n\n\n\n\nPreToolUse\n4개\ndev 서버 tmux 강제, tmux 사용 알림, git push 일시정지, .md 생성 차단\n\n\nPostToolUse\n5개\nPR URL 로그, Prettier 자동 포맷, TypeScript 체크, console.log 경고, Codiens 실시간 전송\n\n\nStop\n3개\nconsole.log 최종 감사, MEMORY.md 리마인더, Codiens 세션 동기화\n\n\nUserPromptSubmit\n1개\n학습 키워드 감지 시 MEMORY.md 업데이트 리마인더\n\n\nPreCompact\n1개\n컨텍스트 압축 전 MEMORY.md 강제 저장 알림\n\n\n\n\n\n\n코드 품질 Hook:\n\n.ts/.tsx/.js/.jsx 파일 편집 후 Prettier 자동 포맷\n.ts/.tsx 파일 편집 후 TypeScript 타입 체크 (tsc --noEmit)\nconsole.log 발견 시 경고, 세션 종료 시 최종 감사\ngit push 전 리뷰 일시정지 (Enter로 계속, Ctrl+C로 취소)\n\nCodiens Hook (사용 데이터 수집):\n\n도구 사용 시 실시간 데이터 전송 (cc-sync-realtime.mjs)\n세션 종료 시 전체 세션 데이터 동기화 (cc-sync-session.mjs)\n\nMemory Hook:\n\n학습 키워드(“실수”, “오류”, “에러” 등) 감지 시 MEMORY.md 업데이트 리마인더\n컨텍스트 압축 전 학습 내용 강제 저장 알림\n\n\n\n\n\n\n./verify.sh\n[OK] CLAUDE.md (global instructions)\n[OK] settings.json\n[OK] settings.local.json (optional)\n[OK] rules/ (11 files)\n[OK] agents/ (9 files)\n[OK] commands/ (35 files)\n[OK] hooks/ (4 files)\n[OK] 전체 rule/agent/hooks.json 검증 통과\n\n\n\n\n~/.claude/\n├── CLAUDE.md                    # 팀 공통 글로벌 지침\n├── settings.json                # Claude Code 설정\n├── settings.local.json          # 개인별 설정\n├── rules/                       # 코딩 규칙 (11개)\n├── agents/                      # 전문 에이전트 (9개)\n├── commands/                    # 슬래시 명령어 (35개)\n├── hooks/                       # 훅 스크립트\n├── skills/                      # bioSkills (425개)\n├── templates/speckit/           # SpecKit 템플릿\n└── backup-YYYYMMDD-HHMMSS/     # 기존 설정 백업\n\n~/.tmux.conf                     # tmux 설정\n\n\n\n구분\n수량\n\n\n\n\n에이전트\n9개\n\n\n슬래시 명령어\n35개 (기본 26 + SpecKit 9)\n\n\nbioSkills\n425개\n\n\n코딩 규칙 (rules)\n11개\n\n\nHook\n14개\n\n\n합계\n494개\n\n\n\n\n\n\n\n\n\n\n에이전트\n용도\n\n\n\n\nplanner\n기능 구현 계획 수립\n\n\narchitect\n시스템 설계 및 아키텍처\n\n\ntdd-guide\n테스트 주도 개발 가이드\n\n\ncode-reviewer\n코드 품질/보안 리뷰\n\n\nsecurity-reviewer\n보안 취약점 분석\n\n\nbuild-error-resolver\n빌드 에러 해결\n\n\ne2e-runner\nPlaywright E2E 테스팅\n\n\nrefactor-cleaner\n데드 코드 정리\n\n\ndoc-updater\n문서 업데이트\n\n\n\n\n\n\n\n\n\n설치 직후 Codiens 대시보드에 데이터가 전송되지 않는 문제가 있었습니다. 원인은 ~/.claude/hooks/lib/constants.mjs의 API URL 도메인 오타:\n// 수정 전 (오타)\nexport const API_BASE_URL = process.env.CC_API_URL || 'https://tech.dev.srarch.net/api/claude-code';\n\n// 수정 후\nexport const API_BASE_URL = process.env.CC_API_URL || 'https://tech.dev.sgarch.net/api/claude-code';\n\n\n\n\n\n\n\n\n\n\n문제\n해결\n\n\n\n\nSSH 클론 실패\nssh -T git@seegene으로 연결 확인\n\n\n설정 미적용\nClaude Code 재시작 (/exit 후 claude)\n\n\n기존 설정 충돌\n./uninstall.sh --restore로 백업 복원\n\n\ntmux 오류\ntmux kill-server 후 재시작\n\n\nHook 오류\nwhich node로 Node.js 확인 (Hook 실패해도 Claude Code에 영향 없음)\n\n\n\n\n\n\n\n\n이 설정을 통해 팀 전체가 동일한 규칙, 에이전트, Hook 시스템을 공유하며 Claude Code를 활용할 수 있습니다. 특히 바이오인포매틱스 팀에게는 425개의 bioSkills가 큰 도움이 됩니다.\n다음 포스트에서는 실제 프로젝트에서 Claude Code의 에이전트와 슬래시 명령어를 활용하는 워크플로우를 다루겠습니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#배경",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#배경",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "Claude Code는 Anthropic에서 만든 CLI 기반 AI 코딩 어시스턴트입니다. 터미널에서 직접 실행되며, 코드 작성/수정, 디버깅, Git 작업, 테스트 등 소프트웨어 엔지니어링 전반을 지원합니다.\n이 포스트에서는 팀 전체가 동일한 Claude Code 환경을 사용할 수 있도록 팀 설정 레포지토리를 클론하고, 모든 도구를 설치하는 과정을 기록합니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-ssh를-이용한-private-레포-클론",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-ssh를-이용한-private-레포-클론",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "팀 설정 레포는 GitHub 조직의 private 레포이므로 SSH 인증이 필요합니다.\n\n\n~/.ssh/config에 조직용 호스트를 등록합니다:\nHost seegene\n  User git\n  HostName github.com\n  Port 22\n  IdentityFile ~/.ssh/seegene\n\n\n\ngit clone git@seegene:seegene/claude-code-team-config.git\nSSH 호스트 별칭(seegene)을 사용하면 HTTPS 인증 없이도 private 레포에 접근할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-팀-기본-설정-설치-install.sh",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-팀-기본-설정-설치-install.sh",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "cd claude-code-team-config\n./install.sh\n이 스크립트가 ~/.claude/에 설치하는 항목:\n\n\n\n항목\n설명\n\n\n\n\nCLAUDE.md\n팀 공통 글로벌 지침 (한국어 응답, 코딩 스타일 등)\n\n\nsettings.json\n훅, 권한, Agent Teams 환경변수\n\n\nrules/ (9개)\n보안, 테스팅, Git 워크플로우 등 코딩 규칙\n\n\nagents/ (9개)\nplanner, architect, tdd-guide 등 전문 에이전트\n\n\ncommands/ (26개)\n/plan, /tdd, /code-review 등 슬래시 명령어\n\n\nhooks/\nPrettier 자동 포맷, TypeScript 체크, Codiens 데이터 수집\n\n\n\n기존 설정이 있으면 ~/.claude/backup-YYYYMMDD-HHMMSS/에 자동 백업됩니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-생산성-도구-설치",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-생산성-도구-설치",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "brew install tmux bat lsd fzf\n\n\n\n도구\n용도\n\n\n\n\ntmux\nAgent Teams 분할 창 모드에 필수\n\n\nbat\n문법 강조가 되는 cat 대체\n\n\nlsd\n아이콘이 있는 ls 대체\n\n\nfzf\n퍼지 검색 (Ctrl+R로 명령어 히스토리 검색)\n\n\n\n\n\n# 마우스 지원\nset -g mouse on\n\n# 256색 지원\nset -g default-terminal \"screen-256color\"\nset -ga terminal-overrides \",xterm-256color:Tc\"\n\n# 패널 분할 (직관적)\nbind | split-window -h\nbind - split-window -v\n\n# vim 스타일 패널 이동\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n# 히스토리 크기\nset -g history-limit 50000\n\n# 인덱스 1부터 시작\nset -g base-index 1\nsetw -g pane-base-index 1\ntmux는 Claude Code의 Agent Teams 기능에 필수입니다. 여러 에이전트가 각각 독립된 tmux 패널에서 동시에 작업하는 것을 시각적으로 확인할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-bioskills-설치-바이오인포매틱스-425개-스킬",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-bioskills-설치-바이오인포매틱스-425개-스킬",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "./install-bioskills.sh\nGPTomics/bioSkills 레포에서 바이오인포매틱스 전문 스킬을 설치합니다. 56개 카테고리, 425개 스킬이 ~/.claude/skills/에 설치됩니다.\n\n\n\n\n\n카테고리\n스킬 예시\n\n\n\n\nvariant-calling\nGATK, DeepVariant, VCF 처리\n\n\nsingle-cell\nscRNA-seq, trajectory, perturb-seq\n\n\nalignment\nSAM/BAM 처리, MSA, pairwise alignment\n\n\nchip-seq\npeak calling, motif analysis\n\n\nspatial-transcriptomics\n공간 전사체 분석\n\n\nworkflows\nNextflow, Snakemake, WDL 파이프라인\n\n\n\n설치 후 Claude Code에 자연어로 요청하면 해당 스킬이 자동 활성화됩니다:\n\"RNA-seq 카운트 데이터에서 차등 발현 유전자를 찾아줘\"\n\"BAM 파일에서 변이를 호출해줘\""
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-speckit-설치-spec-driven-development",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-speckit-설치-spec-driven-development",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "./install-speckit.sh\nGitHub의 spec-kit 기반 Spec-Driven Development 도구를 설치합니다.\n\n\n\n\n\n명령어\n기능\n\n\n\n\n/speckit-constitution\n프로젝트 원칙 수립\n\n\n/speckit-specify\n기능 명세 작성\n\n\n/speckit-plan\n구현 계획 생성\n\n\n/speckit-tasks\n태스크 분해\n\n\n/speckit-taskstoissues\n태스크를 이슈로 변환\n\n\n/speckit-implement\n명세 기반 구현\n\n\n/speckit-checklist\nQA 체크리스트 생성\n\n\n/speckit-analyze\n기존 코드 분석\n\n\n/speckit-clarify\n요구사항 명확화"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-hook-시스템",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-hook-시스템",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "팀 설정에는 14개의 Hook이 포함되어 있습니다. Hook은 Claude Code의 도구 실행 전후에 자동으로 실행되는 스크립트입니다.\n\n\n\n\n\n\n\n\n\n\n타이밍\n수량\n주요 동작\n\n\n\n\nPreToolUse\n4개\ndev 서버 tmux 강제, tmux 사용 알림, git push 일시정지, .md 생성 차단\n\n\nPostToolUse\n5개\nPR URL 로그, Prettier 자동 포맷, TypeScript 체크, console.log 경고, Codiens 실시간 전송\n\n\nStop\n3개\nconsole.log 최종 감사, MEMORY.md 리마인더, Codiens 세션 동기화\n\n\nUserPromptSubmit\n1개\n학습 키워드 감지 시 MEMORY.md 업데이트 리마인더\n\n\nPreCompact\n1개\n컨텍스트 압축 전 MEMORY.md 강제 저장 알림\n\n\n\n\n\n\n코드 품질 Hook:\n\n.ts/.tsx/.js/.jsx 파일 편집 후 Prettier 자동 포맷\n.ts/.tsx 파일 편집 후 TypeScript 타입 체크 (tsc --noEmit)\nconsole.log 발견 시 경고, 세션 종료 시 최종 감사\ngit push 전 리뷰 일시정지 (Enter로 계속, Ctrl+C로 취소)\n\nCodiens Hook (사용 데이터 수집):\n\n도구 사용 시 실시간 데이터 전송 (cc-sync-realtime.mjs)\n세션 종료 시 전체 세션 데이터 동기화 (cc-sync-session.mjs)\n\nMemory Hook:\n\n학습 키워드(“실수”, “오류”, “에러” 등) 감지 시 MEMORY.md 업데이트 리마인더\n컨텍스트 압축 전 학습 내용 강제 저장 알림"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-설치-검증",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#단계-설치-검증",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "./verify.sh\n[OK] CLAUDE.md (global instructions)\n[OK] settings.json\n[OK] settings.local.json (optional)\n[OK] rules/ (11 files)\n[OK] agents/ (9 files)\n[OK] commands/ (35 files)\n[OK] hooks/ (4 files)\n[OK] 전체 rule/agent/hooks.json 검증 통과"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#최종-설치-현황",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#최종-설치-현황",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "~/.claude/\n├── CLAUDE.md                    # 팀 공통 글로벌 지침\n├── settings.json                # Claude Code 설정\n├── settings.local.json          # 개인별 설정\n├── rules/                       # 코딩 규칙 (11개)\n├── agents/                      # 전문 에이전트 (9개)\n├── commands/                    # 슬래시 명령어 (35개)\n├── hooks/                       # 훅 스크립트\n├── skills/                      # bioSkills (425개)\n├── templates/speckit/           # SpecKit 템플릿\n└── backup-YYYYMMDD-HHMMSS/     # 기존 설정 백업\n\n~/.tmux.conf                     # tmux 설정\n\n\n\n구분\n수량\n\n\n\n\n에이전트\n9개\n\n\n슬래시 명령어\n35개 (기본 26 + SpecKit 9)\n\n\nbioSkills\n425개\n\n\n코딩 규칙 (rules)\n11개\n\n\nHook\n14개\n\n\n합계\n494개"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#에이전트-목록",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#에이전트-목록",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "에이전트\n용도\n\n\n\n\nplanner\n기능 구현 계획 수립\n\n\narchitect\n시스템 설계 및 아키텍처\n\n\ntdd-guide\n테스트 주도 개발 가이드\n\n\ncode-reviewer\n코드 품질/보안 리뷰\n\n\nsecurity-reviewer\n보안 취약점 분석\n\n\nbuild-error-resolver\n빌드 에러 해결\n\n\ne2e-runner\nPlaywright E2E 테스팅\n\n\nrefactor-cleaner\n데드 코드 정리\n\n\ndoc-updater\n문서 업데이트"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#트러블슈팅",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#트러블슈팅",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "설치 직후 Codiens 대시보드에 데이터가 전송되지 않는 문제가 있었습니다. 원인은 ~/.claude/hooks/lib/constants.mjs의 API URL 도메인 오타:\n// 수정 전 (오타)\nexport const API_BASE_URL = process.env.CC_API_URL || 'https://tech.dev.srarch.net/api/claude-code';\n\n// 수정 후\nexport const API_BASE_URL = process.env.CC_API_URL || 'https://tech.dev.sgarch.net/api/claude-code';\n\n\n\n\n\n\n\n\n\n\n문제\n해결\n\n\n\n\nSSH 클론 실패\nssh -T git@seegene으로 연결 확인\n\n\n설정 미적용\nClaude Code 재시작 (/exit 후 claude)\n\n\n기존 설정 충돌\n./uninstall.sh --restore로 백업 복원\n\n\ntmux 오류\ntmux kill-server 후 재시작\n\n\nHook 오류\nwhich node로 Node.js 확인 (Hook 실패해도 Claude Code에 영향 없음)"
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#마무리",
    "href": "docs/blog/posts/Claude_Code/01-팀_설정_구축/01-Claude_Code_팀_설정_구축.html#마무리",
    "title": "Claude Code Max 팀 설정 완전 가이드",
    "section": "",
    "text": "이 설정을 통해 팀 전체가 동일한 규칙, 에이전트, Hook 시스템을 공유하며 Claude Code를 활용할 수 있습니다. 특히 바이오인포매틱스 팀에게는 425개의 bioSkills가 큰 도움이 됩니다.\n다음 포스트에서는 실제 프로젝트에서 Claude Code의 에이전트와 슬래시 명령어를 활용하는 워크플로우를 다루겠습니다."
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/index.html",
    "href": "docs/blog/posts/Azure_Cloud/index.html",
    "title": "Azure Cloud",
    "section": "",
    "text": "01-Data Lake Storage",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Azure Cloud"
    ]
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/index.html#azure-cloud",
    "href": "docs/blog/posts/Azure_Cloud/index.html#azure-cloud",
    "title": "Azure Cloud",
    "section": "",
    "text": "01-Data Lake Storage",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Azure Cloud"
    ]
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html",
    "title": "01-Data Lake Storage",
    "section": "",
    "text": "Azure Portal에서 Storage Account를 생성합니다.\n\n\n\n\n\n\n\nimage\n\n\n구독(Subscription) - 설명: 사용할 Azure 구독을 선택합니다. - 입력 값: swlab-test-subs\n리소스 그룹(Resource Group) - 설명: 스토리지 계정을 포함할 리소스 그룹을 선택하거나 새로 만듭니다. - 입력 값: rg-aipoc-test-krc-001\n\n\n\n스토리지 계정 이름 - 설명: 전역적으로 고유한 스토리지 계정 이름을 입력합니다 (3-24자, 소문자와 숫자만 사용 가능). - 입력 값: saaipoctestkrc001\n지역(Region) - 설명: 스토리지 계정이 생성될 Azure 데이터 센터 위치를 선택합니다. - 입력 값: (Asia Pacific) Korea Central\n기본 스토리지 유형 - 설명: 사용할 스토리지 서비스 유형을 선택합니다. - 입력 값: Azure Blob Storage 또는 Azure Data Lake Storage Gen 2 - 참고: 관련 지침을 제공하는 데 도움이 됩니다. 스토리지를 이 리스크 유형으로 제한하지 않습니다.\n기본 워크로드 - 설명: 빅 데이터 분석을 위해 데이터 레이크를 호스트하는 데 가장 적합한 워크로드와 가장 일치하는 항목을 선택하여 모범 사례를 기반으로 구축된 권장 구성을 가져옵니다. 안제든지 이 구성을 변경할 수 있습니다. - 입력 값: 빅 데이터 분석 - 참고: 📊 빅 데이터 분석을 위해 데이터 레이크를 호스트하는 데 가장 적합\n성능(Performance) - 설명: 스토리지 계정의 성능 계층을 선택합니다. - 표준: 대부분 시나리오에 권장됨 (범용 v2 계정) - 프리미엄: 짧은 대기 시간이 필요한 경우에 권장됨 - 선택 값: 표준: 대부분 시나리오에 권장됨(범용 v2 계정) - 참고: 📊 표준 성능 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n중복도(Redundancy) - 설명: 데이터 내구성을 위한 복제 전략을 선택합니다. - LRS: 로컬 중복 스토리지 - ZRS: 영역 중복 스토리지 - GRS: 지역 중복 스토리지 - RA-GRS: 읽기 액세스 지역 중복 스토리지 - 선택 값: LRS(로컬 중복 스토리지) - 참고: 📊 ZRS 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n\n\n\n\n\n\n\nimage\n\n\n\n\nREST API 작업을 위한 보안 전송 필요 - 설명: HTTPS를 통한 보안 연결만 허용합니다. - 선택 값: ☑ 체크됨\n개별 컨테이너에 대한 익명 액세스 허용 - 설명: 익명 Blob 액세스를 컨테이너 수준에서 허용할지 여부를 설정합니다. - 선택 값: ☐ 체크 안 됨\n스토리지 계정 키 액세스 사용 - 설명: 공유 키를 통한 스토리지 계정 액세스를 허용합니다. - 선택 값: ☑ 체크됨\nAzure Portal에서 Microsoft Entra 인증 기본값 사용 - 설명: Azure Portal 액세스 시 Microsoft Entra ID를 기본 인증 방법으로 사용합니다. - 선택 값: ☐ 체크 안 됨\n최소 TLS 버전 - 설명: 클라이언트에서 요청하는 최소 TLS(전송 계층 보안) 버전을 설정합니다. - 선택 값: 버전 1.2\n복사 작업에 대해 허용된 범위(미리 보기) - 설명: 데이터 복사 작업의 허용 범위를 지정합니다. - 선택 값: 모든 스토리지 계정에서\n\n\n\nData Lake Storage Gen2 엔드포인트로 보안되는 계층 구조 네임스페이스는 파일 및 디렉터리 의미 체계를 사용하고, 빅 데이터 분석 워크로드를 가속화하고, ACL(액세스 제어 목록)을 사용합니다.\n계층 구조 네임스페이스 사용 - 설명: Data Lake Storage Gen2 엔드포인트를 보안되는 계층 구조 네임스페이스입니다. 파일 및 디렉터리 의미 체계를 사용하고, ACL(액세스 제어 목록)을 사용합니다. - 선택 값: ☑ 체크됨 - 참고: 📊 계층 구조 네임스페이스 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n\n\n\nBLOB 및 Data Lake Gen2 엔드포인트는 기본적으로 프로토비전됨\nSFTP 사용 - 설명: SFTP 프로토콜을 통한 파일 전송을 활성화합니다. - 선택 값: ☑ 체크됨 - 참고: ℹ️ 로컬 사용자 기능은 SFTP로 활성화됩니다. 스토리지 계정을 만든 후 SFTP 끝점에 액세스할 로컬 사용자 ID를 만듭니다.\n네트워크 파일 시스템 v3 사용 - 설명: NFS v3 프로토콜을 사용하여 파일 시스템을 마운트합니다. - 선택 값: ☑ 체크됨\n\n\n\n테넌트 간 복제 허용 - 설명: 다른 Azure AD 테넌트 간 Blob 복제를 허용합니다. - 선택 값: ☐ 체크 안 됨 - 참고: ℹ️ 테넌트 간 복제 및 계층 구조 네임스페이스를 동시에 사용하도록 설정할 수 없습니다.\n액세스 계층 - 설명: 데이터 액세스 빈도에 따른 기본 저장소 계층을 선택합니다. - 핫: 자주 액세스하는 데이터 및 일상적인 사용 시나리오에 최적화됨 - 쿨: 자주 액세스하지 않는 데이터 및 백업 시나리오에 최적화됨 - 콜드: 거의 액세스하지 않는 데이터 및 백업 시나리오에 최적화됨 - 선택 값: 핫 - 참고: 📊 핫 Blob 액세스 계층 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n\n\n\n큰 파일 공유 사용 - 설명: 최대 100TiB 용량의 대용량 파일 공유를 지원합니다. - 선택 값: ☑ 체크됨 (비활성화됨)\n\n\n\n\n\n\n\nimage\n\n\n\n\n공용 네트워크를 통해 어디서나 리소스에 액세스합니다.\n참고: 공용 네트워크를 통해 리소스에 액세스할 수 있게 되면 보안 위험이 증가합니다.\n공용 네트워크 액세스 - 설명: 공용 인터넷에서 스토리지 계정에 대한 액세스 권한을 설정합니다. - 선택 값: 사용 - 이 리소스에 대한 리스크 액세스 구성을 사용하여 선택 인바운드 엔드포인트를 제한하는 옵션을 사용하며 인바운드 및 아웃바운드 엔드포인트를 허용합니다. - 옵션: - ⭕ 사용: 이 리소스에 대한 리스크 액세스 구성을 사용하여 선택 인바운드 엔드포인트를 제한하는 옵션을 사용하며 인바운드 및 아웃바운드 엔드포인트를 허용합니다. - ⚪ 사용 안 함: 아웃바운드 엔드포인트를 허용하면서 인바운드 엔드포인트를 제한합니다. - ⚪ 경계로 보호(가장 제한됨): 네트워크 보안 경계를 사용하여 인바운드 및 아웃바운드 엔드포인트를 제한합니다. 경계로 보호는 리소스를 보호하기 위해 가장 높은 수준의 인바운드 및 아웃바운드 제한을 제공합니다.\n공용 네트워크 액세스 범위 - 설명: 공용 네트워크를 통해 액세스할 수 있는 리소스의 범위를 지정합니다. - 선택 값: 선택한 가상 네트워크 및 IP 주소에서 사용 - 옵션: - ⚪ 모든 네트워크에서 사용 - ⭕ 선택한 가상 네트워크 및 IP 주소에서 사용\n\n\n\n선택한 네트워크만 이 스토리지 계정에 액세스할 수 있습니다.\n가상 네트워크 구독 - 설명: 가상 네트워크가 속한 Azure 구독을 선택합니다. - 입력 값: swlab-test-subs\n가상 네트워크 - 설명: 스토리지 계정에 액세스할 수 있는 가상 네트워크를 선택합니다. - 입력 값: vnet-aipoc-test-krc-insilico-001 - 링크: [가상 네트워크 만들기] / [선택한 가상 네트워크 관리]\n서브넷 - 설명: 가상 네트워크 내에서 액세스를 허용할 서브넷을 선택합니다. - 입력 값: subnet-aipoc-test-krc-insilico-001(172.16.0.0/24)\n\n\n\nAllow select public internet IP addresses to access your resource.\n허용된 IP 주소 - 설명: 스토리지 계정에 액세스할 수 있는 공용 IP 주소를 지정합니다. - 입력 값: 61.74.175.54\n\n\n\n\n\n\n경고⚠️ 중요\n\n\n\n이 값을 자신의 로컬 PC Public IP로 설정을 해 주어야지만, 추후 Azure Portal을 통해 파일 업로드가 가능합니다.\n\n\n\n\n\n프라이빗 엔드포인트 - 설명: 프라이빗 엔드포인트를 만들어 이 리소스에 대한 프라이빗 연결을 허용합니다. 추가 프라이빗 엔드포인트 연결은 스토리지 계정 또는 프라이빗 링크 센터 내에서 만들 수 있습니다. - 선택 값: [+ 프라이빗 엔드포인트 추가] 클릭 가능 - 상태: 프라이빗 엔드포인트를 만들려면 (추가를 클릭합니다)\n\n\n\n트래픽이 원본에서 Azure 엔드포인트로 이동하는 과정에서 트래픽을 라우팅할 방법을 결정하세요. 대부분의 고객의 경우 Microsoft 네트워크 라우팅이 권장됩니다.\n라우팅 기본 설정 - 설명: 트래픽이 원본에서 Azure 엔드포인트로 이동하는 과정에서 트래픽을 라우팅할 방법을 결정합니다. - 선택 값: Microsoft 네트워크 라우팅 - 옵션: - ⭕ Microsoft 네트워크 라우팅 - ⚪ 인터넷 라우팅\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n전부 디폴트 설정으로 진행\n\nBlob에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 덮어쓴 Blob을 포함하여 이전에 삭제로 표시되었던 Blob을 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 Blob 보존 기간(일): 7\n컨테이너에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 이전에 삭제로 표시된 컨테이너를 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 컨테이너 보존 기간(일): 7 - 참고: 자주 덮어쓰는 데이터에 대해 일시 삭제를 사용하도록 설정하면 스토리지 비용이 증가할 수 있습니다.\n파일 공유에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 이전에 삭제로 표시된 파일 공유를 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 파일 공유 보존 기간(일): 7\n\n\n\nBlob에 버전 관리 사용 - 설명: 버전 관리를 사용하여 Blob의 이전 버전을 자동으로 유지합니다. - 선택 값: ☐ 체크 안 됨 - 참고: 워크로드, 생성된 버전 수에 미치는 영향, 결과 비용을 고려하세요. 데이터 수명 주기를 자동으로 관리하여 비용을 최적화합니다.\nBlob 변경 피드 사용 - 설명: 계정의 Blob에 대한 만들기, 수정 및 삭제 변경 내용을 추적합니다. - 선택 값: ☐ 체크 안 됨\n\n\n\n버전 수준 불변성 지원 사용 - 설명: 모든 Blob 버전에 적용할 계정 수준에서 시간 기반 보존 정책을 설정할 수 있습니다. - 선택 값: ☐ 체크 안 됨 - 참고: 계정 수준에서 기본 정책을 설정하려면 이 기능을 사용하여 도록 설정합니다. 이 기능을 사용하지 않고도 컨테이너 수준에서 기본 정책을 설정하거나 특정 Blob 버전에 대한 정책을 설정할 수 있습니다. 이 속성을 사용하려면 버전 관리를 필요합니다."
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#기본-설정",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#기본-설정",
    "title": "01-Data Lake Storage",
    "section": "",
    "text": "image\n\n\n구독(Subscription) - 설명: 사용할 Azure 구독을 선택합니다. - 입력 값: swlab-test-subs\n리소스 그룹(Resource Group) - 설명: 스토리지 계정을 포함할 리소스 그룹을 선택하거나 새로 만듭니다. - 입력 값: rg-aipoc-test-krc-001\n\n\n\n스토리지 계정 이름 - 설명: 전역적으로 고유한 스토리지 계정 이름을 입력합니다 (3-24자, 소문자와 숫자만 사용 가능). - 입력 값: saaipoctestkrc001\n지역(Region) - 설명: 스토리지 계정이 생성될 Azure 데이터 센터 위치를 선택합니다. - 입력 값: (Asia Pacific) Korea Central\n기본 스토리지 유형 - 설명: 사용할 스토리지 서비스 유형을 선택합니다. - 입력 값: Azure Blob Storage 또는 Azure Data Lake Storage Gen 2 - 참고: 관련 지침을 제공하는 데 도움이 됩니다. 스토리지를 이 리스크 유형으로 제한하지 않습니다.\n기본 워크로드 - 설명: 빅 데이터 분석을 위해 데이터 레이크를 호스트하는 데 가장 적합한 워크로드와 가장 일치하는 항목을 선택하여 모범 사례를 기반으로 구축된 권장 구성을 가져옵니다. 안제든지 이 구성을 변경할 수 있습니다. - 입력 값: 빅 데이터 분석 - 참고: 📊 빅 데이터 분석을 위해 데이터 레이크를 호스트하는 데 가장 적합\n성능(Performance) - 설명: 스토리지 계정의 성능 계층을 선택합니다. - 표준: 대부분 시나리오에 권장됨 (범용 v2 계정) - 프리미엄: 짧은 대기 시간이 필요한 경우에 권장됨 - 선택 값: 표준: 대부분 시나리오에 권장됨(범용 v2 계정) - 참고: 📊 표준 성능 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n중복도(Redundancy) - 설명: 데이터 내구성을 위한 복제 전략을 선택합니다. - LRS: 로컬 중복 스토리지 - ZRS: 영역 중복 스토리지 - GRS: 지역 중복 스토리지 - RA-GRS: 읽기 액세스 지역 중복 스토리지 - 선택 값: LRS(로컬 중복 스토리지) - 참고: 📊 ZRS 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다."
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#고급-설정",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#고급-설정",
    "title": "01-Data Lake Storage",
    "section": "",
    "text": "image\n\n\n\n\nREST API 작업을 위한 보안 전송 필요 - 설명: HTTPS를 통한 보안 연결만 허용합니다. - 선택 값: ☑ 체크됨\n개별 컨테이너에 대한 익명 액세스 허용 - 설명: 익명 Blob 액세스를 컨테이너 수준에서 허용할지 여부를 설정합니다. - 선택 값: ☐ 체크 안 됨\n스토리지 계정 키 액세스 사용 - 설명: 공유 키를 통한 스토리지 계정 액세스를 허용합니다. - 선택 값: ☑ 체크됨\nAzure Portal에서 Microsoft Entra 인증 기본값 사용 - 설명: Azure Portal 액세스 시 Microsoft Entra ID를 기본 인증 방법으로 사용합니다. - 선택 값: ☐ 체크 안 됨\n최소 TLS 버전 - 설명: 클라이언트에서 요청하는 최소 TLS(전송 계층 보안) 버전을 설정합니다. - 선택 값: 버전 1.2\n복사 작업에 대해 허용된 범위(미리 보기) - 설명: 데이터 복사 작업의 허용 범위를 지정합니다. - 선택 값: 모든 스토리지 계정에서\n\n\n\nData Lake Storage Gen2 엔드포인트로 보안되는 계층 구조 네임스페이스는 파일 및 디렉터리 의미 체계를 사용하고, 빅 데이터 분석 워크로드를 가속화하고, ACL(액세스 제어 목록)을 사용합니다.\n계층 구조 네임스페이스 사용 - 설명: Data Lake Storage Gen2 엔드포인트를 보안되는 계층 구조 네임스페이스입니다. 파일 및 디렉터리 의미 체계를 사용하고, ACL(액세스 제어 목록)을 사용합니다. - 선택 값: ☑ 체크됨 - 참고: 📊 계층 구조 네임스페이스 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n\n\n\nBLOB 및 Data Lake Gen2 엔드포인트는 기본적으로 프로토비전됨\nSFTP 사용 - 설명: SFTP 프로토콜을 통한 파일 전송을 활성화합니다. - 선택 값: ☑ 체크됨 - 참고: ℹ️ 로컬 사용자 기능은 SFTP로 활성화됩니다. 스토리지 계정을 만든 후 SFTP 끝점에 액세스할 로컬 사용자 ID를 만듭니다.\n네트워크 파일 시스템 v3 사용 - 설명: NFS v3 프로토콜을 사용하여 파일 시스템을 마운트합니다. - 선택 값: ☑ 체크됨\n\n\n\n테넌트 간 복제 허용 - 설명: 다른 Azure AD 테넌트 간 Blob 복제를 허용합니다. - 선택 값: ☐ 체크 안 됨 - 참고: ℹ️ 테넌트 간 복제 및 계층 구조 네임스페이스를 동시에 사용하도록 설정할 수 없습니다.\n액세스 계층 - 설명: 데이터 액세스 빈도에 따른 기본 저장소 계층을 선택합니다. - 핫: 자주 액세스하는 데이터 및 일상적인 사용 시나리오에 최적화됨 - 쿨: 자주 액세스하지 않는 데이터 및 백업 시나리오에 최적화됨 - 콜드: 거의 액세스하지 않는 데이터 및 백업 시나리오에 최적화됨 - 선택 값: 핫 - 참고: 📊 핫 Blob 액세스 계층 워크로드에는 빅 데이터 분석(를) 사용하는 것이 좋습니다.\n\n\n\n큰 파일 공유 사용 - 설명: 최대 100TiB 용량의 대용량 파일 공유를 지원합니다. - 선택 값: ☑ 체크됨 (비활성화됨)"
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#네트워킹-설정",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#네트워킹-설정",
    "title": "01-Data Lake Storage",
    "section": "",
    "text": "image\n\n\n\n\n공용 네트워크를 통해 어디서나 리소스에 액세스합니다.\n참고: 공용 네트워크를 통해 리소스에 액세스할 수 있게 되면 보안 위험이 증가합니다.\n공용 네트워크 액세스 - 설명: 공용 인터넷에서 스토리지 계정에 대한 액세스 권한을 설정합니다. - 선택 값: 사용 - 이 리소스에 대한 리스크 액세스 구성을 사용하여 선택 인바운드 엔드포인트를 제한하는 옵션을 사용하며 인바운드 및 아웃바운드 엔드포인트를 허용합니다. - 옵션: - ⭕ 사용: 이 리소스에 대한 리스크 액세스 구성을 사용하여 선택 인바운드 엔드포인트를 제한하는 옵션을 사용하며 인바운드 및 아웃바운드 엔드포인트를 허용합니다. - ⚪ 사용 안 함: 아웃바운드 엔드포인트를 허용하면서 인바운드 엔드포인트를 제한합니다. - ⚪ 경계로 보호(가장 제한됨): 네트워크 보안 경계를 사용하여 인바운드 및 아웃바운드 엔드포인트를 제한합니다. 경계로 보호는 리소스를 보호하기 위해 가장 높은 수준의 인바운드 및 아웃바운드 제한을 제공합니다.\n공용 네트워크 액세스 범위 - 설명: 공용 네트워크를 통해 액세스할 수 있는 리소스의 범위를 지정합니다. - 선택 값: 선택한 가상 네트워크 및 IP 주소에서 사용 - 옵션: - ⚪ 모든 네트워크에서 사용 - ⭕ 선택한 가상 네트워크 및 IP 주소에서 사용\n\n\n\n선택한 네트워크만 이 스토리지 계정에 액세스할 수 있습니다.\n가상 네트워크 구독 - 설명: 가상 네트워크가 속한 Azure 구독을 선택합니다. - 입력 값: swlab-test-subs\n가상 네트워크 - 설명: 스토리지 계정에 액세스할 수 있는 가상 네트워크를 선택합니다. - 입력 값: vnet-aipoc-test-krc-insilico-001 - 링크: [가상 네트워크 만들기] / [선택한 가상 네트워크 관리]\n서브넷 - 설명: 가상 네트워크 내에서 액세스를 허용할 서브넷을 선택합니다. - 입력 값: subnet-aipoc-test-krc-insilico-001(172.16.0.0/24)\n\n\n\nAllow select public internet IP addresses to access your resource.\n허용된 IP 주소 - 설명: 스토리지 계정에 액세스할 수 있는 공용 IP 주소를 지정합니다. - 입력 값: 61.74.175.54\n\n\n\n\n\n\n경고⚠️ 중요\n\n\n\n이 값을 자신의 로컬 PC Public IP로 설정을 해 주어야지만, 추후 Azure Portal을 통해 파일 업로드가 가능합니다.\n\n\n\n\n\n프라이빗 엔드포인트 - 설명: 프라이빗 엔드포인트를 만들어 이 리소스에 대한 프라이빗 연결을 허용합니다. 추가 프라이빗 엔드포인트 연결은 스토리지 계정 또는 프라이빗 링크 센터 내에서 만들 수 있습니다. - 선택 값: [+ 프라이빗 엔드포인트 추가] 클릭 가능 - 상태: 프라이빗 엔드포인트를 만들려면 (추가를 클릭합니다)\n\n\n\n트래픽이 원본에서 Azure 엔드포인트로 이동하는 과정에서 트래픽을 라우팅할 방법을 결정하세요. 대부분의 고객의 경우 Microsoft 네트워크 라우팅이 권장됩니다.\n라우팅 기본 설정 - 설명: 트래픽이 원본에서 Azure 엔드포인트로 이동하는 과정에서 트래픽을 라우팅할 방법을 결정합니다. - 선택 값: Microsoft 네트워크 라우팅 - 옵션: - ⭕ Microsoft 네트워크 라우팅 - ⚪ 인터넷 라우팅"
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#데이터-보호-설정",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#데이터-보호-설정",
    "title": "01-Data Lake Storage",
    "section": "",
    "text": "image\n\n\n\n\n\n전부 디폴트 설정으로 진행\n\nBlob에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 덮어쓴 Blob을 포함하여 이전에 삭제로 표시되었던 Blob을 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 Blob 보존 기간(일): 7\n컨테이너에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 이전에 삭제로 표시된 컨테이너를 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 컨테이너 보존 기간(일): 7 - 참고: 자주 덮어쓰는 데이터에 대해 일시 삭제를 사용하도록 설정하면 스토리지 비용이 증가할 수 있습니다.\n파일 공유에 일시 삭제 사용 - 설명: 일시 삭제를 사용하면 이전에 삭제로 표시된 파일 공유를 복구할 수 있습니다. - 선택 값: ☑ 체크됨 - 삭제된 파일 공유 보존 기간(일): 7\n\n\n\nBlob에 버전 관리 사용 - 설명: 버전 관리를 사용하여 Blob의 이전 버전을 자동으로 유지합니다. - 선택 값: ☐ 체크 안 됨 - 참고: 워크로드, 생성된 버전 수에 미치는 영향, 결과 비용을 고려하세요. 데이터 수명 주기를 자동으로 관리하여 비용을 최적화합니다.\nBlob 변경 피드 사용 - 설명: 계정의 Blob에 대한 만들기, 수정 및 삭제 변경 내용을 추적합니다. - 선택 값: ☐ 체크 안 됨\n\n\n\n버전 수준 불변성 지원 사용 - 설명: 모든 Blob 버전에 적용할 계정 수준에서 시간 기반 보존 정책을 설정할 수 있습니다. - 선택 값: ☐ 체크 안 됨 - 참고: 계정 수준에서 기본 정책을 설정하려면 이 기능을 사용하여 도록 설정합니다. 이 기능을 사용하지 않고도 컨테이너 수준에서 기본 정책을 설정하거나 특정 Blob 버전에 대한 정책을 설정할 수 있습니다. 이 속성을 사용하려면 버전 관리를 필요합니다."
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#azure-portal에서-컨테이너를-생성합니다.",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#azure-portal에서-컨테이너를-생성합니다.",
    "title": "01-Data Lake Storage",
    "section": "2.1 Azure Portal에서 컨테이너를 생성합니다.",
    "text": "2.1 Azure Portal에서 컨테이너를 생성합니다.\nbc-aipoc-test-krc001 이름의 컨테이너가 이미 존재하여 생성되지 않지만 아래와 같은 방법으로 컨테이너를 생성할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#디렉터리-추가",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#디렉터리-추가",
    "title": "01-Data Lake Storage",
    "section": "2.2 디렉터리 추가",
    "text": "2.2 디렉터리 추가\n컨테이너 내부로 접속하여 디렉터리를 추가할 수 있습니다. ::: {.callout-important} ## ⚠️ 주의사항\n네트워크 설정 중 Public network access가 Enabled from selected networks로 설정되어 있어야 합니다.\nDisable 또는 Secured by perimeter(Most restricted)로 설정되어 있으면 컨테이너 내부로 접속할 수 없습니다. :::\n폴더 구조는 아래와 같은 구조로 되어있습니다.\nrag-container\n ├─ code\n │   ├─ moduleA\n │   ├─ moduleB\n │   └─ moduleC\n └─ docs\n     ├─ moduleA\n     ├─ moduleB\n     └─ moduleC"
  },
  {
    "objectID": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#blob-파일을-업로드합니다.",
    "href": "docs/blog/posts/Azure_Cloud/01-클라우드_인프라_설정/01-Data_Lake_Storage.html#blob-파일을-업로드합니다.",
    "title": "01-Data Lake Storage",
    "section": "2.3 Blob 파일을 업로드합니다.",
    "text": "2.3 Blob 파일을 업로드합니다.\n 업로드 버튼을 이용해서 미리 다운로드 받은 코드 파일을 업로드합니다."
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/index.html",
    "href": "docs/blog/posts/Claude_Code/index.html",
    "title": "Claude Code",
    "section": "",
    "text": "01-Claude Code Max 팀 설정 완전 가이드",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Claude Code"
    ]
  },
  {
    "objectID": "docs/blog/posts/Claude_Code/index.html#claude-code",
    "href": "docs/blog/posts/Claude_Code/index.html#claude-code",
    "title": "Claude Code",
    "section": "",
    "text": "01-Claude Code Max 팀 설정 완전 가이드",
    "crumbs": [
      "Blog",
      "Blog Posts",
      "Claude Code"
    ]
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "인실리코(in-silico) 알고리즘 코드를 분석하는 코드 설명 Agent를 구축하려면, Agent가 코드를 이해하기 위한 구조화된 메타데이터가 필요합니다.\n여기서 말하는 메타데이터란:\n\n모든 클래스/함수/변수의 물리명, 논리명, 데이터 타입, 기본값, 의미\n모듈 간 입출력 연결 관계\n파이프라인 단계 간 의존성\n중간 산출물의 파일 스키마 (구분자, 인코딩, 컬럼 정의)\n알고리즘 파라미터, 임계값, 허용 범위\n암묵적 규칙 (코드에 명시되지 않은 관계, 상태 전이 조건)\n\n이 메타데이터를 14개 카테고리로 분류하여 METADATA_INVESTIGATION.md라는 단일 문서로 생성하는 것이 목표입니다.\n\n\n\n\n\n\n\n\n\n\n\n항목\n값\n\n\n\n\n레포지토리\ninsilico-core-automsa\n\n\nPython 파일 수\n152개\n\n\n총 코드 줄 수\n~29,178줄\n\n\n파이프라인 단계\n9개 서비스 (5 Step)\n\n\n역할\n유전자/단백질 키워드 기반 서열 수집, BLAST 확장, 대표 서열 선정, 다중 서열 정렬(MSA) 자동화\n\n\n\n파이프라인 구조:\nAutoMSA -&gt; DRS -&gt; monoom -&gt; multiom -&gt; multiom-output -&gt; multitom -&gt; multiplex-excel\n이 규모의 코드베이스에서 원자적 수준의 메타데이터를 추출하면 1,400개 이상의 항목이 나옵니다.\n\n\n\n\n\n\n하나의 Claude Code 에이전트에게 전체 152개 파일을 분석하라고 요청했습니다.\n\n\n\nContext window 한계에 도달. 약 50개 파일을 읽은 시점에서 컨텍스트가 부족해져, 나머지 100개 파일은 조사되지 않았습니다.\n\n\n\n\n대규모 코드베이스는 단일 에이전트로 처리할 수 없다. Context window는 유한하고, 152개 파일 x 평균 192줄 = ~29K줄의 코드를 한 번에 읽고 분석하는 것은 불가능합니다.\n\n\n\n\n\n\n\n\n14개 카테고리를 3개 그룹으로 나누어 병렬 실행:\n\nAgent A: Cat 1-3 (파이프라인 구조, 중간 산출물)\nAgent B: Cat 4-7 (입력/파라미터, DB, 품질)\nAgent C: Cat 8-14 (라이브러리, 환경, 인프라, 로깅, 기타)\n\n\n\n\n3개 에이전트 모두 완료. 그러나 산출물의 형식이 프롬프트와 불일치:\n\n\n\n\n\n\n\n문제\n상세\n\n\n\n\nN.1-N.4 구조 누락\n각 카테고리에 “추출 가능 이유”, “조사 대상 파일”, “해석 메타데이터” 섹션이 없음\n\n\n테이블 헤더 영문\n한국어 8열 헤더 대신 영어 헤더 사용\n\n\n통계표 누락\n최종 카테고리별 항목 수 집계표 없음\n\n\n품질 체크리스트 누락\n12개 항목 체크리스트 없음\n\n\n\n\n\n\n\nLLM은 추상적 규칙보다 구체적 예시를 훨씬 잘 따른다. 프롬프트에 “N.1, N.2, N.3, N.4 구조로 작성하세요”라고 써도, 실제 완성된 예시가 없으면 에이전트가 자의적으로 해석합니다.\n\n\n\n\n\n\n\n\n\n참조 문서(multiom 레포의 METADATA_INVESTIGATION.md)를 Gold Standard로 제공\n7개 에이전트로 더 세밀하게 분할:\n\nCat 1 / Cat 2-3 / Cat 4 / Cat 5-6 / Cat 7-8 / Cat 9-11 / Cat 12-14\n\n각 에이전트에게 구체적 형식 예시와 참조 문서를 함께 전달\n\n\n\n\n\n\n\n카테고리\n항목 수\n\n\n\n\n1. 모듈 간 I/O\n213\n\n\n2. 파이프라인 의존성\n62\n\n\n3. 중간 산출물\n218\n\n\n4. 입력 JSON/UI\n128\n\n\n5. 알고리즘 파라미터\n129\n\n\n6. DB 스키마/쿼리\n110\n\n\n7. 품질 규칙\n99\n\n\n8. 라이브러리/API\n53\n\n\n9. 환경 변수\n35\n\n\n10. 병렬 처리\n18\n\n\n11. 실행 모드\n45\n\n\n12. 인프라\n44\n\n\n13. 로깅\n81\n\n\n14. 기타\n176\n\n\n합계\n1,411\n\n\n\n형식은 올바르게 나왔으나, Cat 3의 파일 스키마가 누락되었습니다.\n\n\n\n\n중요한 요구사항은 독립 섹션으로 분리해야 한다. Cat 3 설명 안에 “파일 스키마를 반드시 기술하세요”라고 묻어 놓으면 에이전트가 놓칩니다. CRITICAL 표시와 함께 별도 섹션으로 올려야 합니다.\n\n\n\n\n\n\nCat 3의 36개 중간 산출물 파일에 대해 전용 에이전트를 추가 투입했습니다.\n\n\n각 파일에 대해:\n\n\n\n\n\n\n\n속성\n예시 (InExCrossInfo.txt)\n\n\n\n\n구분자\n\\t (탭)\n\n\n인코딩\nUTF-8\n\n\n헤더 행\n없음\n\n\n행 의미\n1행 = 1개 키-값 쌍\n\n\n정렬 순서\ndict 순회 순서\n\n\n생성 함수\nget_in_ex_cross_info.py::output_default_in_ex_cross_file()\n\n\n소비 함수\nin_ex_param_info.py::InExParamInfo.__init__()\n\n\n좌표 체계\nN/A\n\n\n컬럼 정의\nkey(str) + value(str)\n\n\n\n\n\n\n\nFASTA 헤더 타입 종합 정리 (SeqInfo 11가지 + MatchSeqInfo 4가지 헤더 형식)\nTSV 엔티티 컬럼 비교표 (IN/EX/CROSS 37개 컬럼 대조)\n\n\n\n\n\n3,782줄의 METADATA_INVESTIGATION.md\n1,411개 원자적 메타데이터 항목\n36개 파일 스키마 상세\n총 8개 에이전트 사용 (7개 카테고리 + 1개 파일 스키마)\n\n\n\n\n\n\n7개 에이전트 방식(3차 시도)이 성공한 후, 32개 레포지토리를 대량 처리하기 위해 다른 접근법을 시도했습니다. 이 시도에서 발견한 실패 모드는 앞의 3가지 시도와는 질적으로 다른 문제였습니다.\n\n\nPhase 1: 5개 백그라운드 에이전트가 파일 그룹별로 전체 코드를 읽음\n         (core, enum/dto, entity, common modules, pipeline steps)\n         ↓\nPhase 2: 각 에이전트가 읽은 내용의 \"요약\"을 반환\n         ↓\nPhase 3: 메인 에이전트가 요약을 기반으로 METADATA_INVESTIGATION.md 합성\n이 접근법은 합리적으로 보였습니다. 152개 파일을 5개로 나눠 병렬 읽기, 요약, 합성. 그러나 결과는 참담했습니다.\n\n\n\n\n\n\n지표\n7개 추출 에이전트 (3차)\n5개 읽기 에이전트 + 합성\n\n\n\n\n총 항목 수\n1,411\n436 (69% 감소)\n\n\nCat 3 항목 수\n218\n16 (93% 감소)\n\n\nCat 3 파일 스키마\n36개 상세 기술\n0\n\n\nDTO 필드 추출\n개별 행으로 전수\n완전 누락\n\n\n\nCat 3에 들어갔어야 할 항목들이 대량 누락되었습니다:\n\n\n\n\n\n\n\n\n누락된 항목 유형\n예상 규모\n실제 추출\n\n\n\n\nDTO 클래스 전체 필드 (SeqInfo 859줄, MatchSeqInfo 1273줄 등)\n200+ 필드\n0\n\n\nEntity Enum 필드 (25개 파일)\n300+ 필드\n0\n\n\n중간 파일 스키마 (컬럼 정의)\n수십 개\n0\n\n\n단계 간 반환값\n30+ 항목\n0\n\n\n\n\n\n\n원인 1: 2단계 간접 처리에서 정보 소실\n읽기 에이전트가 MatchSeqInfo 클래스(1,273줄)를 읽으면, 요약에는 이렇게 돌아옵니다:\nMatchSeqInfo: 매칭 서열 정보를 나타내는 포괄적 클래스.\n1273줄. 여러 헤더 형식, 정렬 상세, 어셈블리/분류 정보 포함.\n다수의 getter/setter 메서드.\n이 요약으로는 score, gene, product, pct_id, coverage 같은 개별 필드를 원자적 행으로 추출할 수 없습니다. 요약은 구조를 전달하지만, 원자적 항목을 전달하지 못합니다.\n원인 2: 컨텍스트 압박에 의한 카테고리 범위 축소\n합성 에이전트는 14개 카테고리를 모두 작성해야 합니다. 요약 기반으로 남은 컨텍스트가 빠듯하면 초기 카테고리를 빨리 끝내고 넘어가려는 경향이 생깁니다. Cat 3이 첫 번째 쓰기 배치에 포함되어 있었고, 뒤에 11개 카테고리가 남아있다는 압박에 요약 모드로 전환되었습니다.\n원인 3: 카테고리 범위를 좁게 재해석\n프롬프트는 Cat 3에 “모델 클래스의 모든 필드”, “메서드 반환값”, “메모리 상 상태 변수”를 명시적으로 요구합니다. 그러나 합성 에이전트는 “중간 산출물 = 디스크에 기록되는 파일”로 좁게 해석하여, DTO 필드/Entity Enum/반환값을 전부 누락시켰습니다.\n원인 4: 카테고리 간 중복 회피 과잉 적용\n프롬프트는 “주 카테고리에 배치하고 비고에 관련 카테고리를 표기”라고 명시합니다. 그러나 합성 에이전트는 DTO 필드를 Cat 5(파라미터)나 Cat 1(I/O)에 분산 배치해야 한다고 판단하여, Cat 3에서는 파일 경로만 나열했습니다.\n\n\n\n읽기 에이전트 (Reading Agent)\n+---------------------------------------------+\n| 입력: \"이 파일들을 읽어라\"                    |\n| 출력: 파일 내용의 요약 (구조적, 서술적)        |\n| 한계: 원자적 항목 추출을 하지 않음             |\n+---------------------------------------------+\n\n추출 에이전트 (Extraction Agent)\n+---------------------------------------------+\n| 입력: \"이 파일에서 Cat N의 메타데이터를 추출하라\" |\n| 출력: 구조화된 테이블 (행 = 원자적 항목)        |\n| 강점: 목적이 명확하므로 원자적 디테일 유지      |\n+---------------------------------------------+\n\n\n\n\n\n\n\n\n비교 항목\n읽기 + 합성\n직접 추출\n\n\n\n\n에이전트 역할\n“파일을 읽고 요약하라”\n“Cat N의 메타데이터를 추출하라”\n\n\n정보 흐름\n코드 -&gt; 요약 -&gt; 테이블 (2단계)\n코드 -&gt; 테이블 (1단계)\n\n\n원자적 디테일\n요약에서 소실\n직접 보존\n\n\nCat 3 결과\n16개 항목\n218개 항목\n\n\n실패 지점\n요약 &lt;-&gt; 합성 사이의 간극\n없음 (단일 변환)\n\n\n\n\n에이전트에게 “읽어라”와 “추출하라”는 전혀 다른 지시입니다. 메타데이터 추출에서는 에이전트가 카테고리를 인식한 상태에서 직접 원자적 항목을 추출해야 합니다. 읽기 에이전트의 요약을 합성하는 것은 “전화 게임(Chinese whispers)”과 같아서, 단계마다 정보가 소실됩니다.\n\n\n\n\n\n\n4번의 시행착오에서 도출한 원칙들을 종합합니다.\n\n\n\n\n단일 에이전트 전제의 프롬프트를 여러 에이전트에게 나눠주면 실패합니다. 어떻게 나누고, 어떻게 합칠 것인지를 프롬프트 자체에 포함해야 합니다.\n## 분산 실행 프로토콜\n\n### 분할 단위\n- Agent A: Cat 1-2 (파이프라인 구조)\n- Agent B: Cat 3 (중간 산출물) -- 단독\n- Agent C: Cat 4-5 (입력/파라미터)\n...\n\n### 에이전트별 산출물 규칙\n- 파일명: `/tmp/{repo}_metadata/cat_{NN}.md`\n- 자기 완결적 (헤더 불필요, 카테고리 내용만)\n- 마지막에 `&lt;!-- ROW_COUNT: {숫자} --&gt;` 주석 추가\n\n\n\n모든 에이전트가 152개 파일을 각자 탐색하면 토큰 낭비입니다. 파일을 미리 분류해서 에이전트 범위를 좁혀야 합니다.\n\n\n\n“N.1-N.4 구조로 작성하세요”보다 실제 완성된 카테고리 1개를 통째로 예시로 보여주는 것이 효과적입니다.\n\n\n\n카테고리 설명 안에 묻어 놓으면 에이전트가 놓칩니다.\n\n\n\n\n\n\nPython 파일 수\n권장 에이전트 수\n\n\n\n\n~30개\n1 (단일)\n\n\n30~80개\n3\n\n\n80~150개\n5~7\n\n\n150개+\n7~10\n\n\n\n\n\n\n병합 단계에서 통계표 누락, 행 수 불일치가 발생합니다. 오케스트레이터가 따를 체크리스트가 필요합니다.\n\n\n\n\n\n\n“이 파일들을 읽어라”가 아니라 “이 파일에서 Cat N의 메타데이터를 추출하라”로 지시해야 원자적 디테일이 보존됩니다.\n\n\n\nCat 3 (중간 산출물)은 DTO 필드 추출 + 파일 스키마 분석이 합쳐져 가장 분량이 큽니다 (218개 항목 + 637줄 파일 스키마). 다른 카테고리와 묶으면 context window를 초과합니다.\n\n\n\nfor repo in repos:\n    if repo.file_count &lt; 30:\n        single_extraction_agent(repo)\n    else:\n        parallel_extraction_agents(repo, categories)\n\n\n\n\n프롬프트를 아무리 정밀하게 만들어도 해결할 수 없는 문제가 있습니다.\n\n\n같은 프롬프트 + 같은 코드로 두 번 실행하면:\n\n\n\n\n\n\n\n문제\n예시\n\n\n\n\n행 수 불일치\n이번 1,411개, 다음번 1,380개\n\n\n행 순서 변동\nuclust_pct_id가 3번이었다가 7번\n\n\n논리명 흔들림\n“UCLUST Percent Identity” vs “UCLUST Identity Threshold”\n\n\n누락/추가\nprivate 메서드 포함 여부가 달라짐\n\n\n카테고리 분류 흔들림\n같은 항목이 Cat 5에 갔다가 Cat 7에 감\n\n\n\n\n\n\n코드 설명 Agent가 메타데이터를 참조할 때, 실행마다 메타데이터가 달라지면 Agent의 응답도 불안정해집니다. 주기적으로 메타데이터를 갱신해야 하는 운영 환경에서는 치명적입니다.\n\n\n\n\n\n\n접근법\n재현성\n\n\n\n\n정밀 프롬프트만\n~70%\n\n\n프롬프트 + 참조 예시\n~80%\n\n\n프롬프트로는 한계\n100%는 불가능\n\n\n\n\n\n\n\n\n\n\n\nTSV/FASTA 파일의 스키마(구분자, 컬럼 순서, 생산/소비 함수)는 코드의 실제 write 로직 (f-string 안의 컬럼 순서, 조건부 컬럼 등)을 분석해야 합니다. 단순 추출로는 불가능하며, 전용 에이전트가 코드 흐름을 따라가야 합니다.\n\n\n\n\n통계표의 행 수 (에이전트가 *로 남겨두는 경우)\n파이프라인 구조 다이어그램\n전체 파일 목록 (상단)\n품질 검증 체크리스트 (하단)\n\n이들은 오케스트레이터(리더 에이전트 또는 스크립트)가 자동 생성해야 합니다.\n\n\n\nbypassPermissions 모드로 에이전트를 실행해도 /tmp 디렉토리 생성에 실패하는 경우가 있었습니다. 산출물 디렉토리는 에이전트 실행 전에 미리 생성해 두어야 합니다.\n\n\n\n\n\n4번의 시행착오에서 배운 핵심:\n\n대규모 코드베이스는 단일 에이전트로 처리할 수 없다 – Context window의 물리적 한계\nLLM은 추상적 규칙보다 구체적 예시를 따른다 – Gold Standard 참조 문서가 필수\n중요 요구사항은 독립 섹션으로 분리해야 한다 – 프롬프트 안에 묻히면 누락됨\n“읽어라”와 “추출하라”는 근본적으로 다른 지시다 – 요약 기반 합성은 원자적 디테일을 체계적으로 소실시킨다\n순수 LLM 접근은 비결정적이다 – 프롬프트 개선만으로는 100% 재현성을 달성할 수 없다\n에이전트의 역할 설계가 결과 품질을 결정한다 – 같은 코드를 봐도 역할에 따라 전혀 다른 출력이 나온다\n\n한마디로: “무엇을 추출할 것인가”는 프롬프트의 영역이지만, “에이전트에게 무엇을 시킬 것인가”는 역할 설계의 영역이다.\n이 한계를 인식한 후 설계한 해결책 – AST 코드 파서와 LLM을 결합한 하이브리드 파이프라인 – 은 다음 글에서 다룹니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#왜-코드베이스-메타데이터가-필요한가",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#왜-코드베이스-메타데이터가-필요한가",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "인실리코(in-silico) 알고리즘 코드를 분석하는 코드 설명 Agent를 구축하려면, Agent가 코드를 이해하기 위한 구조화된 메타데이터가 필요합니다.\n여기서 말하는 메타데이터란:\n\n모든 클래스/함수/변수의 물리명, 논리명, 데이터 타입, 기본값, 의미\n모듈 간 입출력 연결 관계\n파이프라인 단계 간 의존성\n중간 산출물의 파일 스키마 (구분자, 인코딩, 컬럼 정의)\n알고리즘 파라미터, 임계값, 허용 범위\n암묵적 규칙 (코드에 명시되지 않은 관계, 상태 전이 조건)\n\n이 메타데이터를 14개 카테고리로 분류하여 METADATA_INVESTIGATION.md라는 단일 문서로 생성하는 것이 목표입니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#대상-automsa-파이프라인",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#대상-automsa-파이프라인",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "항목\n값\n\n\n\n\n레포지토리\ninsilico-core-automsa\n\n\nPython 파일 수\n152개\n\n\n총 코드 줄 수\n~29,178줄\n\n\n파이프라인 단계\n9개 서비스 (5 Step)\n\n\n역할\n유전자/단백질 키워드 기반 서열 수집, BLAST 확장, 대표 서열 선정, 다중 서열 정렬(MSA) 자동화\n\n\n\n파이프라인 구조:\nAutoMSA -&gt; DRS -&gt; monoom -&gt; multiom -&gt; multiom-output -&gt; multitom -&gt; multiplex-excel\n이 규모의 코드베이스에서 원자적 수준의 메타데이터를 추출하면 1,400개 이상의 항목이 나옵니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-단일-에이전트-실패",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-단일-에이전트-실패",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "하나의 Claude Code 에이전트에게 전체 152개 파일을 분석하라고 요청했습니다.\n\n\n\nContext window 한계에 도달. 약 50개 파일을 읽은 시점에서 컨텍스트가 부족해져, 나머지 100개 파일은 조사되지 않았습니다.\n\n\n\n\n대규모 코드베이스는 단일 에이전트로 처리할 수 없다. Context window는 유한하고, 152개 파일 x 평균 192줄 = ~29K줄의 코드를 한 번에 읽고 분석하는 것은 불가능합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-3개-분산-에이전트",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-3개-분산-에이전트",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "14개 카테고리를 3개 그룹으로 나누어 병렬 실행:\n\nAgent A: Cat 1-3 (파이프라인 구조, 중간 산출물)\nAgent B: Cat 4-7 (입력/파라미터, DB, 품질)\nAgent C: Cat 8-14 (라이브러리, 환경, 인프라, 로깅, 기타)\n\n\n\n\n3개 에이전트 모두 완료. 그러나 산출물의 형식이 프롬프트와 불일치:\n\n\n\n\n\n\n\n문제\n상세\n\n\n\n\nN.1-N.4 구조 누락\n각 카테고리에 “추출 가능 이유”, “조사 대상 파일”, “해석 메타데이터” 섹션이 없음\n\n\n테이블 헤더 영문\n한국어 8열 헤더 대신 영어 헤더 사용\n\n\n통계표 누락\n최종 카테고리별 항목 수 집계표 없음\n\n\n품질 체크리스트 누락\n12개 항목 체크리스트 없음\n\n\n\n\n\n\n\nLLM은 추상적 규칙보다 구체적 예시를 훨씬 잘 따른다. 프롬프트에 “N.1, N.2, N.3, N.4 구조로 작성하세요”라고 써도, 실제 완성된 예시가 없으면 에이전트가 자의적으로 해석합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-7개-분산-에이전트-형식-교정",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-7개-분산-에이전트-형식-교정",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "참조 문서(multiom 레포의 METADATA_INVESTIGATION.md)를 Gold Standard로 제공\n7개 에이전트로 더 세밀하게 분할:\n\nCat 1 / Cat 2-3 / Cat 4 / Cat 5-6 / Cat 7-8 / Cat 9-11 / Cat 12-14\n\n각 에이전트에게 구체적 형식 예시와 참조 문서를 함께 전달\n\n\n\n\n\n\n\n카테고리\n항목 수\n\n\n\n\n1. 모듈 간 I/O\n213\n\n\n2. 파이프라인 의존성\n62\n\n\n3. 중간 산출물\n218\n\n\n4. 입력 JSON/UI\n128\n\n\n5. 알고리즘 파라미터\n129\n\n\n6. DB 스키마/쿼리\n110\n\n\n7. 품질 규칙\n99\n\n\n8. 라이브러리/API\n53\n\n\n9. 환경 변수\n35\n\n\n10. 병렬 처리\n18\n\n\n11. 실행 모드\n45\n\n\n12. 인프라\n44\n\n\n13. 로깅\n81\n\n\n14. 기타\n176\n\n\n합계\n1,411\n\n\n\n형식은 올바르게 나왔으나, Cat 3의 파일 스키마가 누락되었습니다.\n\n\n\n\n중요한 요구사항은 독립 섹션으로 분리해야 한다. Cat 3 설명 안에 “파일 스키마를 반드시 기술하세요”라고 묻어 놓으면 에이전트가 놓칩니다. CRITICAL 표시와 함께 별도 섹션으로 올려야 합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#cat-3-파일-스키마-보강-전용-에이전트",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#cat-3-파일-스키마-보강-전용-에이전트",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "Cat 3의 36개 중간 산출물 파일에 대해 전용 에이전트를 추가 투입했습니다.\n\n\n각 파일에 대해:\n\n\n\n\n\n\n\n속성\n예시 (InExCrossInfo.txt)\n\n\n\n\n구분자\n\\t (탭)\n\n\n인코딩\nUTF-8\n\n\n헤더 행\n없음\n\n\n행 의미\n1행 = 1개 키-값 쌍\n\n\n정렬 순서\ndict 순회 순서\n\n\n생성 함수\nget_in_ex_cross_info.py::output_default_in_ex_cross_file()\n\n\n소비 함수\nin_ex_param_info.py::InExParamInfo.__init__()\n\n\n좌표 체계\nN/A\n\n\n컬럼 정의\nkey(str) + value(str)\n\n\n\n\n\n\n\nFASTA 헤더 타입 종합 정리 (SeqInfo 11가지 + MatchSeqInfo 4가지 헤더 형식)\nTSV 엔티티 컬럼 비교표 (IN/EX/CROSS 37개 컬럼 대조)\n\n\n\n\n\n3,782줄의 METADATA_INVESTIGATION.md\n1,411개 원자적 메타데이터 항목\n36개 파일 스키마 상세\n총 8개 에이전트 사용 (7개 카테고리 + 1개 파일 스키마)"
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-읽기요약합성-방식의-실패",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#차-시도-읽기요약합성-방식의-실패",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "7개 에이전트 방식(3차 시도)이 성공한 후, 32개 레포지토리를 대량 처리하기 위해 다른 접근법을 시도했습니다. 이 시도에서 발견한 실패 모드는 앞의 3가지 시도와는 질적으로 다른 문제였습니다.\n\n\nPhase 1: 5개 백그라운드 에이전트가 파일 그룹별로 전체 코드를 읽음\n         (core, enum/dto, entity, common modules, pipeline steps)\n         ↓\nPhase 2: 각 에이전트가 읽은 내용의 \"요약\"을 반환\n         ↓\nPhase 3: 메인 에이전트가 요약을 기반으로 METADATA_INVESTIGATION.md 합성\n이 접근법은 합리적으로 보였습니다. 152개 파일을 5개로 나눠 병렬 읽기, 요약, 합성. 그러나 결과는 참담했습니다.\n\n\n\n\n\n\n지표\n7개 추출 에이전트 (3차)\n5개 읽기 에이전트 + 합성\n\n\n\n\n총 항목 수\n1,411\n436 (69% 감소)\n\n\nCat 3 항목 수\n218\n16 (93% 감소)\n\n\nCat 3 파일 스키마\n36개 상세 기술\n0\n\n\nDTO 필드 추출\n개별 행으로 전수\n완전 누락\n\n\n\nCat 3에 들어갔어야 할 항목들이 대량 누락되었습니다:\n\n\n\n\n\n\n\n\n누락된 항목 유형\n예상 규모\n실제 추출\n\n\n\n\nDTO 클래스 전체 필드 (SeqInfo 859줄, MatchSeqInfo 1273줄 등)\n200+ 필드\n0\n\n\nEntity Enum 필드 (25개 파일)\n300+ 필드\n0\n\n\n중간 파일 스키마 (컬럼 정의)\n수십 개\n0\n\n\n단계 간 반환값\n30+ 항목\n0\n\n\n\n\n\n\n원인 1: 2단계 간접 처리에서 정보 소실\n읽기 에이전트가 MatchSeqInfo 클래스(1,273줄)를 읽으면, 요약에는 이렇게 돌아옵니다:\nMatchSeqInfo: 매칭 서열 정보를 나타내는 포괄적 클래스.\n1273줄. 여러 헤더 형식, 정렬 상세, 어셈블리/분류 정보 포함.\n다수의 getter/setter 메서드.\n이 요약으로는 score, gene, product, pct_id, coverage 같은 개별 필드를 원자적 행으로 추출할 수 없습니다. 요약은 구조를 전달하지만, 원자적 항목을 전달하지 못합니다.\n원인 2: 컨텍스트 압박에 의한 카테고리 범위 축소\n합성 에이전트는 14개 카테고리를 모두 작성해야 합니다. 요약 기반으로 남은 컨텍스트가 빠듯하면 초기 카테고리를 빨리 끝내고 넘어가려는 경향이 생깁니다. Cat 3이 첫 번째 쓰기 배치에 포함되어 있었고, 뒤에 11개 카테고리가 남아있다는 압박에 요약 모드로 전환되었습니다.\n원인 3: 카테고리 범위를 좁게 재해석\n프롬프트는 Cat 3에 “모델 클래스의 모든 필드”, “메서드 반환값”, “메모리 상 상태 변수”를 명시적으로 요구합니다. 그러나 합성 에이전트는 “중간 산출물 = 디스크에 기록되는 파일”로 좁게 해석하여, DTO 필드/Entity Enum/반환값을 전부 누락시켰습니다.\n원인 4: 카테고리 간 중복 회피 과잉 적용\n프롬프트는 “주 카테고리에 배치하고 비고에 관련 카테고리를 표기”라고 명시합니다. 그러나 합성 에이전트는 DTO 필드를 Cat 5(파라미터)나 Cat 1(I/O)에 분산 배치해야 한다고 판단하여, Cat 3에서는 파일 경로만 나열했습니다.\n\n\n\n읽기 에이전트 (Reading Agent)\n+---------------------------------------------+\n| 입력: \"이 파일들을 읽어라\"                    |\n| 출력: 파일 내용의 요약 (구조적, 서술적)        |\n| 한계: 원자적 항목 추출을 하지 않음             |\n+---------------------------------------------+\n\n추출 에이전트 (Extraction Agent)\n+---------------------------------------------+\n| 입력: \"이 파일에서 Cat N의 메타데이터를 추출하라\" |\n| 출력: 구조화된 테이블 (행 = 원자적 항목)        |\n| 강점: 목적이 명확하므로 원자적 디테일 유지      |\n+---------------------------------------------+\n\n\n\n\n\n\n\n\n비교 항목\n읽기 + 합성\n직접 추출\n\n\n\n\n에이전트 역할\n“파일을 읽고 요약하라”\n“Cat N의 메타데이터를 추출하라”\n\n\n정보 흐름\n코드 -&gt; 요약 -&gt; 테이블 (2단계)\n코드 -&gt; 테이블 (1단계)\n\n\n원자적 디테일\n요약에서 소실\n직접 보존\n\n\nCat 3 결과\n16개 항목\n218개 항목\n\n\n실패 지점\n요약 &lt;-&gt; 합성 사이의 간극\n없음 (단일 변환)\n\n\n\n\n에이전트에게 “읽어라”와 “추출하라”는 전혀 다른 지시입니다. 메타데이터 추출에서는 에이전트가 카테고리를 인식한 상태에서 직접 원자적 항목을 추출해야 합니다. 읽기 에이전트의 요약을 합성하는 것은 “전화 게임(Chinese whispers)”과 같아서, 단계마다 정보가 소실됩니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#설계-원칙-정리",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#설계-원칙-정리",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "4번의 시행착오에서 도출한 원칙들을 종합합니다.\n\n\n\n\n단일 에이전트 전제의 프롬프트를 여러 에이전트에게 나눠주면 실패합니다. 어떻게 나누고, 어떻게 합칠 것인지를 프롬프트 자체에 포함해야 합니다.\n## 분산 실행 프로토콜\n\n### 분할 단위\n- Agent A: Cat 1-2 (파이프라인 구조)\n- Agent B: Cat 3 (중간 산출물) -- 단독\n- Agent C: Cat 4-5 (입력/파라미터)\n...\n\n### 에이전트별 산출물 규칙\n- 파일명: `/tmp/{repo}_metadata/cat_{NN}.md`\n- 자기 완결적 (헤더 불필요, 카테고리 내용만)\n- 마지막에 `&lt;!-- ROW_COUNT: {숫자} --&gt;` 주석 추가\n\n\n\n모든 에이전트가 152개 파일을 각자 탐색하면 토큰 낭비입니다. 파일을 미리 분류해서 에이전트 범위를 좁혀야 합니다.\n\n\n\n“N.1-N.4 구조로 작성하세요”보다 실제 완성된 카테고리 1개를 통째로 예시로 보여주는 것이 효과적입니다.\n\n\n\n카테고리 설명 안에 묻어 놓으면 에이전트가 놓칩니다.\n\n\n\n\n\n\nPython 파일 수\n권장 에이전트 수\n\n\n\n\n~30개\n1 (단일)\n\n\n30~80개\n3\n\n\n80~150개\n5~7\n\n\n150개+\n7~10\n\n\n\n\n\n\n병합 단계에서 통계표 누락, 행 수 불일치가 발생합니다. 오케스트레이터가 따를 체크리스트가 필요합니다.\n\n\n\n\n\n\n“이 파일들을 읽어라”가 아니라 “이 파일에서 Cat N의 메타데이터를 추출하라”로 지시해야 원자적 디테일이 보존됩니다.\n\n\n\nCat 3 (중간 산출물)은 DTO 필드 추출 + 파일 스키마 분석이 합쳐져 가장 분량이 큽니다 (218개 항목 + 637줄 파일 스키마). 다른 카테고리와 묶으면 context window를 초과합니다.\n\n\n\nfor repo in repos:\n    if repo.file_count &lt; 30:\n        single_extraction_agent(repo)\n    else:\n        parallel_extraction_agents(repo, categories)\n\n\n\n\n프롬프트를 아무리 정밀하게 만들어도 해결할 수 없는 문제가 있습니다.\n\n\n같은 프롬프트 + 같은 코드로 두 번 실행하면:\n\n\n\n\n\n\n\n문제\n예시\n\n\n\n\n행 수 불일치\n이번 1,411개, 다음번 1,380개\n\n\n행 순서 변동\nuclust_pct_id가 3번이었다가 7번\n\n\n논리명 흔들림\n“UCLUST Percent Identity” vs “UCLUST Identity Threshold”\n\n\n누락/추가\nprivate 메서드 포함 여부가 달라짐\n\n\n카테고리 분류 흔들림\n같은 항목이 Cat 5에 갔다가 Cat 7에 감\n\n\n\n\n\n\n코드 설명 Agent가 메타데이터를 참조할 때, 실행마다 메타데이터가 달라지면 Agent의 응답도 불안정해집니다. 주기적으로 메타데이터를 갱신해야 하는 운영 환경에서는 치명적입니다.\n\n\n\n\n\n\n접근법\n재현성\n\n\n\n\n정밀 프롬프트만\n~70%\n\n\n프롬프트 + 참조 예시\n~80%\n\n\n프롬프트로는 한계\n100%는 불가능"
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#실전에서-배운-세부-교훈들",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#실전에서-배운-세부-교훈들",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "TSV/FASTA 파일의 스키마(구분자, 컬럼 순서, 생산/소비 함수)는 코드의 실제 write 로직 (f-string 안의 컬럼 순서, 조건부 컬럼 등)을 분석해야 합니다. 단순 추출로는 불가능하며, 전용 에이전트가 코드 흐름을 따라가야 합니다.\n\n\n\n\n통계표의 행 수 (에이전트가 *로 남겨두는 경우)\n파이프라인 구조 다이어그램\n전체 파일 목록 (상단)\n품질 검증 체크리스트 (하단)\n\n이들은 오케스트레이터(리더 에이전트 또는 스크립트)가 자동 생성해야 합니다.\n\n\n\nbypassPermissions 모드로 에이전트를 실행해도 /tmp 디렉토리 생성에 실패하는 경우가 있었습니다. 산출물 디렉토리는 에이전트 실행 전에 미리 생성해 두어야 합니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#결론",
    "href": "docs/blog/posts/Agent/01-코드분석/01-메타데이터_시행착오와_설계_원칙.html#결론",
    "title": "LLM 에이전트로 코드 메타데이터 추출하기: 시행착오와 설계 원칙",
    "section": "",
    "text": "4번의 시행착오에서 배운 핵심:\n\n대규모 코드베이스는 단일 에이전트로 처리할 수 없다 – Context window의 물리적 한계\nLLM은 추상적 규칙보다 구체적 예시를 따른다 – Gold Standard 참조 문서가 필수\n중요 요구사항은 독립 섹션으로 분리해야 한다 – 프롬프트 안에 묻히면 누락됨\n“읽어라”와 “추출하라”는 근본적으로 다른 지시다 – 요약 기반 합성은 원자적 디테일을 체계적으로 소실시킨다\n순수 LLM 접근은 비결정적이다 – 프롬프트 개선만으로는 100% 재현성을 달성할 수 없다\n에이전트의 역할 설계가 결과 품질을 결정한다 – 같은 코드를 봐도 역할에 따라 전혀 다른 출력이 나온다\n\n한마디로: “무엇을 추출할 것인가”는 프롬프트의 영역이지만, “에이전트에게 무엇을 시킬 것인가”는 역할 설계의 영역이다.\n이 한계를 인식한 후 설계한 해결책 – AST 코드 파서와 LLM을 결합한 하이브리드 파이프라인 – 은 다음 글에서 다룹니다."
  },
  {
    "objectID": "docs/blog/posts/Agent/index.html",
    "href": "docs/blog/posts/Agent/index.html",
    "title": "Agent",
    "section": "",
    "text": "메타데이터 추출 시행착오와 설계 원칙\n하이브리드 파이프라인 설계: AST + LLM 자동화"
  },
  {
    "objectID": "docs/blog/posts/Agent/index.html#agent",
    "href": "docs/blog/posts/Agent/index.html#agent",
    "title": "Agent",
    "section": "",
    "text": "메타데이터 추출 시행착오와 설계 원칙\n하이브리드 파이프라인 설계: AST + LLM 자동화"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html",
    "href": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html",
    "title": "OpenAI 외 도구 호출 에이전트(Tool Calling Agent)",
    "section": "",
    "text": "OpenAI 외에도 Anthropic, Google Gemini, Together.ai, Ollama, Mistral과 같은 더 광범위한 공급자 구현을 지원합니다.\n이번 챕터에서는 다양한 LLM 을 사용하여 도구 호출 에이전트를 생성하고 실행하는 방법을 살펴보겠습니다.\n참고 링크\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH15-Agents\")\n코드\nfrom langchain.tools import tool\nfrom typing import List, Dict\nfrom langchain_teddynote.tools import GoogleNews\n\n\n# 도구 정의\n@tool\ndef search_news(query: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Search Google News by input keyword\"\"\"\n    news_tool = GoogleNews()\n    return news_tool.search_by_keyword(query, k=5)\n\n\nprint(f\"도구 이름: {search_news.name}\")\nprint(f\"도구 설명: {search_news.description}\")\n코드\n# tools 정의\ntools = [search_news]"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#agent-용-프롬프트-생성",
    "href": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#agent-용-프롬프트-생성",
    "title": "OpenAI 외 도구 호출 에이전트(Tool Calling Agent)",
    "section": "1 Agent 용 프롬프트 생성",
    "text": "1 Agent 용 프롬프트 생성\n\nchat_history : 이전 대화 내용을 저장하는 변수 (멀티턴을 지원하지 않는다면, 생략 가능합니다.)\nagent_scratchpad : 에이전트가 임시로 저장하는 변수\ninput : 사용자의 입력\n\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.agents import create_tool_calling_agent\n\n# 프롬프트 생성\n# 프롬프트는 에이전트에게 모델이 수행할 작업을 설명하는 텍스트를 제공합니다. (도구의 이름과 역할을 입력)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant. \"\n            \"Make sure to use the `search_news` tool for searching keyword related news.\",\n        ),\n        (\"placeholder\", \"{chat_history}\"),\n        (\"human\", \"{input}\"),\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#tool-calling-을-지원하는-다양한-llm-목록",
    "href": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#tool-calling-을-지원하는-다양한-llm-목록",
    "title": "OpenAI 외 도구 호출 에이전트(Tool Calling Agent)",
    "section": "2 Tool Calling 을 지원하는 다양한 LLM 목록",
    "text": "2 Tool Calling 을 지원하는 다양한 LLM 목록\n실습 진행을 위해서는 아래 내용을 설정해야 합니다.\nAnthropic\n\nAnthropic API 키 발급 관련\n.env 파일 내 ANTHROPIC_API_KEY 에 발급받은 키를 설정하세요\n\nGemini\n\nGemini API 키 발급 관련\n.env 파일 내 GOOGLE_API_KEY 에 발급받은 키를 설정하세요\n\nTogether AI\n\nTogether AI API 키 발급 관련\n.env 파일 내 TOGETHER_API_KEY 에 발급받은 키를 설정하세요\n\nOllama\n\nOllama Tool Calling 지원 모델 리스트\n이번 실습에 사용할 llama3.1 모델\n터미널 창에 ollama pull llama3.1 명령어를 입력하여 모델을 다운로드 받습니다.\n이전에 Ollama 를 사용하지 않았다면, Ollama 를 참고해 주세요.\n\nlangchain-ollama 설치를 한 뒤 진행해 주세요.\n\n\n코드\n# !pip install -qU langchain-ollama==0.1.3\n\n\n\n\n코드\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_openai import ChatOpenAI\nfrom langchain_ollama import ChatOllama\nimport os\n\n# GPT-4o-mini\ngpt = ChatOpenAI(model=\"gpt-4o-mini\")\n\n# Claude-3-5-sonnet\nclaude = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n\n# Gemini-1.5-pro-latest\ngemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n\n# Llama-3.1-70B-Instruct-Turbo\nllama = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n)\n\n# Llama-3.1\nollama = ChatOllama(model=\"llama3.1\", temperature=0)\n\n# Qwen2.5 7B (한글 성능 괜찮은 편)\nqwen = ChatOllama(\n    model=\"qwen2.5:latest\",\n)\n\n\nLLM 기반으로 Agent 를 생성합니다.\n\n\n코드\nfrom langchain.agents import create_tool_calling_agent\n\n# Agent 생성\ngpt_agent = create_tool_calling_agent(gpt, tools, prompt)\nclaude_agent = create_tool_calling_agent(claude, tools, prompt)\ngemini_agent = create_tool_calling_agent(gemini, tools, prompt)\nllama_agent = create_tool_calling_agent(llama, tools, prompt)\nollama_agent = create_tool_calling_agent(ollama, tools, prompt)\nqwen_agent = create_tool_calling_agent(qwen, tools, prompt)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#agentexecutor-생성-후-실행-및-결과-확인",
    "href": "docs/blog/posts/RAG/15-Agent/04-Agent-More-LLMs.html#agentexecutor-생성-후-실행-및-결과-확인",
    "title": "OpenAI 외 도구 호출 에이전트(Tool Calling Agent)",
    "section": "3 AgentExecutor 생성 후 실행 및 결과 확인",
    "text": "3 AgentExecutor 생성 후 실행 및 결과 확인\n\n\n코드\nfrom langchain.agents import AgentExecutor\n\n# gpt_agent 실행\nagent_executor = AgentExecutor(\n    agent=gpt_agent,\n    tools=tools,\n    verbose=True,\n    handle_parsing_errors=True,\n)\n\nresult = agent_executor.invoke({\"input\": \"AI 투자와 관련된 뉴스를 검색해 주세요.\"})\n\nprint(\"Agent 실행 결과:\")\nprint(result[\"output\"])\n\n\n다양한 llm을 사용하여 에이전트를 실행합니다.\n다음은 입력받은 llm을 사용하여 Agent 를 생성하고 실행하여 결과를 출력하는 함수입니다.\n\n\n코드\ndef execute_agent(llm, tools, input_text, label):\n    agent = create_tool_calling_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n    result = executor.invoke({\"input\": input_text})\n    print(f\"[{label}] 결과입니다.\")\n    if isinstance(result[\"output\"], list) and len(result[\"output\"]) &gt; 0:\n        for item in result[\"output\"]:\n            if \"text\" in item:\n                print(item[\"text\"])\n    elif isinstance(result[\"output\"], str):\n        print(result[\"output\"])\n    else:\n        print(result[\"output\"])\n\n\n각 llm 별로 에이전트를 생성하고 실행하여 결과를 출력합니다.\n\n\n코드\nquery = (\n    \"AI 투자와 관련된 뉴스를 검색하고, 결과를 Instagram 게시글 형식으로 작성해 주세요.\"\n)\n\n\n\n\n코드\n# gpt\nexecute_agent(gpt, tools, query, \"gpt\")\n\n\n\n\n코드\n# claude\nexecute_agent(claude, tools, query, \"claude\")\n\n\n\n\n코드\n# gemini\nexecute_agent(gemini, tools, query, \"gemini\")\n\n\n\n\n코드\n# llama3.1 70B (Together.ai)\nexecute_agent(\n    llama,\n    tools,\n    \"Search AI related news and write it in Instagram post format\",\n    \"llama3.1 70B\",\n)\n\n\n\n\n코드\n# llama3.1 8B (ollama)\nexecute_agent(ollama, tools, query, \"llama3.1(Ollama)\")\n\n\n\n\n코드\n# qwen2.5 7B (ollama)\nquery = \"AI 투자와 관련된 뉴스를 검색하고, 결과를 Instagram 게시글 형식으로 작성해 주세요. 한글로 답변하세요!\"\n\nexecute_agent(qwen, tools, query, \"qwen2.5(Ollama)\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/05-Iter-Human-In-the-Loop.html",
    "href": "docs/blog/posts/RAG/15-Agent/05-Iter-Human-In-the-Loop.html",
    "title": "Iteration 기능과 사람 개입(Human-in-the-loop)",
    "section": "",
    "text": "iter() 메서드는 에이전트의 실행 과정을 단계별로 반복할 수 있게 해주는 반복자(iterator)를 생성합니다.\n중간 과정에서 사용자의 입력을 받아 계속 진행할지 묻는 기능을 제공합니다. 이를 Human-in-the-loop 라고 합니다.\n\n\n코드\n# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH15-Agents\")\n\n\n먼저, 도구(tool) 를 정의합니다.\n\n\n코드\nfrom langchain.agents import tool\n\n\n@tool\ndef add_function(a: float, b: float) -&gt; float:\n    \"\"\"Adds two numbers together.\"\"\"\n    return a + b\n\n\n다음으로는 add_function 을 사용하여 덧셈 계산을 수행하는 Agent 를 정의합니다.\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\n\n# 도구 정의\ntools = [add_function]\n\n# LLM 생성\ngpt = ChatOpenAI(model=\"gpt-4o-mini\")\n\n# prompt 생성\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant.\",\n        ),\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n\n# Agent 생성\ngpt_agent = create_tool_calling_agent(gpt, tools, prompt)\n\n# AgentExecutor 생성\nagent_executor = AgentExecutor(\n    agent=gpt_agent,\n    tools=tools,\n    verbose=False,\n    max_iterations=10,\n    handle_parsing_errors=True,\n)\n\n\n\n0.1 AgentExecutor의 iter()\n이 메서드는 AgentExecutor의 실행 과정을 단계별로 반복할 수 있게 해주는 반복자(iterator)를 생성합니다.\n함수 설명 iter() 는 에이전트가 최종 출력에 도달하기까지 거치는 단계들을 순차적으로 접근할 수 있는 AgentExecutorIterator 객체를 반환합니다.\n주요 기능 - 단계별 실행 접근: 에이전트의 실행 과정을 단계별로 살펴볼 수 있습니다.\n흐름 정리\n\"114.5 + 121.2 + 34.2 + 110.1\" 의 덧셈 계산을 수행하기 위해서는 단계별로 계산이 수행되게 됩니다.\n\n114.5 + 121.2 = 235.7\n235.7 + 34.2 = 270.9\n270.9 + 110.1 = 381.0\n\n이러한 계산 과정을 단계별로 살펴볼 수 있습니다.\n이때,\n단계별로 계산 결과를 사용자에게 보여주고, 사용자가 계속 진행할지 묻습니다. (Human-in-the-loop)\n사용자가 ’y’가 아닌 다른 입력을 하면 반복 중단됩니다.\n\n\n코드\n# 계산할 질문 설정\nquestion = \"114.5 + 121.2 + 34.2 + 110.1 의 계산 결과는?\"\n\n# agent_executor를 반복적으로 실행\nfor step in agent_executor.iter({\"input\": question}):\n    if output := step.get(\"intermediate_step\"):\n        action, value = output[0]\n        if action.tool == \"add_function\":\n            # Tool 실행 결과 출력\n            print(f\"\\nTool Name: {action.tool}, 실행 결과: {value}\")\n        # 사용자에게 계속 진행할지 묻습니다.\n        _continue = input(\"계속 진행하시겠습니다? (y/n)?:\\n\") or \"Y\"\n        # 사용자가 'y'가 아닌 다른 입력을 하면 반복 중단\n        if _continue.lower() != \"y\":\n            break\n\n# 최종 결과 출력\nif \"output\" in step:\n    print(step[\"output\"])"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "",
    "text": "도구 호출을 사용하면 모델이 하나 이상의 도구(tool) 가 호출되어야 하는 시기를 감지하고 해당 도구에 전달해야 하는 입력 으로 전달할 수 있습니다.\nAPI 호출에서 도구를 설명하고 모델이 이러한 도구를 호출하기 위한 인수가 포함된 JSON과 같은 구조화된 객체를 출력하도록 지능적으로 선택할 수 있습니다.\n도구 API 의 목표는 일반 텍스트 완성이나 채팅 API를 사용하여 수행할 수 있는 것보다 더 안정적으로 유효하고 유용한 도구 호출(tool call) 을 반환하는 것입니다.\n이러한 구조화된 출력을 도구 호출 채팅 모델에 여러 도구를 바인딩하고 모델이 호출할 도구를 선택할 수 있다는 사실과 결합하여 쿼리가 해결될 때까지 반복적으로 도구를 호출하고 결과를 수신하는 에이전트를 만들 수 있습니다.\n이것은 OpenAI 의 특정 도구 호출 스타일에 맞게 설계된 OpenAI 도구 에이전트의 보다 일반화된 버전 입니다.\n이 에이전트는 LangChain의 ToolCall 인터페이스를 사용하여 OpenAI 외에도 Anthropic, Google Gemini, Mistral과 같은 더 광범위한 공급자 구현을 지원합니다.\n참고 링크\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH15-Agents\")\n코드\nfrom langchain.tools import tool\nfrom typing import List, Dict, Annotated\nfrom langchain_teddynote.tools import GoogleNews\nfrom langchain_experimental.utilities import PythonREPL\n\n\n# 도구 생성\n@tool\ndef search_news(query: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Search Google News by input keyword\"\"\"\n    news_tool = GoogleNews()\n    return news_tool.search_by_keyword(query, k=5)\n\n\n# 도구 생성\n@tool\ndef python_repl_tool(\n    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n):\n    \"\"\"Use this to execute python code. If you want to see the output of a value,\n    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n    result = \"\"\n    try:\n        result = PythonREPL().run(code)\n    except BaseException as e:\n        print(f\"Failed to execute. Error: {repr(e)}\")\n    finally:\n        return result\n\n\nprint(f\"도구 이름: {search_news.name}\")\nprint(f\"도구 설명: {search_news.description}\")\nprint(f\"도구 이름: {python_repl_tool.name}\")\nprint(f\"도구 설명: {python_repl_tool.description}\")\n코드\n# tools 정의\ntools = [search_news, python_repl_tool]"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agent-프롬프트-생성",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agent-프롬프트-생성",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "1 Agent 프롬프트 생성",
    "text": "1 Agent 프롬프트 생성\n\nchat_history : 이전 대화 내용을 저장하는 변수 (멀티턴을 지원하지 않는다면, 생략 가능합니다.)\nagent_scratchpad : 에이전트가 임시로 저장하는 변수\ninput : 사용자의 입력\n\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# 프롬프트 생성\n# 프롬프트는 에이전트에게 모델이 수행할 작업을 설명하는 텍스트를 제공합니다. (도구의 이름과 역할을 입력)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant. \"\n            \"Make sure to use the `search_news` tool for searching keyword related news.\",\n        ),\n        (\"placeholder\", \"{chat_history}\"),\n        (\"human\", \"{input}\"),\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agent-생성",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agent-생성",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "2 Agent 생성",
    "text": "2 Agent 생성\n\n\n코드\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_tool_calling_agent\n\n# LLM 정의\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Agent 생성\nagent = create_tool_calling_agent(llm, tools, prompt)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agentexecutor",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html#agentexecutor",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "3 AgentExecutor",
    "text": "3 AgentExecutor\nAgentExecutor는 도구를 사용하는 에이전트를 실행하는 클래스입니다.\n주요 속성 - agent: 실행 루프의 각 단계에서 계획을 생성하고 행동을 결정하는 에이전트 - tools: 에이전트가 사용할 수 있는 유효한 도구 목록 - return_intermediate_steps: 최종 출력과 함께 에이전트의 중간 단계 경로를 반환할지 여부 - max_iterations: 실행 루프를 종료하기 전 최대 단계 수 - max_execution_time: 실행 루프에 소요될 수 있는 최대 시간 - early_stopping_method: 에이전트가 AgentFinish를 반환하지 않을 때 사용할 조기 종료 방법. (“force” or “generate”) - \"force\" 는 시간 또는 반복 제한에 도달하여 중지되었다는 문자열을 반환합니다. - \"generate\" 는 에이전트의 LLM 체인을 마지막으로 한 번 호출하여 이전 단계에 따라 최종 답변을 생성합니다. - handle_parsing_errors: 에이전트의 출력 파서에서 발생한 오류 처리 방법. (True, False, 또는 오류 처리 함수) - trim_intermediate_steps: 중간 단계를 트리밍하는 방법. (-1 trim 하지 않음, 또는 트리밍 함수)\n주요 메서드 1. invoke: 에이전트 실행 2. stream: 최종 출력에 도달하는 데 필요한 단계를 스트리밍\n주요 기능 1. 도구 검증: 에이전트와 호환되는 도구인지 확인 2. 실행 제어: 최대 반복 횟수 및 실행 시간 제한 설정 가능 3. 오류 처리: 출력 파싱 오류에 대한 다양한 처리 옵션 제공 4. 중간 단계 관리: 중간 단계 트리밍 및 반환 옵션 5. 비동기 지원: 비동기 실행 및 스트리밍 지원\n최적화 팁 - max_iterations와 max_execution_time을 적절히 설정하여 실행 시간 관리 - trim_intermediate_steps를 활용하여 메모리 사용량 최적화 - 복잡한 작업의 경우 stream 메서드를 사용하여 단계별 결과 모니터링\n\n\n코드\nfrom langchain.agents import AgentExecutor\n\n# AgentExecutor 생성\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    max_iterations=10,\n    max_execution_time=10,\n    handle_parsing_errors=True,\n)\n\n# AgentExecutor 실행\nresult = agent_executor.invoke({\"input\": \"AI 투자와 관련된 뉴스를 검색해 주세요.\"})\n\nprint(\"Agent 실행 결과:\")\nprint(result[\"output\"])"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html#stream-출력으로-단계별-결과-확인",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html#stream-출력으로-단계별-결과-확인",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "4 Stream 출력으로 단계별 결과 확인",
    "text": "4 Stream 출력으로 단계별 결과 확인\nAgentExecutor의 stream() 메소드를 사용하여 에이전트의 중간 단계를 스트리밍할 것입니다.\nstream()의 출력은 (Action, Observation) 쌍 사이에서 번갈아 나타나며, 최종적으로 에이전트가 목표를 달성했다면 답변으로 마무리됩니다.\n다음과 같은 형태로 보일 것입니다.\n\nAction 출력\nObservation 출력\nAction 출력\nObservation 출력\n\n… (목표 달성까지 계속) …\n그 다음, 최종 목표가 달성되면 에이전트는 최종 답변을 출력할 것입니다.\n이러한 출력의 내용은 다음과 같이 요약됩니다.\n\n\n\n\n\n\n\n출력\n내용\n\n\n\n\nAction\nactions: AgentAction 또는 그 하위 클래스messages: 액션 호출에 해당하는 채팅 메시지\n\n\nObservation\nsteps: 현재 액션과 그 관찰을 포함한 에이전트가 지금까지 수행한 작업의 기록messages: 함수 호출 결과(즉, 관찰)를 포함한 채팅 메시지\n\n\nFinal Answer\noutput: AgentFinishmessages: 최종 출력을 포함한 채팅 메시지\n\n\n\n\n::: {#bf7b4031 .cell execution_count=8}\n``` {.python .cell-code}\nfrom langchain.agents import AgentExecutor\n\n# AgentExecutor 생성\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=False,\n    handle_parsing_errors=True,\n)\n:::\n\n\n코드\n# 스트리밍 모드로 실행합니다\nresult = agent_executor.stream({\"input\": \"AI 투자와 관련된 뉴스를 검색해 주세요.\"})\n\nfor step in result:\n    # 중간 단계 출력\n    print(step)\n    print(\"===\" * 20)\n\n\n\n4.1 중간 단계 출력을 사용자 정의 함수로 출력\n다음의 3개 함수를 정의하고 이를 통해 중간 단계 출력을 사용자 정의합니다.\n\ntool_callback: 도구 호출 출력을 처리하는 함수\nobservation_callback: 관찰(Observation) 출력을 처리하는 함수\nresult_callback: 최종 답변 출력을 처리하는 함수\n\n\n\n코드\n# 업데이트\n# !pip install -U langchain-teddynote\n\n\n아래는 Agent 의 중간 단계 과정을 깔끔하게 출력하기 위하여 사용되는 콜백 함수입니다.\n이 콜백 함수는 Streamlit 에서 중간 단계를 출력하여 사용자에게 제공할 때 유용할 수 있습니다.\n\n\n코드\nfrom langchain_teddynote.messages import AgentStreamParser\n\nagent_stream_parser = AgentStreamParser()\n\n\n스트리밍 방식으로 Agent 의 응답 과정을 확인합니다.\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresult = agent_executor.stream(\n    {\"input\": \"matplotlib 을 사용하여 pie 차트를 그리는 코드를 작성하고 실행하세요.\"}\n)\n\nfor step in result:\n    # 중간 단계를 parser 를 사용하여 단계별로 출력\n    # print(step)\n    agent_stream_parser.process_agent_steps(step)\n\n\n다음은 callback 을 수정하여 사용하는 방법입니다.\n\n\n코드\n# AgentCallbacks와 AgentStreamParser를 langchain_teddynote.messages에서 가져옵니다.\nfrom langchain_teddynote.messages import AgentCallbacks, AgentStreamParser\n\n\n# 도구 호출 시 실행되는 콜백 함수입니다.\ndef tool_callback(tool) -&gt; None:\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 도구 호출 &gt;&gt;&gt;&gt;&gt;&gt;\")\n    print(f\"Tool: {tool.get('tool')}\")  # 사용된 도구의 이름을 출력합니다.\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 도구 호출 &gt;&gt;&gt;&gt;&gt;&gt;\")\n\n\n# 관찰 결과를 출력하는 콜백 함수입니다.\ndef observation_callback(observation) -&gt; None:\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 관찰 내용 &gt;&gt;&gt;&gt;&gt;&gt;\")\n    print(\n        f\"Observation: {observation.get('observation')[0]}\"\n    )  # 관찰 내용을 출력합니다.\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 관찰 내용 &gt;&gt;&gt;&gt;&gt;&gt;\")\n\n\n# 최종 결과를 출력하는 콜백 함수입니다.\ndef result_callback(result: str) -&gt; None:\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 최종 답변 &gt;&gt;&gt;&gt;&gt;&gt;\")\n    print(result)  # 최종 답변을 출력합니다.\n    print(\"&lt;&lt;&lt;&lt;&lt;&lt;&lt; 최종 답변 &gt;&gt;&gt;&gt;&gt;&gt;\")\n\n\n# AgentCallbacks 객체를 생성하여 각 단계별 콜백 함수를 설정합니다.\nagent_callbacks = AgentCallbacks(\n    tool_callback=tool_callback,\n    observation_callback=observation_callback,\n    result_callback=result_callback,\n)\n\n# AgentStreamParser 객체를 생성하여 에이전트의 실행 과정을 파싱합니다.\nagent_stream_parser = AgentStreamParser(agent_callbacks)\n\n\n아래의 출력 내용을 확인해 보면 중간 내용의 출력 값이 내가 변경한 콜백 함수의 출력 값으로 변경된 것을 확인할 수 있습니다.\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresult = agent_executor.stream({\"input\": \"AI 투자관련 뉴스를 검색해 주세요.\"})\n\nfor step in result:\n    # 중간 단계를 parser 를 사용하여 단계별로 출력\n    agent_stream_parser.process_agent_steps(step)"
  },
  {
    "objectID": "docs/blog/posts/RAG/15-Agent/03-Agent.html#이전-대화내용-기억하는-agent",
    "href": "docs/blog/posts/RAG/15-Agent/03-Agent.html#이전-대화내용-기억하는-agent",
    "title": "도구 호출 에이전트(Tool Calling Agent)",
    "section": "5 이전 대화내용 기억하는 Agent",
    "text": "5 이전 대화내용 기억하는 Agent\n이전의 대화내용을 기억하기 위해서는 RunnableWithMessageHistory 를 사용하여 AgentExecutor 를 감싸줍니다.\nRunnableWithMessageHistory 에 대한 자세한 내용은 아래 링크를 참고해 주세요.\n참고 - RunnableWithMessageHistory\n\n\n코드\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\n# session_id 를 저장할 딕셔너리 생성\nstore = {}\n\n\n# session_id 를 기반으로 세션 기록을 가져오는 함수\ndef get_session_history(session_ids):\n    if session_ids not in store:  # session_id 가 store에 없는 경우\n        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n        store[session_ids] = ChatMessageHistory()\n    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n\n\n# 채팅 메시지 기록이 추가된 에이전트를 생성합니다.\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    # 대화 session_id\n    get_session_history,\n    # 프롬프트의 질문이 입력되는 key: \"input\"\n    input_messages_key=\"input\",\n    # 프롬프트의 메시지가 입력되는 key: \"chat_history\"\n    history_messages_key=\"chat_history\",\n)\n\n\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresponse = agent_with_chat_history.stream(\n    {\"input\": \"안녕? 내 이름은 테디야!\"},\n    # session_id 설정\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\n# 출력 확인\nfor step in response:\n    agent_stream_parser.process_agent_steps(step)\n\n\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresponse = agent_with_chat_history.stream(\n    {\"input\": \"내 이름이 뭐라고?\"},\n    # session_id 설정\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\n# 출력 확인\nfor step in response:\n    agent_stream_parser.process_agent_steps(step)\n\n\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresponse = agent_with_chat_history.stream(\n    {\n        \"input\": \"내 이메일 주소는 teddy@teddynote.com 이야. 회사 이름은 테디노트 주식회사야.\"\n    },\n    # session_id 설정\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\n# 출력 확인\nfor step in response:\n    agent_stream_parser.process_agent_steps(step)\n\n\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresponse = agent_with_chat_history.stream(\n    {\n        \"input\": \"최신 뉴스 5개를 검색해서 이메일의 본문으로 작성해줘. \"\n        \"수신인에는 `셜리 상무님` 그리고, 발신인에는 내 인적정보를 적어줘.\"\n        \"정중한 어조로 작성하고, 메일의 시작과 끝에는 적절한 인사말과 맺음말을 적어줘.\"\n    },\n    # session_id 설정\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\n# 출력 확인\nfor step in response:\n    agent_stream_parser.process_agent_steps(step)\n\n\n\n\n코드\n# 질의에 대한 답변을 스트리밍으로 출력 요청\nresponse = agent_with_chat_history.stream(\n    {\n        \"input\": \"내 이름이 뭐야?\"\n    },\n    # session_id 설정\n    config={\"configurable\": {\"session_id\": \"def456\"}},\n)\n\n# 출력 확인\nfor step in response:\n    agent_stream_parser.process_agent_steps(step)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "",
    "text": "RAG 시스템에서 리트리버가 과도한 수의 문서를 반환하면 할루시네이션 가능성이 증가한다. 예를 들어, 실제 질의와 관련된 부분이 5곳(k=5)에만 있는데 k=20으로 설정하면, LLM이 15곳의 무관한 내용까지 검토하게 되어 마치 함정을 설치하는 것과 같다. 이상적으로는 LLM에게 정확히 5곳만 제공해야 하지만, 질의를 미리 알 수 없으므로 이를 구현하기는 어렵다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#문제-상황-무관한-문서의-과다-검색",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#문제-상황-무관한-문서의-과다-검색",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "",
    "text": "RAG 시스템에서 리트리버가 과도한 수의 문서를 반환하면 할루시네이션 가능성이 증가한다. 예를 들어, 실제 질의와 관련된 부분이 5곳(k=5)에만 있는데 k=20으로 설정하면, LLM이 15곳의 무관한 내용까지 검토하게 되어 마치 함정을 설치하는 것과 같다. 이상적으로는 LLM에게 정확히 5곳만 제공해야 하지만, 질의를 미리 알 수 없으므로 이를 구현하기는 어렵다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#해결책-문맥-압축-contextual-compression",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#해결책-문맥-압축-contextual-compression",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "2 해결책: 문맥 압축 (Contextual Compression)",
    "text": "2 해결책: 문맥 압축 (Contextual Compression)\nContextualCompressionRetriever는 이 문제를 해결하기 위해 설계되었다. 다량의 문서(k=10~20)를 검색한 후 문맥 압축을 통해 관련성 높은 문서만 선별(k=5)하여 반환한다.\n압축 방식의 종류:\n\nLLM 기반 압축: LLM을 활용하여 관련 내용만 추출\n\n장점: 할루시네이션 감소, 의미 기반 판단\n단점: 비용 증가, 응답 시간 증가\n\nEmbedding 기반 압축: 유사도 임계값으로 문서 필터링\n\n장점: 저비용, 빠른 처리, 중복 제거 가능\n단점: 의미적 이해 제한"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#작동-원리",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#작동-원리",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "3 작동 원리",
    "text": "3 작동 원리\nContextualCompressionRetriever는 두 단계로 작동한다:\n\n검색 단계: 질의를 base retriever에 전달하여 초기 문서 검색 (비효율적이지만 폭넓은 범위)\n압축 단계: Document Compressor를 통해 관련성 낮은 문서 제거 또는 내용 축약\n\n“압축”의 의미: - 개별 문서의 내용을 축약 (관련 내용만 추출) - 무관한 문서를 완전히 필터링 제거\n이를 통해 LLM에 전달되는 토큰 수를 줄이면서도 관련성을 유지한다.\n\n출처: https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#환경-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#환경-설정",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "4 환경 설정",
    "text": "4 환경 설정\n\n\n코드\n# API 키를 환경변수로 관리\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적 설정 (선택사항: 디버깅 및 모니터링용)\nfrom langchain_teddynote import logging\nlogging.langsmith(\"CH10-Retriever\")\n\n\n\n\n코드\n# 패키지 설치 (테디노트 커스텀 압축기)\n!pip install -qU langchain-teddynote"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#헬퍼-함수-문서-출력",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#헬퍼-함수-문서-출력",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "5 헬퍼 함수: 문서 출력",
    "text": "5 헬퍼 함수: 문서 출력\n\n\n코드\n# 문서를 시각적으로 보기 좋게 출력하는 함수\ndef pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#기본-retriever-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#기본-retriever-설정",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "6 기본 Retriever 설정",
    "text": "6 기본 Retriever 설정\n벡터 스토어 retriever를 초기화하고 문서를 검색합니다. 기본 retriever는 관련 문서와 무관한 문서를 함께 반환합니다.\n\n\n코드\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# 문서 로드 및 청크 분할\nloader = TextLoader(\"./data/appendix-keywords.txt\")\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\ntexts = loader.load_and_split(text_splitter)\n\n# 벡터 저장소 생성 및 검색기 설정\nretriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n\n# 검색 실행\ndocs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\npretty_print_docs(docs)\n\n\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding\n----------------------------------------------------------------------------------------------------\n문서 2:\n\n정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n\nPage Rank\n----------------------------------------------------------------------------------------------------\n문서 3:\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n----------------------------------------------------------------------------------------------------\n문서 4:\n\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n\n데이터 마이닝\n결과 해석: 기본 리트리버는 쿼리와 무관한 문서(2, 3, 4번)까지 함께 반환됨. 이로 인해 LLM에 불필요한 컨텍스트가 전달되어 비용과 할루시네이션 위험 증가."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#맥락적-압축contextualcompression",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#맥락적-압축contextualcompression",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "7 맥락적 압축(ContextualCompression)",
    "text": "7 맥락적 압축(ContextualCompression)\nLLMChainExtractor는 LLM을 사용하여 문서에서 관련 내용만 추출합니다.\n\n\n코드\nfrom langchain_teddynote.document_compressors import LLMChainExtractor # deprecated lang chain module을 테디가 다시 커스터마이징함\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n\n# LLM을 사용하여 문서 압축기 생성\ncompressor = LLMChainExtractor.from_llm(llm)\n# compressor = LLMChainExtractor.from_llm(llm, prompt= PromptTemplate.from_template(\"\"\"주어진 문서에서 entity 들을 추출해 주세요.\"\"\")) # 이건 원본 문서를 paraphrasing하는 역할을 한다.\ncompression_retriever = ContextualCompressionRetriever(\n    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n    base_compressor=compressor,\n    base_retriever=retriever,\n)\n\n# 일반 리트리버 vs 압축 리트리버 성능 비교\n\n## 일반 리트리버\npretty_print_docs(retriever.invoke(\"Semantic Search 에 대해서 알려줘.\"))\n\nprint(\"=========================================================\")\nprint(\"============== LLMChainExtractor 적용 후 ==================\")\n\n## 압축 리트리버\ncompressed_docs = (\n    compression_retriever.invoke(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n        \"Semantic Search 에 대해서 알려줘.\"\n    )\n)\npretty_print_docs(compressed_docs)  # 검색된 문서를 예쁘게 출력\n\n\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding\n----------------------------------------------------------------------------------------------------\n문서 2:\n\n정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n\nPage Rank\n----------------------------------------------------------------------------------------------------\n문서 3:\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n----------------------------------------------------------------------------------------------------\n문서 4:\n\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n\n데이터 마이닝\n=========================================================\n============== LLMChainExtractor 적용 후 ==================\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n\n결과 해석: - 기존 리트리버: 4개 문서 모두 반환 (약 800토큰) - 압축 리트리버: 관련 문서만 추출 (약 200토큰, 75% 감소) - 효과: 토큰 사용량 감소로 비용 절감, 할루시네이션 위험 감소"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#llmchainfilter-문서-선택-기반-필터링",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#llmchainfilter-문서-선택-기반-필터링",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "8 LLMChainFilter: 문서 선택 기반 필터링",
    "text": "8 LLMChainFilter: 문서 선택 기반 필터링\nLLMChainFilter는 문서 내용을 수정하지 않고 관련 문서만 선택하여 반환합니다.\n\n\n코드\nfrom langchain_teddynote.document_compressors import LLMChainFilter # 입력쿼리와 관련된 내용만 필터링, 문서 내용 변경은 하지않음\n\n# LLM을 사용하여 LLMChainFilter 객체를 생성\n_filter = LLMChainFilter.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성\n    base_compressor=_filter,\n    base_retriever=retriever,\n)\n\ncompressed_docs = compression_retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\npretty_print_docs(compressed_docs)\n\n\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding\n\n8.1 EmbeddingsFilter\n\n각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느리다\nEmbeddingsFilter는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠른 옵션을 제공\n이를 통해 검색 결과의 관련성을 유지하면서도 계산 비용과 시간을 절약\nEmbeddingsFilter 와 ContextualCompressionRetriever 를 사용하여 관련 문서를 압축하고 검색하는 과정\n\nEmbeddingsFilter 를 사용하여 지정된 유사도 임계값(0.86) 이상인 문서를 필터링 합니다.\n\n\n\n\n코드\nfrom langchain.retrievers.document_compressors import EmbeddingsFilter\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\nembeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=embeddings_filter, base_retriever=retriever\n)\n\ncompressed_docs = compression_retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\npretty_print_docs(compressed_docs)\n\n\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#documentcompressorpipeline-다단계-압축",
    "href": "docs/blog/posts/RAG/10-Retriever/02-ContextualCompressionRetriever.html#documentcompressorpipeline-다단계-압축",
    "title": "문맥 압축 검색기(ContextualCompressionRetriever)",
    "section": "9 DocumentCompressorPipeline: 다단계 압축",
    "text": "9 DocumentCompressorPipeline: 다단계 압축\n여러 compressor를 순차적으로 결합하여 최적화된 결과를 얻는다\n파이프라인 단계: 1. TextSplitter: 문서를 작은 청크로 분할 2. EmbeddingsRedundantFilter: 중복 문서 제거 (유사도 0.95 이상) 3. EmbeddingsFilter: 관련성 필터링 (유사도 0.86 이상) 4. LLMChainExtractor: 최종 내용 추출\n\n\n코드\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\nfrom langchain_community.document_transformers import EmbeddingsRedundantFilter\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정합니다.\nsplitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n\n# 임베딩을 사용하여 중복 필터를 생성: 검색된 문서 사이의 유사도 검색을 수행, 0.95도 이상의 문서들은 중복 문서라 판단하고 드랍시켜줌\nredundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n\n# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정합니다.\nrelevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n\npipeline_compressor = DocumentCompressorPipeline(\n    # 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터, LLMChainExtractor를 변환기를 순서대로 설정합니다.\n    # 반드시, 이 기능들을 순서대로 쓰는게 아니라 필요시 파이프라인을 추가할 수 있다는 것에 집중\n    transformers=[\n        splitter,\n        redundant_filter,\n        relevant_filter,\n        LLMChainExtractor.from_llm(llm),\n    ]\n)\n\n\nContextualCompressionRetriever를 초기화하며, base_compressor로 pipeline_compressor를, base_retriever로 retriever를 사용합니다.\n\n\n코드\ncompression_retriever = ContextualCompressionRetriever(\n    # 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화\n    base_compressor=pipeline_compressor,\n    base_retriever=retriever,\n)\n\ncompressed_docs = compression_retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\npretty_print_docs(compressed_docs)\n\n\n문서 1:\n\nSemantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html",
    "title": "앙상블 검색기(Ensemble Retriever)",
    "section": "",
    "text": "EnsembleRetriever는 여러 검색기를 결합하여 더 강력한 검색 결과를 제공하는 LangChain의 기능이다. 다양한 검색 알고리즘의 장점을 활용하여 단일 알고리즘보다 더 나은 성능을 달성할 수 있으며, 사용자들 사이에서 높은 호평을 받고 있다. 다수의 리트리버를 사용하여 나온 결과를 Reciprocal Rank Fusion(RRF) 알고리즘으로 재순위화한 후 하나의 통합 검색 결과를 LLM의 입력으로 제공한다.\n주요 특징\n각 검색기의 장점\n이러한 상호 보완적인 특성으로 인해 EnsembleRetriever는 다양한 검색 시나리오에서 향상된 성능을 제공한다.\n자세한 내용은 LangChain 공식 문서를 참조하자."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#환경-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#환경-설정",
    "title": "앙상블 검색기(Ensemble Retriever)",
    "section": "1 환경 설정",
    "text": "1 환경 설정\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력\nlogging.langsmith(\"CH10-Retriever\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#기본-사용법-두-개의-검색기-결합",
    "href": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#기본-사용법-두-개의-검색기-결합",
    "title": "앙상블 검색기(Ensemble Retriever)",
    "section": "2 기본 사용법: 두 개의 검색기 결합",
    "text": "2 기본 사용법: 두 개의 검색기 결합\nEnsembleRetriever를 초기화하여 BM25Retriever와 FAISS 검색기를 결합한다. 각 검색기는 고유한 검색 방식을 사용한다: - BM25Retriever: 키워드 기반 검색 (sparse search) - FAISS: 임베딩 기반 의미 검색 (dense search)\n\n\n코드\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n\n# 샘플 문서 데이터셋 (Apple 관련 문서들)\ndoc_list = [\n    \"I like apples\",\n    \"I like apple company\",\n    \"I like apple's iphone\",\n    \"Apple is my favorite company\",\n    \"I like apple's ipad\",\n    \"I like apple's macbook\",\n]\n\n# BM25 Retriever 초기화 (키워드 기반 검색)\nbm25_retriever = BM25Retriever.from_texts(doc_list)\nbm25_retriever.k = 1  # 검색 결과 개수\n\n# FAISS Retriever 초기화 (의미 기반 검색)\nembedding = OpenAIEmbeddings()\nfaiss_vectorstore = FAISS.from_texts(doc_list, embedding)\nfaiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 1})\n\n# Ensemble Retriever 생성 (가중치: BM25 70%, FAISS 30%)\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, faiss_retriever],\n    weights=[0.7, 0.3],  # BM25에 더 높은 가중치 부여\n)\n\n\n\n2.1 예제 1: 키워드 기반 검색에 유리한 쿼리\nensemble_retriever 객체를 사용하여 관련 문서를 검색한다. 각 검색기의 결과를 비교하면 가중치의 영향을 확인할 수 있다.\n\n\n코드\n# 검색 실행\nquery = \"my favorite fruit is apple\"\nensemble_result = ensemble_retriever.invoke(query)\nbm25_result = bm25_retriever.invoke(query)\nfaiss_result = faiss_retriever.invoke(query)\n\n# 결과 출력\nprint(\"[Ensemble Retriever 결과]\")\nfor doc in ensemble_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\nprint(\"[BM25 Retriever 결과]\")\nfor doc in bm25_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\nprint(\"[FAISS Retriever 결과]\")\nfor doc in faiss_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\n\n[Ensemble Retriever 결과]\n  I like apples\n\n[BM25 Retriever 결과]\n  I like apples\n\n[FAISS Retriever 결과]\n  Apple is my favorite company\n\n결과 해석: - BM25는 정확한 키워드 매칭(“apple”과 “fruit”)으로 “I like apples” 문서를 찾음 - FAISS는 의미 유사성으로 “favorite”과 관련된 “Apple is my favorite company” 문서를 찾음 - Ensemble은 BM25 가중치(0.7)가 높아서 BM25 결과를 우선함\n\n\n2.2 예제 2: 의미 검색이 더 유용한 복잡한 쿼리\n더 자연스러운 문장 형태의 쿼리로 두 검색 방식의 차이를 관찰한다.\n\n\n코드\n# 복잡한 쿼리 실행\nquery = \"Apple company makes my favorite iphone\"\nensemble_result = ensemble_retriever.invoke(query)\nbm25_result = bm25_retriever.invoke(query)\nfaiss_result = faiss_retriever.invoke(query)\n\n# 결과 출력\nprint(\"[Ensemble Retriever 결과]\")\nfor doc in ensemble_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\nprint(\"[BM25 Retriever]\")\nfor doc in bm25_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\nprint(\"[FAISS Retriever 결과]\")\nfor doc in faiss_result:\n    print(f\"Content: {doc.page_content}\")\n    print()\n\n\n[Ensemble Retriever 결과]\n  Apple is my favorite company\n  I like apple's iphone\n\n[BM25 Retriever 결과]\n  Apple is my favorite company\n\n[FAISS Retriever 결과]\n  I like apple's iphone\n결과 해석: - BM25는 “Apple”과 “company”라는 정확한 키워드에만 반응 → 한 문서만 반환 - FAISS는 “favorite”과 “iphone”의 의미 관계를 이해 → 더 정확한 문서 검색 - Ensemble은 두 결과를 결합 → 다양한 관점에서의 정보 제공 - 실제 사용에서는 의미 검색의 가중치를 높이는 것이 복잡한 쿼리에서 더 효과적"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#고급-기능-런타임에-가중치-동적-변경",
    "href": "docs/blog/posts/RAG/10-Retriever/03-EnsembleRetriever.html#고급-기능-런타임에-가중치-동적-변경",
    "title": "앙상블 검색기(Ensemble Retriever)",
    "section": "3 고급 기능: 런타임에 가중치 동적 변경",
    "text": "3 고급 기능: 런타임에 가중치 동적 변경\nConfigurableField를 사용하면 프로그램 실행 중(런타임)에 각 검색기의 가중치를 동적으로 변경할 수 있다. 이는 다양한 쿼리 유형에 대응하는 유연한 검색 시스템을 구축할 수 있다는 장점이 있다.\n\n3.1 동적 가중치 설정 구성\n\n\n코드\nfrom langchain_core.runnables import ConfigurableField\n\n# ConfigurableField를 사용한 동적 가중치 설정\nensemble_retriever = EnsembleRetriever(\n    # 리트리버 목록을 설정합니다. 여기서는 bm25_retriever와 faiss_retriever를 사용합니다.\n    retrievers=[bm25_retriever, faiss_retriever],\n).configurable_fields(\n    weights=ConfigurableField(\n        id=\"ensemble_weights\",  # 외부에서 참조할 식별자\n        name=\"Ensemble Weights\",\n        description=\"BM25와 FAISS 검색기의 가중치 비율 (합계=1.0)\",\n    )\n)\n\n\n\n검색 시 config 매개변수를 통해 검색 설정을 지정\n\nensemble_weights 옵션의 가중치를 [1, 0]으로 설정하여 모든 검색 결과의 가중치가 BM25 retriever 에 더 많이 부여 되도록\n\n\n\n\n코드\n# BM25 전용 검색 설정\nconfig = {\"configurable\": {\"ensemble_weights\": [1, 0]}}\n\n# 동일한 쿼리로 검색 실행\ndocs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n\nprint(\"[BM25만 사용 (가중치 [1.0, 0.0])]\")\nfor i, doc in enumerate(docs, 1):\n    print(f\"{i}. {doc.page_content}\")\n\n\n[BM25만 사용 (가중치 [1.0, 0.0])]\n1. I like apples\n결과 해석: BM25만 사용할 때는 정확한 키워드 매칭만 이루어짐. “apple”과 “fruit”을 포함한 문서만 반환됨.\n\n\n3.2 케이스 2: FAISS 가중치 극대화 (의미 기반 검색)\nFAISS 검색기에 전체 가중치(1.0)를 할당하여 의미 기반 검색만 수행한다.\n\n\n코드\n# FAISS 전용 검색 설정\nconfig = {\"configurable\": {\"ensemble_weights\": [0, 1]}}\n\n# 동일한 쿼리로 검색 실행\ndocs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n\nprint(\"[FAISS만 사용 (가중치 [0.0, 1.0])]\")\nfor i, doc in enumerate(docs, 1):\n    print(f\"{i}. {doc.page_content}\")\n\n\n[FAISS만 사용 (가중치 [0.0, 1.0])]\n1. Apple is my favorite company\n결과 해석: FAISS만 사용할 때는 의미 유사성으로 검색. “my favorite”이라는 표현을 “Apple is my favorite company”에서 찾아 반환함."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html",
    "title": "Ensemble Retriever Convex Combination(CC) 추가",
    "section": "",
    "text": "written by@teddynote\n아래의 주석을 풀고 패키지를 업데이트 후 진행합니다.\n코드\n# 업데이트 후 진행\n# !pip install -qU langchain-teddynote\n코드\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html#실험을-위한-사전-셋업",
    "href": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html#실험을-위한-사전-셋업",
    "title": "Ensemble Retriever Convex Combination(CC) 추가",
    "section": "1 실험을 위한 사전 셋업",
    "text": "1 실험을 위한 사전 셋업\n\n\n코드\nfrom langchain.retrievers import EnsembleRetriever as OriginalEnsembleRetriever\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PDFPlumberLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_teddynote.retrievers import KiwiBM25Retriever\n\n# 문서 로드(Load Documents)\nloader = PDFPlumberLoader(\"data/디지털정부혁신 추진계획.pdf\")\n\n# 문서 분할(Split Documents): 테스트를 위하여 작은 Chunk Size로 설정\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\nsplit_documents = loader.load_and_split(text_splitter)\n\n# 임베딩(Embedding) 생성\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# FaissRetriever 생성\nfaiss = FAISS.from_documents(\n    documents=split_documents, embedding=embeddings\n).as_retriever(search_kwargs={\"k\": 5})\n\n# KiwiBM25Retriever 생성(한글 형태소 분석기 + BM25 알고리즘)\nbm25 = KiwiBM25Retriever.from_documents(documents=split_documents, embedding=embeddings)\nbm25.k = 5\n\n# LangChain 버전의 EnsembleRetriever\noriginal_ensemble_retriever = OriginalEnsembleRetriever(retrievers=[faiss, bm25])\n\n\nCC 방식과 RRF 방식의 EnsembleRetriever 생성\n\n\n코드\nfrom langchain_teddynote.retrievers import (\n    EnsembleRetriever,\n    EnsembleMethod,\n)\n\n# RRF 방식의 EnsembleRetriever (기본값으로 RRF 가 설정되어 있음)\nrrf_ensemble_retriever = EnsembleRetriever(\n    retrievers=[faiss, bm25], method=EnsembleMethod.RRF\n)\n\n# CC 방식의 EnsembleRetriever\ncc_ensemble_retriever = EnsembleRetriever(\n    retrievers=[faiss, bm25], method=EnsembleMethod.CC  # method 지정: CC\n)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html#검색-결과-비교",
    "href": "docs/blog/posts/RAG/10-Retriever/11-CC-EnsembleRetriever.html#검색-결과-비교",
    "title": "Ensemble Retriever Convex Combination(CC) 추가",
    "section": "2 검색 결과 비교",
    "text": "2 검색 결과 비교\n\n\n코드\ndef pretty_print(query):\n    for i, (original_doc, cc_doc, rrf_doc) in enumerate(\n        zip(\n            original_ensemble_retriever.invoke(query),\n            cc_ensemble_retriever.invoke(query),\n            rrf_ensemble_retriever.invoke(query),\n        )\n    ):\n        print(f\"[{i}] [Original] Q: {query}\", end=\"\\n\\n\")\n        print(original_doc.page_content)\n        print(\"-\" * 100)\n        print(f\"[{i}] [RRF] Q: {query}\", end=\"\\n\\n\")\n        print(rrf_doc.page_content)\n        print(\"-\" * 100)\n        print(f\"[{i}] [CC] Q: {query}\", end=\"\\n\\n\")\n        print(cc_doc.page_content)\n        print(\"=\" * 100, end=\"\\n\\n\")\n\n\n\n검색 결과에 \"Original\" 과 \"RRF\" 는 차이가 없어야 합니다. (LangChain 그대로 구현)\n검색 결과에 \"CC\" 는 \"RRF\" 와 차이가 있을 수 있습니다.\n\nRRF 와 CC 방식의 검색 결과 비교하여 문서에 적합한 방식을 차용하시길 바랍니다.\n\n\n코드\n# 검색 결과 비교\npretty_print(\"디지털 트랜스포메이션이란 무엇인가요?\")\n\n\n[0] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 1 디지털 정부혁신 추진전략\n디지털로 여는 좋은 세상\n□ 비전\n※ 부제 : 대한민국이 먼저 갑니다.\n□ 추진원칙 △ 최종 이용자의 관점에서\n△ 공공서비스 수준 향상을 목표로\n----------------------------------------------------------------------------------------------------\n[0] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 1 디지털 정부혁신 추진전략\n디지털로 여는 좋은 세상\n□ 비전\n※ 부제 : 대한민국이 먼저 갑니다.\n□ 추진원칙 △ 최종 이용자의 관점에서\n△ 공공서비스 수준 향상을 목표로\n----------------------------------------------------------------------------------------------------\n[0] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (시스템) 디지털 신기술의 적기 도입과 활용 곤란\n- 기존 복잡한 용역개발 방식은 혁신주기가 짧은 디지털 전환에 부적합\n====================================================================================================\n\n[1] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (시스템) 디지털 신기술의 적기 도입과 활용 곤란\n- 기존 복잡한 용역개발 방식은 혁신주기가 짧은 디지털 전환에 부적합\n----------------------------------------------------------------------------------------------------\n[1] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (시스템) 디지털 신기술의 적기 도입과 활용 곤란\n- 기존 복잡한 용역개발 방식은 혁신주기가 짧은 디지털 전환에 부적합\n----------------------------------------------------------------------------------------------------\n[1] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (디지털 고지‧수납) 각종 고지서·안내문* 등을 온라인(공공‧민간)\n으로 받고, 간편하게 납부할 수 있도록 디지털 고지‧수납 활성화\n====================================================================================================\n\n[2] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\nⅠ. 개 요\n□ 추진 배경\n○ 우리나라는 국가적 초고속 정보통신망 투자와 적극적인 공공정보화\n사업 추진에 힘입어 세계 최고수준의 전자정부를 구축‧운영\n----------------------------------------------------------------------------------------------------\n[2] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\nⅠ. 개 요\n□ 추진 배경\n○ 우리나라는 국가적 초고속 정보통신망 투자와 적극적인 공공정보화\n사업 추진에 힘입어 세계 최고수준의 전자정부를 구축‧운영\n----------------------------------------------------------------------------------------------------\n[2] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 1 디지털 정부혁신 추진전략\n디지털로 여는 좋은 세상\n□ 비전\n※ 부제 : 대한민국이 먼저 갑니다.\n□ 추진원칙 △ 최종 이용자의 관점에서\n△ 공공서비스 수준 향상을 목표로\n====================================================================================================\n\n[3] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (디지털 고지‧수납) 각종 고지서·안내문* 등을 온라인(공공‧민간)\n으로 받고, 간편하게 납부할 수 있도록 디지털 고지‧수납 활성화\n----------------------------------------------------------------------------------------------------\n[3] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ (디지털 고지‧수납) 각종 고지서·안내문* 등을 온라인(공공‧민간)\n으로 받고, 간편하게 납부할 수 있도록 디지털 고지‧수납 활성화\n----------------------------------------------------------------------------------------------------\n[3] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ 오픈소스 중심의 디지털정부 생태계와 공공시장 수요를 바탕으로\n첨단 디지털 산업의 혁신 가속화와 글로벌 도약을 위한 전기 마련\n====================================================================================================\n\n[4] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n보다 안정성‧편의성이 높은 스마트폰 기반 디지털 신분증 도입\n* 학생증, 공무원증 등 이용대상과 목적이 명확한 분야부터 안전성 점검 후 단계적 확대\n----------------------------------------------------------------------------------------------------\n[4] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n보다 안정성‧편의성이 높은 스마트폰 기반 디지털 신분증 도입\n* 학생증, 공무원증 등 이용대상과 목적이 명확한 분야부터 안전성 점검 후 단계적 확대\n----------------------------------------------------------------------------------------------------\n[4] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털 정부혁신 추진계획\n2019. 10. 29.\n관계부처 합동\n====================================================================================================\n\n[5] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ 오픈소스 중심의 디지털정부 생태계와 공공시장 수요를 바탕으로\n첨단 디지털 산업의 혁신 가속화와 글로벌 도약을 위한 전기 마련\n----------------------------------------------------------------------------------------------------\n[5] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n○ 오픈소스 중심의 디지털정부 생태계와 공공시장 수요를 바탕으로\n첨단 디지털 산업의 혁신 가속화와 글로벌 도약을 위한 전기 마련\n----------------------------------------------------------------------------------------------------\n[5] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\nⅠ. 개 요\n□ 추진 배경\n○ 우리나라는 국가적 초고속 정보통신망 투자와 적극적인 공공정보화\n사업 추진에 힘입어 세계 최고수준의 전자정부를 구축‧운영\n====================================================================================================\n\n[6] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털기기 사용이 어려운 분들이 차별없이 서비스를 받도록 지원하겠습니다.\n권익위,\n□1 국민의 소리 청취·분석 시스템 개선 (22년)\n각부처·지자체\n----------------------------------------------------------------------------------------------------\n[6] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털기기 사용이 어려운 분들이 차별없이 서비스를 받도록 지원하겠습니다.\n권익위,\n□1 국민의 소리 청취·분석 시스템 개선 (22년)\n각부처·지자체\n----------------------------------------------------------------------------------------------------\n[6] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n보다 안정성‧편의성이 높은 스마트폰 기반 디지털 신분증 도입\n* 학생증, 공무원증 등 이용대상과 목적이 명확한 분야부터 안전성 점검 후 단계적 확대\n====================================================================================================\n\n[7] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털 정부혁신 추진계획\n2019. 10. 29.\n관계부처 합동\n----------------------------------------------------------------------------------------------------\n[7] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털 정부혁신 추진계획\n2019. 10. 29.\n관계부처 합동\n----------------------------------------------------------------------------------------------------\n[7] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n디지털기기 사용이 어려운 분들이 차별없이 서비스를 받도록 지원하겠습니다.\n권익위,\n□1 국민의 소리 청취·분석 시스템 개선 (22년)\n각부처·지자체\n====================================================================================================\n\n[8] [Original] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 3 디지털 정부혁신 기대효과\n◈ 동 방안을 성공적으로 추진하는 경우, 정부 디지털 서비스에서 세계 최고\n----------------------------------------------------------------------------------------------------\n[8] [RRF] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 3 디지털 정부혁신 기대효과\n◈ 동 방안을 성공적으로 추진하는 경우, 정부 디지털 서비스에서 세계 최고\n----------------------------------------------------------------------------------------------------\n[8] [CC] Q: 디지털 트랜스포메이션이란 무엇인가요?\n\n참고 3 디지털 정부혁신 기대효과\n◈ 동 방안을 성공적으로 추진하는 경우, 정부 디지털 서비스에서 세계 최고\n===================================================================================================="
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html",
    "title": "MultiQueryRetriever",
    "section": "",
    "text": "MultiQueryRetriever는 LLM이 사용자의 단일 쿼리를 기반으로 여러 개의 다양한 쿼리를 자동 생성하여 검색 성능을 향상시키는 고급 검색 기법이다.\n\n\n\n쿼리 확장: 사용자의 원본 질문을 LLM을 통해 여러 관점의 질문들로 변환\n\n다중 검색: 생성된 각 쿼리로 독립적인 검색 수행\n\n결과 합성: 모든 검색 결과를 합집합하여 포괄적인 문서 집합 구성\n\n\n\n\n\n\n\n게으른 사용자 문제\n대부분의 사용자는 상세하고 구체적인 질문을 작성하지 않는다. 이로 인해:\n- LLM 입력 데이터가 불충분해짐\n- 검색 결과의 품질과 범위가 제한됨\n- 사용자가 원하는 정보를 놓칠 가능성 증가\n비논리적 질문 문제\n사람마다 논리적 개성이 강하기 때문에 다음과 같은 문제가 발생한다:\n- 논리적 비약이 포함된 질문\n- 구체성이 부족한 모호한 표현\n- 비유기적이고 정돈되지 않은 문장 구조\n- 질문자만 이해하고 타인은 이해하기 어려운 표현\n\n\n\n\n\n거리 기반 벡터 데이터베이스 검색은 고차원 공간에서 쿼리 임베딩과 문서 임베딩 간의 ’거리’를 기준으로 유사한 문서를 찾는다. 하지만 다음과 같은 한계가 있다:\n\n\n\n벡터 공간에서의 거리 계산은 매우 민감하게 작동한다:\n\n고차원 공간의 차원 저주: 768차원(BERT) 또는 1536차원(OpenAI) 공간에서 작은 변화도 거리에 큰 영향\n\n단어 순서와 표현의 미묘한 차이: “어떻게 사용하나요?” vs “사용법이 무엇인가요?”는 의미상 동일하지만 벡터 공간에서는 다른 위치\n\n동의어와 유의어 처리의 한계: “방법”과 “방식”, “사용법”과 “활용법”이 벡터 공간에서 멀리 떨어져 위치할 수 있음\n\n구체적인 예시:\n쿼리 A: \"OpenAI API 사용방법\"     → 벡터 위치 [0.1, 0.8, -0.3, ...]\n쿼리 B: \"OpenAI API 활용법\"      → 벡터 위치 [0.2, 0.7, -0.1, ...]\n쿼리 C: \"OpenAI API를 어떻게 써요?\" → 벡터 위치 [0.5, 0.4, 0.2, ...]\n\n동일한 의도의 질문이지만 벡터 공간에서 서로 다른 위치에 배치되어\n완전히 다른 문서들이 검색될 수 있음\n\n\n\n\n코사인 유사도의 함정: 벡터의 방향성만 고려하여 의미의 강도나 맥락적 뉘앙스 손실\n\n임베딩 모델의 편향: 학습 데이터의 특성에 따라 특정 표현 방식에 치우친 결과\n\n문맥 길이의 영향: 짧은 쿼리와 긴 쿼리가 동일한 공간에서 비교되어 부정확한 유사도 계산\n\n임베딩이 데이터의 의미를 완전히 포착하지 못할 수 있음\n\n단일 관점의 검색으로 인한 정보 누락 가능성\n\n수동 프롬프트 엔지니어링의 번거로움\n\n\n\n\n\n\n\n\nMultiQueryRetriever는 LLM(Language Learning Model)을 활용하여 주어진 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 자동으로 생성한다. 이를 통해 프롬프트 튜닝 과정을 자동화한다.\n\n\n\n각각의 생성된 쿼리에 대해 관련 문서 집합을 검색하고, 모든 쿼리를 아우르는 고유한 문서들의 합집합을 추출하여 잠재적으로 관련된 더 큰 문서 집합을 얻는다.\n\n\n\n사용자의 비정돈된 질문을 MultiQuery를 통해 논리정연하고 완전한 형식의 문장 및 문단으로 paraphrasing하는 효과가 있다. 이러한 paraphrased text들은 LLM이 이해하기 적합한 형태로 나오기 때문에 답변 성능에 긍정적인 효과를 기대할 수 있다.\n\n\n\n사용자의 무의식적 어휘 선택\n대부분의 사용자는 질문을 작성할 때 어휘 선택(vocabulary choice)을 세심하게 고려하지 않는다. 하지만 벡터 임베딩에서는 각 단어가 고유한 의미 공간을 차지하므로, 유사한 의미의 단어들도 상당한 거리 차이를 보이는 벡터를 생성한다.\n동의어 간 벡터 거리의 문제\n동일하거나 유사한 개념을 표현하는 단어들이 임베딩 공간에서 예상보다 멀리 떨어져 배치되는 현상:\n한국어 예시:\n- \"이용\" → 벡터 A [0.2, 0.8, -0.1, ...]\n- \"사용\" → 벡터 B [0.1, 0.6, 0.3, ...]  \n- \"활용\" → 벡터 C [0.4, 0.2, -0.2, ...]\n\n영어 예시:\n- \"use\" → 벡터 D [0.3, 0.7, 0.1, ...]\n- \"utilize\" → 벡터 E [0.1, 0.4, -0.3, ...]\n- \"exploit\" → 벡터 F [-0.2, 0.5, 0.4, ...]\n- \"take advantage of\" → 벡터 G [0.6, -0.1, 0.2, ...]\n실제 검색 영향 사례\n사용자 질문 A: \"OpenAI API를 이용하는 방법\"\n사용자 질문 B: \"OpenAI API를 사용하는 방법\"  \n사용자 질문 C: \"OpenAI API를 활용하는 방법\"\n\n→ 동일한 의도이지만 완전히 다른 검색 결과를 얻을 수 있음\n→ 특정 단어로 작성된 문서만 검색되고 나머지는 누락될 위험\n언어적 뉘앙스의 벡터화 한계\n\n격식의 정도: “사용”(일반적) vs “이용”(격식) vs “활용”(적극적)\n\n맥락적 의미: “exploit”(부정적 뉘앙스) vs “utilize”(중립적) vs “use”(일반적)\n\n언어 혼재: 한국어-영어 혼용 시 더욱 복잡한 벡터 공간 형성\n\n이러한 어휘 선택의 민감성으로 인해 사용자가 선택한 특정 단어에 따라 검색 품질이 크게 좌우되는 문제가 발생한다.\n\n\n\n여러 관점에서 동일한 질문을 생성함으로써, MultiQueryRetriever는 거리 기반 검색의 제한을 일정 부분 극복하고, 더욱 풍부한 검색 결과를 제공할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#핵심-개념",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#핵심-개념",
    "title": "MultiQueryRetriever",
    "section": "",
    "text": "MultiQueryRetriever는 LLM이 사용자의 단일 쿼리를 기반으로 여러 개의 다양한 쿼리를 자동 생성하여 검색 성능을 향상시키는 고급 검색 기법이다.\n\n\n\n쿼리 확장: 사용자의 원본 질문을 LLM을 통해 여러 관점의 질문들로 변환\n\n다중 검색: 생성된 각 쿼리로 독립적인 검색 수행\n\n결과 합성: 모든 검색 결과를 합집합하여 포괄적인 문서 집합 구성"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#왜-필요한가",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#왜-필요한가",
    "title": "MultiQueryRetriever",
    "section": "",
    "text": "게으른 사용자 문제\n대부분의 사용자는 상세하고 구체적인 질문을 작성하지 않는다. 이로 인해:\n- LLM 입력 데이터가 불충분해짐\n- 검색 결과의 품질과 범위가 제한됨\n- 사용자가 원하는 정보를 놓칠 가능성 증가\n비논리적 질문 문제\n사람마다 논리적 개성이 강하기 때문에 다음과 같은 문제가 발생한다:\n- 논리적 비약이 포함된 질문\n- 구체성이 부족한 모호한 표현\n- 비유기적이고 정돈되지 않은 문장 구조\n- 질문자만 이해하고 타인은 이해하기 어려운 표현\n\n\n\n\n\n거리 기반 벡터 데이터베이스 검색은 고차원 공간에서 쿼리 임베딩과 문서 임베딩 간의 ’거리’를 기준으로 유사한 문서를 찾는다. 하지만 다음과 같은 한계가 있다:\n\n\n\n벡터 공간에서의 거리 계산은 매우 민감하게 작동한다:\n\n고차원 공간의 차원 저주: 768차원(BERT) 또는 1536차원(OpenAI) 공간에서 작은 변화도 거리에 큰 영향\n\n단어 순서와 표현의 미묘한 차이: “어떻게 사용하나요?” vs “사용법이 무엇인가요?”는 의미상 동일하지만 벡터 공간에서는 다른 위치\n\n동의어와 유의어 처리의 한계: “방법”과 “방식”, “사용법”과 “활용법”이 벡터 공간에서 멀리 떨어져 위치할 수 있음\n\n구체적인 예시:\n쿼리 A: \"OpenAI API 사용방법\"     → 벡터 위치 [0.1, 0.8, -0.3, ...]\n쿼리 B: \"OpenAI API 활용법\"      → 벡터 위치 [0.2, 0.7, -0.1, ...]\n쿼리 C: \"OpenAI API를 어떻게 써요?\" → 벡터 위치 [0.5, 0.4, 0.2, ...]\n\n동일한 의도의 질문이지만 벡터 공간에서 서로 다른 위치에 배치되어\n완전히 다른 문서들이 검색될 수 있음\n\n\n\n\n코사인 유사도의 함정: 벡터의 방향성만 고려하여 의미의 강도나 맥락적 뉘앙스 손실\n\n임베딩 모델의 편향: 학습 데이터의 특성에 따라 특정 표현 방식에 치우친 결과\n\n문맥 길이의 영향: 짧은 쿼리와 긴 쿼리가 동일한 공간에서 비교되어 부정확한 유사도 계산\n\n임베딩이 데이터의 의미를 완전히 포착하지 못할 수 있음\n\n단일 관점의 검색으로 인한 정보 누락 가능성\n\n수동 프롬프트 엔지니어링의 번거로움"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#multiqueryretriever의-해결책",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#multiqueryretriever의-해결책",
    "title": "MultiQueryRetriever",
    "section": "",
    "text": "MultiQueryRetriever는 LLM(Language Learning Model)을 활용하여 주어진 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 자동으로 생성한다. 이를 통해 프롬프트 튜닝 과정을 자동화한다.\n\n\n\n각각의 생성된 쿼리에 대해 관련 문서 집합을 검색하고, 모든 쿼리를 아우르는 고유한 문서들의 합집합을 추출하여 잠재적으로 관련된 더 큰 문서 집합을 얻는다.\n\n\n\n사용자의 비정돈된 질문을 MultiQuery를 통해 논리정연하고 완전한 형식의 문장 및 문단으로 paraphrasing하는 효과가 있다. 이러한 paraphrased text들은 LLM이 이해하기 적합한 형태로 나오기 때문에 답변 성능에 긍정적인 효과를 기대할 수 있다.\n\n\n\n사용자의 무의식적 어휘 선택\n대부분의 사용자는 질문을 작성할 때 어휘 선택(vocabulary choice)을 세심하게 고려하지 않는다. 하지만 벡터 임베딩에서는 각 단어가 고유한 의미 공간을 차지하므로, 유사한 의미의 단어들도 상당한 거리 차이를 보이는 벡터를 생성한다.\n동의어 간 벡터 거리의 문제\n동일하거나 유사한 개념을 표현하는 단어들이 임베딩 공간에서 예상보다 멀리 떨어져 배치되는 현상:\n한국어 예시:\n- \"이용\" → 벡터 A [0.2, 0.8, -0.1, ...]\n- \"사용\" → 벡터 B [0.1, 0.6, 0.3, ...]  \n- \"활용\" → 벡터 C [0.4, 0.2, -0.2, ...]\n\n영어 예시:\n- \"use\" → 벡터 D [0.3, 0.7, 0.1, ...]\n- \"utilize\" → 벡터 E [0.1, 0.4, -0.3, ...]\n- \"exploit\" → 벡터 F [-0.2, 0.5, 0.4, ...]\n- \"take advantage of\" → 벡터 G [0.6, -0.1, 0.2, ...]\n실제 검색 영향 사례\n사용자 질문 A: \"OpenAI API를 이용하는 방법\"\n사용자 질문 B: \"OpenAI API를 사용하는 방법\"  \n사용자 질문 C: \"OpenAI API를 활용하는 방법\"\n\n→ 동일한 의도이지만 완전히 다른 검색 결과를 얻을 수 있음\n→ 특정 단어로 작성된 문서만 검색되고 나머지는 누락될 위험\n언어적 뉘앙스의 벡터화 한계\n\n격식의 정도: “사용”(일반적) vs “이용”(격식) vs “활용”(적극적)\n\n맥락적 의미: “exploit”(부정적 뉘앙스) vs “utilize”(중립적) vs “use”(일반적)\n\n언어 혼재: 한국어-영어 혼용 시 더욱 복잡한 벡터 공간 형성\n\n이러한 어휘 선택의 민감성으로 인해 사용자가 선택한 특정 단어에 따라 검색 품질이 크게 좌우되는 문제가 발생한다.\n\n\n\n여러 관점에서 동일한 질문을 생성함으로써, MultiQueryRetriever는 거리 기반 검색의 제한을 일정 부분 극복하고, 더욱 풍부한 검색 결과를 제공할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#api-키-및-추적-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#api-키-및-추적-설정",
    "title": "MultiQueryRetriever",
    "section": "2.1 API 키 및 추적 설정",
    "text": "2.1 API 키 및 추적 설정\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n\n\n\n\n코드\n# LangSmith 추적을 설정한다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력한다.\nlogging.langsmith(\"CH10-Retriever\")\n\n\n\n\n코드\n# 샘플 벡터DB 구축\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# 블로그 포스트 (Parent Document)  로드\nloader = WebBaseLoader(\n    \"https://teddylee777.github.io/openai/openai-assistant-tutorial/\", encoding=\"utf-8\"\n)\n\n# 문서 분할\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\ndocs = loader.load_and_split(text_splitter)\n\n# 임베딩 정의\nopenai_embedding = OpenAIEmbeddings()\n\n# 벡터DB 생성\ndb = FAISS.from_documents(docs, openai_embedding)\n\n# retriever 생성\nretriever = db.as_retriever()\n\n# 문서 검색\nquery = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\nrelevant_docs = retriever.invoke(query)\n\n# 검색된 문서의 개수 출력\nlen(relevant_docs)\n\n\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n4\n검색된 결과 중 1개 문서의 내용을 출력한다.\n\n\n코드\n# 1번 문서를 출력합니다.\nprint(relevant_docs[1].page_content)\n\n\n가장 강력한 도구로서, Assistant에게 사용자 정의 함수를 지정할 수 있습니다. 이는 Chat Completions API에서의 함수 호출과 매우 유사합니다.\n\n\nFunction calling(함수 호출) 도구를 사용하면 Assistant 에게 사용자 정의 함수 를 설명하여 호출해야 하는 함수를 인자와 함께 지능적으로 반환하도록 할 수 있습니다.\n\n\nAssistant API는 실행 중에 함수를 호출할 때 실행을 일시 중지하며, 함수 호출 결과를 다시 제공하여 Run 실행을 계속할 수 있습니다. (이는 사용자 피드백을 받아 재게할 수 있는 의미이기도 합니다. 아래 튜토리얼에서 상세히 다룹니다)."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#기본-사용법",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#기본-사용법",
    "title": "MultiQueryRetriever",
    "section": "3.1 기본 사용법",
    "text": "3.1 기본 사용법\nMultiQueryRetriever에 사용할 LLM을 지정하고 질의 생성에 사용하면, retriever가 나머지 작업을 처리한다.\n\n3.1.1 MultiQueryRetriever 생성\n\n\n코드\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\n\n# ChatOpenAI 언어 모델을 초기화한다. temperature는 0으로 설정한다.\nllm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n\nmultiquery_retriever = MultiQueryRetriever.from_llm(  # MultiQueryRetriever를 언어 모델을 사용하여 초기화한다.\n    # 벡터 데이터베이스의 retriever와 언어 모델을 전달한다.\n    retriever=db.as_retriever(),\n    llm=llm,\n)\n\n\n아래는 다중 쿼리를 생성하는 중간 과정을 디버깅하기 위하여 실행하는 코드이다.\n먼저 \"langchain.retrievers.multi_query\" 로거를 가져온다. 이는 logging.getLogger() 함수를 사용하여 수행된다. 그 다음, 이 로거의 로그 레벨을 INFO로 설정하여, INFO 레벨 이상의 로그 메시지만 출력되도록 할 수 있다.\n\n\n코드\n# 쿼리에 대한 로깅 설정\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n\n\n\n\n3.1.2 MultiQueryRetriever 실행 및 결과 분석\n이 코드는 multiquery_retriever 객체의 invoke 메서드를 사용하여 주어진 question과 관련된 문서를 검색한다. 검색된 문서들은 unique_docs라는 변수에 저장되며, 이 변수의 길이를 확인함으로써 검색된 관련 문서의 총 개수를 알 수 있다. 이 과정을 통해 사용자의 질문에 대한 관련 정보를 효과적으로 찾아내고 그 양을 파악할 수 있다.\n\n\n코드\n# 질문을 정의한다.\nquestion = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n# 문서 검색\nrelevant_docs = multiquery_retriever.invoke(question)\n\n# 검색된 고유한 문서의 개수를 반환합니다.\nprint(\n    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n    end=\"\\n===============\\n\",\n)\n\n# 검색된 문서의 내용을 출력합니다.\nprint(relevant_docs[0].page_content)\n\n\nINFO:langchain.retrievers.multi_query:Generated queries: ['OpenAI Assistant API에서 Functions 기능을 사용하는 방법에 대해 설명해 주세요.  ', 'OpenAI Assistant API의 Functions를 활용하는 방법은 무엇인가요?  ', 'OpenAI Assistant API의 Functions 사용에 대한 가이드를 제공해 주실 수 있나요?']\n\n\n질문 쿼리가 3개로 늘어남\n\n===============\n검색된 문서 개수: 5\n===============\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.\n\n\n\n주요내용\n\n단일 쿼리 질문에서는 검색된 문서 개수가 4개\n추가된 질문 쿼리를 합하여 검색된 문서 개수가 5개 (1개가 더 늘어남)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#고급-기법-lcel-chain-활용-방법",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#고급-기법-lcel-chain-활용-방법",
    "title": "MultiQueryRetriever",
    "section": "3.2 고급 기법: LCEL Chain 활용 방법",
    "text": "3.2 고급 기법: LCEL Chain 활용 방법\n\n3.2.1 사용자 정의 프롬프트로 커스터마이징\n기본 MultiQueryRetriever 대신 LCEL(LangChain Expression Language) Chain을 활용하여 더욱 세밀한 제어가 가능한 커스텀 리트리버를 만들 수 있다.\n주요 특징:\n- 사용자 정의 프롬프트 정의 가능\n- 생성될 쿼리 개수 조정 가능 (예: 5개)\n- 생성된 쿼리를 \"\\n\" 구분자로 구분하여 반환\n- 사용자 정의 Chain과 함께 사용 가능\n\n\n3.2.2 LCEL Chain 구현\n\n\n코드\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# 프롬프트 템플릿을 정의한다.(5개의 질문을 생성하도록 프롬프트를 작성하였다)\nprompt = PromptTemplate.from_template(\n    \"\"\"You are an AI language model assistant. \nYour task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \nBy generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. \nYour response should be a list of values separated by new lines, eg: `foo\\nbar\\nbaz\\n`\n\n#ORIGINAL QUESTION: \n{question}\n\n#Answer in Korean:\n\"\"\"\n)\n\n# 언어 모델 인스턴스를 생성한다.\nllm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n\n# LLMChain을 생성한다.\ncustom_multiquery_chain = (\n    {\"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n)\n\n# 질문을 정의한다.\nquestion = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n\n# 체인을 실행하여 생성된 다중 쿼리를 확인한다.\nmulti_queries = custom_multiquery_chain.invoke(question)\n# 결과를 확인한다.(5개 질문 생성)\nmulti_queries\n\n\n'OpenAI Assistant API의 Functions 기능을 사용하는 방법을 설명해 주세요.  \\nOpenAI Assistant API에서 Functions를 활용하는 방법이 궁금합니다.  \\nOpenAI Assistant API의 Functions를 어떻게 사용할 수 있는지 알려주세요.  \\nFunctions를 사용하여 OpenAI Assistant API를 활용하는 방법에 대해 설명해 주세요.  \\nOpenAI Assistant API의 Functions 사용법에 대한 자세한 정보를 제공해 주세요.  '\n\n\n3.2.3 출력 포맷 및 고려사항\n중요한 프롬프트 지시사항:\nYour response should be a list of values separated by new lines, eg: `foo\\nbar\\nbaz\\n`\n이 지시사항이 중요한 이유:\n- 파싱 용이성: 나중에 list 형식으로 파싱이 용이하게 출력이 되도록 하기 위해서다.\n- 강력한 구분자: 리스트 형식으로 스플릿을 구현할 때 명확한 구분자(new line)를 인식시켜야 오류가 발생하지 않는다.\n- 시스템 안정성: 일관된 출력 형식으로 후속 처리의 안정성을 보장한다.\n\n\n3.2.4 Custom Chain을 MultiQueryRetriever에 적용\n이전에 생성한 Chain을 MultiQueryRetriever에 전달하여 검색을 수행할 수 있다.\n\n\n코드\nmultiquery_retriever = MultiQueryRetriever.from_llm(\n    llm=custom_multiquery_chain, retriever=db.as_retriever()\n)\n\n\nMultiQueryRetriever를 사용하여 문서를 검색하고 결과를 확인한다.\n\n\n코드\n# 결과\nrelevant_docs = multiquery_retriever.invoke(question)\n\n# 검색된 고유한 문서의 개수를 반환한다.\nprint(\n    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n    end=\"\\n===============\\n\",\n)\n\n# 검색된 문서의 내용을 출력한다.\nprint(relevant_docs[0].page_content)\n\n\nINFO:langchain.retrievers.multi_query:Generated queries: ['OpenAI Assistant API의 Functions 사용법에 대해 설명해 주세요.  ', 'OpenAI Assistant API에서 Functions를 어떻게 활용할 수 있나요?  ', 'OpenAI Assistant API의 Functions 기능에 대한 정보를 제공해 주세요.  ', 'Functions를 사용하여 OpenAI Assistant API를 어떻게 사용할 수 있는지 알려주세요.  ', 'OpenAI Assistant API의 Functions 사용법에 대한 자세한 내용을 알고 싶습니다.']\n===============\n검색된 문서 개수: 5\n===============\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.\n==============="
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#실험-결과-비교",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#실험-결과-비교",
    "title": "MultiQueryRetriever",
    "section": "4.1 실험 결과 비교",
    "text": "4.1 실험 결과 비교\n\n4.1.1 단일 쿼리 vs 다중 쿼리 비교\n\n\n\n\n\n\n\n\n\n\n방식\n생성된 쿼리 수\n검색된 문서 수\n장점\n단점\n\n\n\n\n단일 쿼리\n1개\n4개\n빠른 처리, 간단한 구조\n제한된 검색 범위\n\n\n기본 MultiQuery\n3개\n5개\n자동 쿼리 확장\n제한된 커스터마이징\n\n\nCustom MultiQuery\n5개\n5개\n높은 커스터마이징, 더 다양한 관점\n복잡한 설정\n\n\n\n\n\n4.1.2 주요 개선 효과\n1. 검색 범위 확장\n- 단일 쿼리: 4개 문서 → 다중 쿼리: 5개 문서 (25% 증가)\n- 더 포괄적인 정보 수집 가능\n2. 쿼리 다양성 향상\n원본: \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n\n생성된 다양한 관점:\n1. \"기능을 사용하는 방법을 설명해 주세요\"\n2. \"Functions를 활용하는 방법이 궁금합니다\"  \n3. \"Functions를 어떻게 사용할 수 있는지 알려주세요\"\n4. \"Functions를 사용하여 API를 활용하는 방법\"\n5. \"Functions 사용법에 대한 자세한 정보\"\n3. 검색 품질 개선\n- 다양한 표현 방식으로 누락될 수 있는 관련 문서까지 포괄\n- 사용자 의도를 더 정확하게 해석하여 검색"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#실무-적용-가이드",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#실무-적용-가이드",
    "title": "MultiQueryRetriever",
    "section": "4.2 실무 적용 가이드",
    "text": "4.2 실무 적용 가이드\n\n4.2.1 언제 사용할까?\n✅ MultiQueryRetriever 사용 권장 상황\n1. 복잡하거나 모호한 쿼리: 사용자 질문이 불명확하거나 여러 해석이 가능한 경우\n2. 포괄적 검색이 필요: 관련된 모든 정보를 놓치지 않고 수집해야 하는 경우\n3. 다양한 표현의 문서: 동일한 개념이 여러 방식으로 표현된 문서들이 있는 경우\n4. 사용자 경험 개선: 사용자가 정확한 키워드를 모르는 상황에서도 좋은 결과를 제공해야 하는 경우\n❌ 일반 Retriever 사용 권장 상황\n1. 명확하고 구체적인 쿼리: 이미 정확한 키워드나 구문이 포함된 경우\n2. 빠른 응답이 중요: 실시간 응답이 중요하고 약간의 정확도 손실을 감수할 수 있는 경우\n3. 제한된 리소스: LLM API 호출 비용을 최소화해야 하는 경우\n4. 단순한 FAQ: 간단한 질문-답변 시스템에서 과도한 기능일 수 있는 경우\n\n\n4.2.2 최적화 전략\n1. 쿼리 생성 수 조정\n# 간단한 질문: 3개\n# 복잡한 질문: 5-7개\n# 매우 복잡한 분석: 10개까지\n\nprompt = PromptTemplate.from_template(\n    \"\"\"Generate {num_queries} different versions of the question:\n    {question}\"\"\"\n)\n2. 도메인별 프롬프트 최적화\n# 기술 문서용\ntech_prompt = \"\"\"Generate technical variations focusing on:\n- Implementation details\n- API usage patterns  \n- Code examples\n- Troubleshooting scenarios\"\"\"\n\n# 비즈니스 문서용  \nbusiness_prompt = \"\"\"Generate business-oriented variations focusing on:\n- Use cases and applications\n- Benefits and advantages\n- Cost and ROI considerations\n- Best practices\"\"\"\n3. 성능 모니터링\n# 검색 결과 품질 추적\ndef evaluate_retrieval_quality(original_query, generated_queries, results):\n    return {\n        'query_diversity': calculate_diversity(generated_queries),\n        'result_coverage': len(set(results)),\n        'relevance_score': calculate_relevance(original_query, results)\n    }"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#핵심-요약",
    "href": "docs/blog/posts/RAG/10-Retriever/06-MultiQueryRetriever.html#핵심-요약",
    "title": "MultiQueryRetriever",
    "section": "4.3 핵심 요약",
    "text": "4.3 핵심 요약\n\n4.3.1 MultiQueryRetriever의 핵심 가치\n“하나로 질문하고, 여러 관점으로 검색한다”\n\n자동 쿼리 확장: 사용자의 단일 질문을 다양한 관점의 여러 질문으로 자동 변환\n\n검색 범위 확대: 단일 쿼리로는 놓칠 수 있는 관련 문서들까지 포괄적으로 수집\n\n사용자 경험 향상: 불완전하거나 모호한 질문도 의도를 파악하여 적절한 답변 제공\n\n유연한 커스터마이징: LCEL Chain을 통해 도메인별, 용도별 최적화 가능\n\n\n\n4.3.2 주요 장점\n\n검색 정확도 향상: 다양한 관점에서의 접근으로 누락 정보 최소화\n\n사용성 개선: 사용자가 정확한 키워드를 몰라도 좋은 결과 제공\n\n자동화: 프롬프트 엔지니어링 과정을 LLM이 자동으로 수행\n\n확장성: 기존 Retriever 위에 간단히 추가하여 성능 향상 가능\n\n\n\n4.3.3 실무 적용 포인트\n\n적정 쿼리 수: 3-5개 (복잡도에 따라 조정)\n\n비용 대비 효과: LLM 호출 비용 증가 vs 검색 품질 향상의 트레이드오프 고려\n\n도메인 특화: 업계별, 용도별 프롬프트 템플릿 개발로 효과 극대화\n\n성능 모니터링: 검색 결과의 다양성과 관련성을 지속적으로 평가\n\nMultiQueryRetriever는 RAG 시스템의 검색 단계를 크게 개선할 수 있는 강력한 도구이다. 특히 사용자의 질문이 불명확하거나 포괄적인 정보 수집이 필요한 상황에서 그 가치를 발휘한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "",
    "text": "TimeWeightedVectorStoreRetriever는 의미론적 유사성(semantic similarity)과 시간에 따른 감쇠(time decay)를 결합하여 사용하는 검색 도구이다. 문서의 관련성(relevance)과 신선도(freshness)를 동시에 평가하여 가장 적합한 결과를 제공한다.\n\n\n\n\n뉴스 검색: 유사한 주제의 기사 중 최신 기사를 우선 검색\n\n규정집/법률 문서: 개정된 최신 조항을 우선 반환\n\n기술 문서: 업데이트된 최신 버전의 문서를 우선 검색\n\n고객 문의 응대: 최근 자주 참조되는 FAQ를 우선 제공\n\n\n\n\n검색 점수는 다음 공식으로 계산된다:\n\\[\\text{score} = \\text{semantic\\_similarity} + (1.0 - \\text{decay\\_rate})^{\\text{hours\\_passed}}\\]\n수식 구성 요소\n\nsemantic_similarity: 쿼리와 문서 간의 의미적 유사도 (벡터 임베딩 기반 코사인 유사도)\n\ndecay_rate: 시간 감쇠율 (0 ~ 1 사이의 값, 높을수록 과거 문서에 큰 페널티)\n\nhours_passed: 마지막 접근 이후 경과한 시간 (시간 단위)\n\ndecay_rate 설정 효과\n\n높은 값 (0.999): 과거 문서에 큰 페널티 → 최신 문서 우선 검색 (뉴스, 실시간 정보)\n\n낮은 값 (0.0001): 시간 페널티 최소화 → 의미적 유사도 우선 (지식 베이스, 영구적 문서)\n\n중간 값 (0.5): 최신성과 관련성의 균형 (일반적인 문서 검색)\n\n\n\n\n1. 마지막 접근 시간 기준\n문서 생성 시점이 아닌 마지막으로 접근된 시점을 기준으로 신선도를 평가한다. 자주 참조되는 문서는 계속 “최신” 상태를 유지하므로, 실제로 중요하고 유용한 정보가 상위에 노출된다.\n2. 동적 가중치 조정\n시간이 지남에 따라 자동으로 가중치가 감소하여, 별도의 재인덱싱 없이도 검색 결과가 최신 트렌드를 반영한다.\n3. 의미적 유사도와의 조합\n단순히 최신 문서만 반환하는 것이 아니라, 쿼리와의 관련성도 함께 고려하여 균형잡힌 검색 결과를 제공한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#timeweightedvectorstoreretriever-개요",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#timeweightedvectorstoreretriever-개요",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "",
    "text": "TimeWeightedVectorStoreRetriever는 의미론적 유사성(semantic similarity)과 시간에 따른 감쇠(time decay)를 결합하여 사용하는 검색 도구이다. 문서의 관련성(relevance)과 신선도(freshness)를 동시에 평가하여 가장 적합한 결과를 제공한다.\n\n\n\n\n뉴스 검색: 유사한 주제의 기사 중 최신 기사를 우선 검색\n\n규정집/법률 문서: 개정된 최신 조항을 우선 반환\n\n기술 문서: 업데이트된 최신 버전의 문서를 우선 검색\n\n고객 문의 응대: 최근 자주 참조되는 FAQ를 우선 제공\n\n\n\n\n검색 점수는 다음 공식으로 계산된다:\n\\[\\text{score} = \\text{semantic\\_similarity} + (1.0 - \\text{decay\\_rate})^{\\text{hours\\_passed}}\\]\n수식 구성 요소\n\nsemantic_similarity: 쿼리와 문서 간의 의미적 유사도 (벡터 임베딩 기반 코사인 유사도)\n\ndecay_rate: 시간 감쇠율 (0 ~ 1 사이의 값, 높을수록 과거 문서에 큰 페널티)\n\nhours_passed: 마지막 접근 이후 경과한 시간 (시간 단위)\n\ndecay_rate 설정 효과\n\n높은 값 (0.999): 과거 문서에 큰 페널티 → 최신 문서 우선 검색 (뉴스, 실시간 정보)\n\n낮은 값 (0.0001): 시간 페널티 최소화 → 의미적 유사도 우선 (지식 베이스, 영구적 문서)\n\n중간 값 (0.5): 최신성과 관련성의 균형 (일반적인 문서 검색)\n\n\n\n\n1. 마지막 접근 시간 기준\n문서 생성 시점이 아닌 마지막으로 접근된 시점을 기준으로 신선도를 평가한다. 자주 참조되는 문서는 계속 “최신” 상태를 유지하므로, 실제로 중요하고 유용한 정보가 상위에 노출된다.\n2. 동적 가중치 조정\n시간이 지남에 따라 자동으로 가중치가 감소하여, 별도의 재인덱싱 없이도 검색 결과가 최신 트렌드를 반영한다.\n3. 의미적 유사도와의 조합\n단순히 최신 문서만 반환하는 것이 아니라, 쿼리와의 관련성도 함께 고려하여 균형잡힌 검색 결과를 제공한다."
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#환경-설정",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#환경-설정",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "2 환경 설정",
    "text": "2 환경 설정\n\n2.1 API 키 로드\n\n\n코드\n# API 키를 환경변수로 관리하기 위한 설정 파일  \nfrom dotenv import load_dotenv  \n\n# API 키 정보 로드  \nload_dotenv()  \n\n\n\n\n2.2 LangSmith 추적 설정\nLangSmith를 활용하면 시간 가중치 계산 과정과 검색 점수 변화를 시각적으로 추적할 수 있다.\n\n\n코드\n# LangSmith 추적 설정 (https://smith.langchain.com)  \n# !pip install langchain-teddynote  \nfrom langchain_teddynote import logging  \n\n# 프로젝트 이름 입력  \nlogging.langsmith(\"CH10-Retriever\")"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#낮은-감쇠율-의미적-유사도-우선",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#낮은-감쇠율-의미적-유사도-우선",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "3 낮은 감쇠율 (의미적 유사도 우선)",
    "text": "3 낮은 감쇠율 (의미적 유사도 우선)\n\n3.1 개념 설명\n낮은 decay_rate (0에 가까운 값)는 시간이 지나도 문서의 점수가 거의 감소하지 않음을 의미한다. 즉, 시간보다 의미적 유사도가 검색 결과에 더 큰 영향을 미친다.\n특징\n\ndecay_rate = 0: 시간 감쇠 없음 → 일반적인 벡터 유사도 검색과 동일\n\ndecay_rate ≈ 0.0000001: 거의 감쇠 없음 → 과거 문서도 유사도가 높으면 상위 노출\n\n적용 시나리오\n\n시간에 구애받지 않는 지식 베이스 (수학 공식, 역사적 사실)\n\n영구적인 참조 문서 (API 문서, 기술 사양서)\n\n오래된 문서도 여전히 가치 있는 경우\n\n\n\n3.2 Retriever 초기화\n벡터 저장소와 매우 낮은 감쇠율을 설정하여 retriever를 생성한다.\n\n\n코드\nfrom datetime import datetime, timedelta  \n\nimport faiss  \nfrom langchain.docstore import InMemoryDocstore  \nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever  \nfrom langchain_community.vectorstores import FAISS  \nfrom langchain_core.documents import Document  \nfrom langchain_openai import OpenAIEmbeddings  \n\n# 임베딩 모델을 정의합니다.  \nembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n\n# 벡터 저장소를 빈 상태로 초기화합니다.  \nembedding_size = 1536  \nindex = faiss.IndexFlatL2(embedding_size)  \nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})  \n\n# 시간 가중치가 적용된 벡터 저장소 검색기를 초기화합니다. (여기서는, 낮은 감쇠율을 적용합니다)  \nretriever = TimeWeightedVectorStoreRetriever(  \n    vectorstore=vectorstore, decay_rate=0.0000000000000000000000001, k=1  \n)  \n\n\n\n\n3.3 샘플 데이터 추가\n두 개의 문서를 추가한다. 첫 번째 문서는 어제 작성된 것으로, 두 번째 문서는 방금 작성된 것으로 설정한다.\n\n\n코드\n# 어제 날짜를 계산  \nyesterday = datetime.now() - timedelta(days=1)  \n\nretriever.add_documents(  \n    # 문서를 추가하고, metadata에 어제 날짜를 설정  \n    [  \n        Document(  \n            page_content=\"테디노트 구독해 주세요.\",  \n            metadata={\"last_accessed_at\": yesterday},  \n        )  \n    ]  \n)  \n\n# 다른 문서를 추가 (metadata는 별도로 설정하지 않음 = 현재 시간으로 자동 설정)  \nretriever.add_documents([Document(page_content=\"테디노트 구독 해주실꺼죠? Please!\")])  \n\n\n['a6c732c4-adb2-45d1-bcbb-a5108a9778f7']  \n\n\n3.4 검색 결과 확인\n\ndecay_rate가 거의 0에 가까우므로, 시간 페널티가 거의 없어 의미적으로 더 유사한 문서가 먼저 반환된다.\n\n과거 문서가 최신 문서로 계산되는 효과가 있다\n\n\n\n\n코드\n# \"테디노트 구독해 주세요.\"가 먼저 반환됨  \n# 이유: 감쇠율이 0에 가까워 시간 페널티가 거의 없고,   \n#       의미적 유사도가 더 높기 때문  \nretriever.invoke(\"테디노트\")  \n\n\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 8, 30, 22, 1, 49, 841379), 'created_at': datetime.datetime(2024, 8, 30, 22, 1, 44, 410635), 'buffer_idx': 0}, page_content='테디노트 구독해 주세요.')]  \n결과 분석\n\n어제 작성된 문서가 반환되는 이유: decay_rate ≈ 0이므로 24시간이 지나도 점수 감소가 미미함\n\n시간보다 콘텐츠의 관련성이 우선시됨"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#높은-감쇠율-최신성-우선",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#높은-감쇠율-최신성-우선",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "4 높은 감쇠율 (최신성 우선)",
    "text": "4 높은 감쇠율 (최신성 우선)\n\n4.1 개념 설명\n높은 decay_rate (1에 가까운 값)는 시간이 지남에 따라 문서의 점수가 급격히 감소함을 의미한다. 즉, 최신 문서가 검색 결과에 더 큰 영향을 미친다.\n특징\n\ndecay_rate = 1: 모든 과거 문서의 시간 점수가 0 → 최신 문서만 반환\n\ndecay_rate ≈ 0.999: 시간이 조금만 지나도 점수가 급격히 감소 → 최신성이 매우 중요\n\n적용 시나리오\n\n뉴스 기사 검색 (최신 뉴스 우선)\n\n실시간 정보 (주식 시세, 날씨)\n\n자주 업데이트되는 문서 (소프트웨어 릴리즈 노트)\n\n시간에 민감한 정보 (이벤트 공지, 할인 정보)\n\n\n\n4.2 Retriever 초기화\n벡터 저장소와 높은 감쇠율을 설정하여 retriever를 생성한다.\n\n\n코드\n# 임베딩 모델을 정의  \nembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n\n# 벡터 저장소를 빈 상태로 초기화  \nembedding_size = 1536  \nindex = faiss.IndexFlatL2(embedding_size)  \nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})  \n\n# 시간 가중치가 적용된 벡터 저장소 검색기를 초기화 (높은 감쇠율 적용)  \nretriever = TimeWeightedVectorStoreRetriever(  \n    vectorstore=vectorstore, decay_rate=0.999, k=1  \n)  \n\n\n\n\n4.3 샘플 데이터 추가\n동일한 두 개의 문서를 추가한다. 첫 번째는 어제 작성, 두 번째는 방금 작성으로 설정한다.\n\n\n코드\n# 어제 날짜를 계산  \nyesterday = datetime.now() - timedelta(days=1)  \n\nretriever.add_documents(  \n    # 문서를 추가하고, metadata에 어제 날짜를 설정  \n    [  \n        Document(  \n            page_content=\"테디노트 구독해 주세요.\",  \n            metadata={\"last_accessed_at\": yesterday},  \n        )  \n    ]  \n)  \n\n# 다른 문서를 추가 (metadata는 별도로 설정하지 않음)  \nretriever.add_documents([Document(page_content=\"테디노트 구독 해주실꺼죠? Please!\")])  \n\n\n['c3349ba9-75c7-49ec-be7a-017bc0917fa2']  \n\n\n4.4 검색 결과 확인\ndecay_rate가 0.999로 높으므로, 24시간이 지난 문서는 시간 페널티를 크게 받아 최신 문서가 먼저 반환된다.\n\n\n코드\n# \"테디노트 구독 해주실꺼죠? Please!\"가 먼저 반환됨  \n# 이유: 감쇠율이 0.999로 높아서 어제 작성된 문서는   \n#       시간 페널티를 크게 받아 점수가 급격히 하락함  \nretriever.invoke(\"테디노트\")  \n\n\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 8, 30, 22, 3, 18, 331780), 'created_at': datetime.datetime(2024, 8, 30, 22, 2, 44, 618745), 'buffer_idx': 1}, page_content='테디노트 구독 해주실꺼죠? Please!')]  \n결과 분석\n\n방금 작성된 문서가 반환되는 이유: 높은 decay_rate로 인해 24시간 지난 문서의 시간 점수가 거의 0에 수렴\n\n의미적 유사도가 낮더라도 최신성이 우선시됨\n\n실제 계산: \\((1.0 - 0.999)^{24} ≈ 0\\) (24시간 경과 시 시간 점수가 거의 0)"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#decay_rate-설정-가이드",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#decay_rate-설정-가이드",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "5 decay_rate 설정 가이드",
    "text": "5 decay_rate 설정 가이드\n\n5.1 값에 따른 동작 비교\n\n\n\n\n\n\n\n\n\ndecay_rate\n시간 감쇠 속도\n우선순위\n적합한 사용 사례\n\n\n\n\n0.0 ~ 0.1\n매우 느림\n의미적 유사도 우선\n지식 베이스, 영구 문서, 역사적 자료\n\n\n0.1 ~ 0.5\n느림\n유사도와 최신성 균형\n일반 문서 검색, FAQ\n\n\n0.5 ~ 0.9\n빠름\n최신성 우선\n업데이트되는 가이드, 제품 정보\n\n\n0.9 ~ 0.999\n매우 빠름\n최신성 강력 우선\n뉴스, 실시간 정보\n\n\n1.0\n즉시 감쇠\n최신 문서만\n실시간 데이터 스트림\n\n\n\n\n\n5.2 설정 원칙\n낮은 decay_rate (0 ~ 0.1)\n\n정보를 거의 “잊지 않음”\n\n시간이 지나도 점수 변화가 미미함\n\n의미적 유사도가 검색의 주요 기준\n\n예: 수학 공식, API 문서, 기술 사양\n\n중간 decay_rate (0.1 ~ 0.5)\n\n적절한 시간 감쇠 적용\n\n최신성과 관련성의 균형\n\n주기적으로 업데이트되지만 과거 정보도 유효한 경우\n\n예: 블로그 포스트, 제품 리뷰, 사용자 가이드\n\n높은 decay_rate (0.9 ~ 0.999)\n\n과거 정보를 빠르게 “잊음”\n\n최신 정보에 압도적으로 높은 점수\n\n시간에 민감한 정보 검색\n\n예: 뉴스 기사, 실시간 알림, 이벤트 공지\n\n\n\n5.3 실무 권장값\n# 지식 베이스 (변하지 않는 정보)  \ndecay_rate = 0.01  \n\n# 일반 문서 검색 (균형)  \ndecay_rate = 0.3  \n\n# 뉴스/실시간 정보 (최신성 중시)  \ndecay_rate = 0.95"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#고급-가상-시간을-이용한-테스트",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#고급-가상-시간을-이용한-테스트",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "6 고급: 가상 시간을 이용한 테스트",
    "text": "6 고급: 가상 시간을 이용한 테스트\n\n6.1 개념 설명\nLangChain의 mock_now 유틸리티를 사용하면 현재 시간을 임의로 설정하여 시간 감쇠 효과를 시뮬레이션할 수 있다. 이를 통해 실제로 시간이 경과하지 않아도 다양한 시간대에서의 검색 동작을 테스트할 수 있다.\n활용 목적\n\n최적의 decay_rate 값 탐색\n\n시간 경과에 따른 검색 결과 변화 예측\n\n프로덕션 배포 전 시간 기반 로직 검증\n\n과거 시점의 검색 결과 재현\n\n\n\n6.2 mock_now 사용법\n\n\n코드\nimport datetime  \n\nfrom langchain.utils import mock_now  \n\n# 현재 시간을 특정 시점으로 설정  \nmock_now(datetime.datetime(2024, 8, 30, 00, 00))  \n\n# 현재 시간 출력  \nprint(datetime.datetime.now())  \n\n\n2024-08-30 22:05:01.844175  \n\n\n6.3 시간 변경 테스트\n다양한 시간대에서 검색 결과가 어떻게 달라지는지 확인하여 적절한 decay_rate를 찾을 수 있다.\n\n\n코드\n# 과거 시점 (2024년 8월 29일)으로 설정하여 검색  \nwith mock_now(datetime.datetime(2024, 8, 29, 00, 00)):  \n    # 해당 시점에서의 검색 결과 확인  \n    print(retriever.invoke(\"테디노트\"))  \n\n\n주의사항\n\n너무 오래된 시간으로 설정하면 decay_rate 계산 시 오류가 발생할 수 있음 (지수 연산 오버플로우)\n\n일반적으로 수년 이내의 시간 범위에서 테스트 권장\n\n\n\n6.4 decay_rate 튜닝 예제\n다양한 시간 간격에서 검색 결과를 테스트하여 최적의 decay_rate를 찾는다.\n\n\n코드\n# 다양한 시간대 테스트  \ntest_times = [  \n    (\"1시간 후\", datetime.timedelta(hours=1)),  \n    (\"1일 후\", datetime.timedelta(days=1)),  \n    (\"1주일 후\", datetime.timedelta(weeks=1)),  \n    (\"1개월 후\", datetime.timedelta(days=30)),  \n]  \n\nfor label, delta in test_times:  \n    future_time = datetime.datetime.now() + delta  \n    with mock_now(future_time):  \n        result = retriever.invoke(\"테디노트\")  \n        print(f\"{label}: {result[0].page_content}\")  \n\n\n[Document(metadata={'last_accessed_at': MockDateTime(2024, 8, 29, 0, 0), 'created_at': datetime.datetime(2024, 8, 30, 22, 2, 44, 618745), 'buffer_idx': 1}, page_content='테디노트 구독 해주실꺼죠? Please!')]"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#실전-활용-전략",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#실전-활용-전략",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "7 실전 활용 전략",
    "text": "7 실전 활용 전략\n\n7.1 전략 1: 하이브리드 검색 시스템\n서로 다른 decay_rate를 가진 여러 retriever를 조합하여 다양한 사용자 요구를 충족한다.\n# 최신성 우선 retriever  \nrecent_retriever = TimeWeightedVectorStoreRetriever(  \n    vectorstore=vectorstore,  \n    decay_rate=0.95,  # 높은 감쇠율  \n    k=3  \n)  \n\n# 관련성 우선 retriever  \nrelevant_retriever = TimeWeightedVectorStoreRetriever(  \n    vectorstore=vectorstore,  \n    decay_rate=0.1,  # 낮은 감쇠율  \n    k=3  \n)  \n\n# 사용자 쿼리 의도에 따라 선택  \nif \"최신\" in user_query or \"최근\" in user_query:  \n    results = recent_retriever.invoke(user_query)  \nelse:  \n    results = relevant_retriever.invoke(user_query)  \n적용 시나리오: 뉴스 플랫폼에서 “최신 뉴스”는 높은 decay_rate, “관련 뉴스”는 낮은 decay_rate 적용\n\n\n7.2 전략 2: 동적 decay_rate 조정\n문서 유형이나 카테고리에 따라 자동으로 decay_rate를 조정한다.\n# 문서 카테고리별 decay_rate 매핑  \ncategory_decay_rates = {  \n    \"news\": 0.95,        # 뉴스는 최신성 중요  \n    \"tutorial\": 0.2,     # 튜토리얼은 시간 무관  \n    \"api_doc\": 0.05,     # API 문서는 영구적  \n    \"blog\": 0.4,         # 블로그는 중간  \n    \"regulation\": 0.7,   # 규정은 업데이트 빈번  \n}  \n\ndef get_retriever_for_category(category):  \n    decay_rate = category_decay_rates.get(category, 0.3)  \n    return TimeWeightedVectorStoreRetriever(  \n        vectorstore=vectorstore,  \n        decay_rate=decay_rate,  \n        k=5  \n    )  \n적용 시나리오: 통합 문서 관리 시스템에서 문서 카테고리를 자동 감지하여 최적의 검색 설정 적용\n\n\n7.3 전략 3: 주기적 접근 시간 업데이트\n중요한 문서의 last_accessed_at를 주기적으로 업데이트하여 항상 상위에 유지한다.\nimport datetime  \n\n# 중요 문서의 접근 시간 갱신  \ndef refresh_important_docs(important_doc_ids):  \n    for doc_id in important_doc_ids:  \n        # 문서의 메타데이터 업데이트  \n        vectorstore.update_metadata(  \n            doc_id,  \n            {\"last_accessed_at\": datetime.datetime.now()}  \n        )  \n\n# 주기적으로 실행 (예: 매일 자정)  \nrefresh_important_docs([\"faq_main\", \"policy_core\", \"guide_essential\"])  \n적용 시나리오: FAQ 시스템에서 핵심 질문들을 항상 상위에 노출\n\n\n7.4 전략 4: 시간대별 가중치 조정\n업무 시간대와 비업무 시간대에 다른 검색 전략 적용\nimport datetime  \n\ndef get_time_aware_retriever():  \n    current_hour = datetime.datetime.now().hour  \n    \n    # 업무 시간 (9-18시): 최신 문서 우선  \n    if 9 &lt;= current_hour &lt;= 18:  \n        decay_rate = 0.8  \n    # 비업무 시간: 관련성 우선  \n    else:  \n        decay_rate = 0.2  \n    \n    return TimeWeightedVectorStoreRetriever(  \n        vectorstore=vectorstore,  \n        decay_rate=decay_rate,  \n        k=5  \n    )  \n적용 시나리오: 고객 지원 챗봇에서 업무 시간에는 최신 공지사항 우선 제공\n\n\n7.5 전략 5: A/B 테스팅을 통한 최적화\n다양한 decay_rate 값으로 A/B 테스팅을 수행하여 사용자 만족도가 높은 값을 찾는다.\nimport random  \n\n# A/B 테스트 그룹 설정  \ndef get_ab_test_retriever(user_id):  \n    # 사용자 ID 기반 그룹 할당  \n    group = hash(user_id) % 3  \n    \n    decay_rates = {  \n        0: 0.3,   # 그룹 A: 낮은 감쇠  \n        1: 0.6,   # 그룹 B: 중간 감쇠  \n        2: 0.9,   # 그룹 C: 높은 감쇠  \n    }  \n    \n    return TimeWeightedVectorStoreRetriever(  \n        vectorstore=vectorstore,  \n        decay_rate=decay_rates[group],  \n        k=5  \n    )  \n\n# 사용자 피드백 수집 후 최적값 선택  \n적용 시나리오: 검색 서비스에서 클릭률(CTR)이 가장 높은 decay_rate 값 발견\n\n\n7.6 전략 6: 계절성/주기성 고려\n특정 시즌이나 주기에 따라 검색 전략을 조정한다.\nimport datetime  \n\ndef get_seasonal_retriever():  \n    current_month = datetime.datetime.now().month  \n    \n    # 연말(11-12월): 연간 리포트 등 최신 정보 중요  \n    if current_month in [11, 12]:  \n        decay_rate = 0.85  \n    # 연초(1-2월): 과거 데이터 참조 중요  \n    elif current_month in [1, 2]:  \n        decay_rate = 0.3  \n    # 일반 기간  \n    else:  \n        decay_rate = 0.5  \n    \n    return TimeWeightedVectorStoreRetriever(  \n        vectorstore=vectorstore,  \n        decay_rate=decay_rate,  \n        k=5  \n    )  \n적용 시나리오: 재무 문서 검색 시스템에서 분기별/연도별 보고서 우선순위 자동 조정"
  },
  {
    "objectID": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#요약",
    "href": "docs/blog/posts/RAG/10-Retriever/09-TimeWeightedVectorStoreRetriever.html#요약",
    "title": "시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)",
    "section": "8 요약",
    "text": "8 요약\nTimeWeightedVectorStoreRetriever는 의미적 유사도와 시간 기반 신선도를 결합하여 상황에 맞는 최적의 검색 결과를 제공한다. decay_rate 값을 조정하여 최신성과 관련성의 균형을 세밀하게 제어할 수 있으며, 뉴스, 규정집, 기술 문서 등 다양한 도메인에서 활용 가능하다. 실전에서는 문서 특성, 사용자 의도, 시간대 등을 고려한 동적 전략을 수립하여 검색 품질을 극대화할 수 있다.\n\n\n코드\nwith mock_now(datetime.datetime(2024, 8, 29, 00, 00)):  \n    # 해당 시점에서의 검색 결과 확인  \n    print(retriever.invoke(\"테디노트\"))  \n\n\n주의사항\n\n너무 오래된 시간으로 설정하면 decay_rate 계산 시 오류가 발생할 수 있음 (지수 연산 오버플로우)\n\n일반적으로 수년 이내의 시간 범위에서 테스트 권장\n\n\n8.1 decay_rate 튜닝 예제\n다양한 시간 간격에서 검색 결과를 테스트하여 최적의 decay_rate를 찾는다.\n\n\n코드\n# 다양한 시간대 테스트  \ntest_times = [  \n    (\"1시간 후\", datetime.timedelta(hours=1)),  \n    (\"1일 후\", datetime.timedelta(days=1)),  \n    (\"1주일 후\", datetime.timedelta(weeks=1)),  \n    (\"1개월 후\", datetime.timedelta(days=30)),  \n]  \n\nfor label, delta in test_times:  \n    future_time = datetime.datetime.now() + delta  \n    with mock_now(future_time):  \n        result = retriever.invoke(\"테디노트\")  \n        print(f\"{label}: {result[0].page_content}\")"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "대화 분석 네가지 기준\n\n\n\n\n\n분류 기준\n구분\n약어\n\n\n\n\n대화 턴 수(Turn)\n싱글턴 vs 멀티턴\nS : M\n\n\n행위 유형(Action)\n정보 검색 vs 기타 행위\nI : O\n\n\n감정적 태도(Stance)\n감정적 vs 비감정적\nS : N\n\n\n선호 구조(Structure)\n선호 vs 비선호 vs 발견되지 않음\nP : D : U\n\n\n\n\n\n\n\n\n\n사용자 세그먼트 분류\n\n\n프롬프트만으로 사용자의 성향을 예측해야 하는 상황에서 유용하게 활용할 수 있습니다. 이후 해당 세그먼트를 통한 프롬프트 인사이트를 소개할 예정입니다.\n\n\n\n\n\n\n정보 검색형 사용자\n즉흥적으로 문제를 해결하려는 경향\n\n\n\n\n\nAI 친화적이고 사교적인 사용자\n긍정적인 대화를 선호함\n\n\n\n\n\n새로운 아이디어를 고민하는 사용자\n여러 턴의 대화를 통해 새로운 가능성을 탐구하는 유형\n\n\n\n\n\n\nSegment 1 &gt;&gt;&gt;&gt; Segment 2 &gt; Segment 3\n\n정보 검색형 사용자(Segment 1)가 가장 많은 비중을 차지한다.\n\n\n\n\n\n\n프롬프트 → LLM 분류 방식 활용\n사용자 발화 데이터를 세그먼트별로 분류하여 분포 파악 가능\n\n\n\n\n\n현재 서비스의 Heavy User, Loyalty User 파악 가능\n\n\n\n\n\n어떤 프롬프트를 기획할 것인지 아이디에이션에 활용\n\n\n\n\n\n사용자 이탈의 원인을 분석 가능\n세그먼트를 지속적으로 추적하여, 어떤 환경적 요인이 특정 세그먼트의 이탈 또는 유입을 유발하는지 분석 가능"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#대화-분석-네-가지-기준",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#대화-분석-네-가지-기준",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "대화 분석 네가지 기준\n\n\n\n\n\n분류 기준\n구분\n약어\n\n\n\n\n대화 턴 수(Turn)\n싱글턴 vs 멀티턴\nS : M\n\n\n행위 유형(Action)\n정보 검색 vs 기타 행위\nI : O\n\n\n감정적 태도(Stance)\n감정적 vs 비감정적\nS : N\n\n\n선호 구조(Structure)\n선호 vs 비선호 vs 발견되지 않음\nP : D : U"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#사용자-세그먼트-분류",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#사용자-세그먼트-분류",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "사용자 세그먼트 분류\n\n\n프롬프트만으로 사용자의 성향을 예측해야 하는 상황에서 유용하게 활용할 수 있습니다. 이후 해당 세그먼트를 통한 프롬프트 인사이트를 소개할 예정입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#대표적인-3가지-유형",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#대표적인-3가지-유형",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "정보 검색형 사용자\n즉흥적으로 문제를 해결하려는 경향\n\n\n\n\n\nAI 친화적이고 사교적인 사용자\n긍정적인 대화를 선호함\n\n\n\n\n\n새로운 아이디어를 고민하는 사용자\n여러 턴의 대화를 통해 새로운 가능성을 탐구하는 유형"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#일반적인-세그먼트-분포",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#일반적인-세그먼트-분포",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "Segment 1 &gt;&gt;&gt;&gt; Segment 2 &gt; Segment 3\n\n정보 검색형 사용자(Segment 1)가 가장 많은 비중을 차지한다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#세그먼트-활용법",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#세그먼트-활용법",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "",
    "text": "프롬프트 → LLM 분류 방식 활용\n사용자 발화 데이터를 세그먼트별로 분류하여 분포 파악 가능\n\n\n\n\n\n현재 서비스의 Heavy User, Loyalty User 파악 가능\n\n\n\n\n\n어떤 프롬프트를 기획할 것인지 아이디에이션에 활용\n\n\n\n\n\n사용자 이탈의 원인을 분석 가능\n세그먼트를 지속적으로 추적하여, 어떤 환경적 요인이 특정 세그먼트의 이탈 또는 유입을 유발하는지 분석 가능"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#segment-1.-single-turn-information-seeking-si-1",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#segment-1.-single-turn-information-seeking-si-1",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.1 Segment 1. Single-Turn, Information Seeking (SI)",
    "text": "2.1 Segment 1. Single-Turn, Information Seeking (SI)\n\n정보 검색형 사용자 — 즉흥적으로 문제를 해결하려는 경향\n\n\n2.1.1 프롬프트 기획 전략\n\n실시간 정보 검색 엔진 연동 — 정보 검색형 사용자의 니즈 충족\nRetention을 위한 멀티턴 유도 장치 — 이탈 방지\n\n정보 검색형 사용자는 싱글턴으로 질문에 대한 답을 얻고 빠르게 이탈하는 특성이 있다. 이러한 사용자를 잡아두기 위해 동적으로 FAQ를 제공하는 방식으로 프롬프트를 기획하면, 자연스럽게 멀티턴 대화로 유도할 수 있다.\n\n\n2.1.2 예시: 멀티턴 유도 & 사용자 만족을 위한 질문 생성\n\n\n\n멀티턴 유도 & 사용자 만족을 위한 질문 생성\n\n\n\n호기심을 자극하는 장치로 활용\n실제로 클릭율이 매우 높음 → 사용자 만족도 향상에 기여"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#segment-2.-multi-turn-stance-preferred-misp-mosp-1",
    "href": "docs/blog/posts/Prompt_Engineering/03-프롬프트_기획_전략/01-세그먼트별_프롬프트_기획.html#segment-2.-multi-turn-stance-preferred-misp-mosp-1",
    "title": "Part 3 — 세그먼트별 프롬프트 기획",
    "section": "2.2 Segment 2. Multi-Turn, Stance, Preferred (MISP, MOSP)",
    "text": "2.2 Segment 2. Multi-Turn, Stance, Preferred (MISP, MOSP)\n\nAI 친화적이고 사교적인 사용자 — 공감대와 유대감을 중요시하며, 긍정적인 대화를 선호\n\n\n2.2.1 프롬프트 기획 전략\n\nAI와 Affiliation 형성 — Gamification 활용 (게임이 아닌 분야에 게임의 요소(점수, 보상, 경쟁, 레벨업 등)를 적용해 참여·동기·몰입을 높이는 기법)\n페르소나 만들기 — 프롬프트 게임, 캐릭터 챗봇\n\n이 세그먼트는 대화의 길이가 상당히 길고, 하고자 하는 바가 명확한 특성이 있다.\n\n\n2.2.2 예시: 캐릭터 챗봇\n\n\n\n캐릭터챗봇\n\n\n캐릭터 챗봇을 통해 사용자와 유대감을 형성할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "범용성: 다양한 상황에서도 동작, 불특정 다수의 사용자 대상 다양한 상황과 과제에 적용 가능한 프롬프트의 결과를 내야 한다.\n\n목적 지향성: 특정 사용자 목적에 세분화된 기능을 제공해야 한다.\n일관성: 여러가지 LLM 모델을 사용하더라도, 생성 할 때마다 결과가 바뀌면 안된다. 어느정도 스펙트럼 안에서 일정한 결과 유지.\n경제성: 단어, 토큰수 운영 비용 API 비용 절감\n\n\n\n\n\n비계식 설계\n인간의 학습 이론(ZPD)을 적용\n외국어를 학습하는 사람들의 학습 능력을 최대한 끌어올리려면, 최대한 쉬운 단계부터 어려운 단계까지 차츰차츰 비계식으로 올라가야 한다.\n단어 -&gt; 문장 -&gt; 문단 완성을 하는 단계별 학습 방식 사용\n프롬프트 설계를 할떄도 이 방식을 사용하면 정말 잘 설계를 할 수 있다.\nUnderstand → Ideation → Analysis → Apply → Evaluate → Generalize\n풀어야될 문제(요구사항)에 대한 이해\n문제에 대한 이해를 바탕으로 여러가지 프롬프트를 만드는 아이디에이션 진행.\n아이디에이션을 통해 구현된 프롬프트를 분석.\n실제로 도입, 적용, 평가\n일반화 할 수 있는 결과를 얻으면 배포 진행.\n단계적 사고구조 유도\n\n\n\n\n\n\n\n최근까지 GPT 4/5\n최신모델을 사용하면 프롬프트가 조금 엉성해도 좋은 결과를 얻을 수 있다.\n그러나 GPT 3.5을 이용해 A 라는 기능을 4/5 만큼 끌어올릴 수 있다면, 이 방법이 좋은 방법이다.\n\n\n\n\n어떤 단어가 좀더 구체적, 또는 명확하게 표현이 되어있는지 확인 조금더 명확한 동작을 하는 단어가 있을 수 있다. 뭐를 나누다 라는 맥락이 없으면 뜻을 정확하게 이해하기가 어려운 언어이다. 예: 분석해, 정리해, 요약해, 작성해 등 –&gt; 정확하게 무엇을 하려고 하는지 맥락 없어도 이해 가능하다.\n\n\n\n구조화라 함은 어떻게 요소별로 나눈다던지 섹션별로 컴포지션을 나누어서, 언어모델이 읽기 편한 구조를 만든다. 대표적으로 2가지 방법(Markdown, 코드 프롬프트)이 있다. 프롬프트를 구조화하면 LLM이 입력을 더 명확하게 해석할 수 있고,\n작업 목적·규칙·입력 데이터·출력 형식을 혼동 없이 처리할 수 있다.\n구조화를 위한 핵심 기법은 Markdown, 코드 프롬프트, XML 태그이다.\n\nMarkdown 기호 사용하기 (Headers, Lists, Emphasis)\n\nMarkdown은 LLM이 가장 잘 해석하는 구조적 문법 중 하나이다.\n섹션을 명확히 나누고, 각 부분을 시각적으로 구분해줌으로써\n모델의 오류를 줄이고 반복 작업 시 유지보수성이 높아진다.\n1.1 헤더(Headers) 활용 헤더는 프롬프트의 구성을 크게 나누는 데 사용한다. 샾 개수만큼 주 제목, 부제목, 소제목 등의 계층적 우선순위 인덱스를 부여할 수 있다.\n# 역할(Role)\n너는 스포츠 기사 분석 엔진이다.\n\n## 작업(Task)\n아래 텍스트에서 장소(도시, 국가, 축구 클럽)를 모두 추출해라.\n\n## 출력 형식(Format)\n- \"장소: A, B, C\" 형태의 한 줄 출력\n- 설명 금지\n\n## 텍스트(Text)\n{{TEXT}}\n1.2 목록(Lists) 활용 하이픈이나, 번호를 이용 –&gt; 수행할 순서가 있는 프롬프트에 사용한다. 절차, 규칙, 조건을 번호 목록으로 정리하면 LLM이 하나씩 적용하기 쉽다.\n다음 규칙을 따라 작업을 수행해라.\n1. 사람 이름은 제외한다.\n2. 축구 클럽은 장소로 포함한다.\n3. 결과는 쉼표로 구분한다.\n4. 설명 문장은 쓰지 않는다.\n1.3 강조(Emphasis) 사용\n중요 조건을 굵게 또는 기울임으로 표시해 모델의 우선순위를 높인다. 실제로 이탤릭, 볼드체로 표현은 되지 않는다. 언어모델에게 일반 줄글의 텍스트가 아닌 다른 요소임을 강조.\n- 반드시 **한국어**로 작성하라.\n- 출력은 **한 줄**로만 작성한다.\n- *설명 문장은 포함하지 않는다.*\n\n코드 프롬프트(Code Prompting)\n\n구조화를 좀더 구체적으로 하는 방식에 대해서 소개. AdaPlanner 논순 발췌, 구글의 리서처 분들이 많이 사용한다. 파이썬 언어를 사용해서 언어 모델이 정확히 수행 해야 하는 내용을 작성. 자연어 대신 코드 형식으로 의도를 명세하면 애매함이 줄어들고 LLM이 구조적으로 이해해 할루시네이션이 감소한다.\n# 역할 정의\nrole = \"정보 추출 엔진\"\n\n# 작업 정의\ntask = \"텍스트에서 '장소명'을 추출해 정렬된 리스트로 반환\"\n\n# 출력 형식 정의\noutput = {\n    \"places\": [\"string\"]\n}\n코드 방식의 장점:\n\n논리 구조가 명확하게 표현됨\n조건과 출력 형식이 혼동되지 않음\n정형화된 포맷을 자주 재사용 가능\n\n\nXML 태그 기반 구조화 (Claude 최적화)\n\n엔트로픽 프롬팅 쿡북 참조하면 나오는 내용. 문장 내 구역 나눔, 해당 구적을 강조하는 효과, 반복작업에서 프롬프트 수정과 편집/용이, 언어모델의 처리 속도 향상 XML 태그는 텍스트 내 특정 영역을 명확히 구분해 Claude뿐 아니라 대부분의 LLM에서 구조적 파싱을 돕는 강력한 방식이다.\n이 방식의 장점은 탬플릿이 가능하다.\n\n태그로 영역 지정\n\n&lt;email&gt;\nShow up at 6AM because I say so.\n&lt;/email&gt;\n\nMake this email more polite.\nLLM은 &lt;email&gt;...&lt;/email&gt; 내부 텍스트만 변환하면 된다는 것을 명확히 인식한다.\n\n데이터 템플릿 예시\n\n&lt;animal&gt;{{ANIMAL}}&lt;/animal&gt;\n입력만 바꿔 여러 데이터를 동일한 템플릿에 적용할 수 있다.\n예:\n&lt;animal&gt;Cow&lt;/animal&gt;\n&lt;animal&gt;Dog&lt;/animal&gt;\n&lt;animal&gt;Seal&lt;/animal&gt;\n장점:\n\n구역별 의미가 명확\n반복 작업에서 수정이 쉽고 안정적\n모델이 태그 단위로 파싱하기 때문에 오류 감소\n\n\n구조화 기법이 필요한 이유\n\n\n명령어, 입력 데이터, 출력 형식을 명확히 구분해 모델의 해석 오류를 줄인다.\n규칙을 시각적으로 구분하여 작업 수행 품질의 일관성과 재현성을 높인다.\n재사용 가능한 템플릿이 되어 프롬프트 제작 비용과 시간 절감에 기여한다.\n\n\n\n\n\n한국어는 짧고 명확하게 작성\n톤·형식·분량을 구체적으로 지시\n\n- 나쁜 예\n5월의 봄을 주제로 시를 써주고 짧게\n작성해줘 봄의 느낌을 살려봐.\n결과: 오월의 푸른 하늘 아래\n새싹이 희망을 노래하네.\n아침 이슬에 빛나는 꽃들,\n봄바람 따라 춤추는 나비.\n모든 거리는 축제가 되어.\n\n- 좋은 예\n너는 시인이야. 한국의 계절, 봄을 상상해봐.\n5월이야. 다양한 꽃이 여기저기 폈어.\n그 분위기에 어울리는 짧은 시를 5줄 이내로\n작성해줘. 한글 고유의 멋이 시에 드러나면 좋겠어.\n결과: 봄바람, 너른 들녘을 스치며\n꽃잎들 춤추듯 흩날리네\n산들산들 부는 바람에\n눈부신 햇살 아래\n은은한 꽃 향기에 취하노라\n\n\n\n\n부정 지시보다 긍정적 행동 지시가 안정적 구체적인 내용을 단문 형식으로 제공함과 언어모델에게 추측을 해야 하는 여지를 주면 안된다.\n\n- 나쁜 예\n다음은 에이전트와 고객의 대화이다.\n고객에게 아이디와 비밀번호를 묻지\n않는다. 반복하지 않는다.\n\n- 좋은 예\n다음은 에이전트와 고객과의 대화내용이다.\n챗봇은 고객의 대화에서 드러난 문제를 진단하고 해결책을\n제시해야 한다. 사용자 아이디와 비밀번호같은 개인정보를\n묻지 않는 대신,\n사용자가 www.samplewebsite.com/help/faq에서 관련\n사항을 찾도록 안내해라.\n\n\n\n\n\n절대 나와서는 안되는 조건을 적용하기\n되도록 LLM 이 추론을 할 수 없게 구체적으로 주기\n긍정문을 쓰되 절대 나와서 안되는 문구들은 2중 부정을 사용"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#대-원칙",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#대-원칙",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "범용성: 다양한 상황에서도 동작, 불특정 다수의 사용자 대상 다양한 상황과 과제에 적용 가능한 프롬프트의 결과를 내야 한다.\n\n목적 지향성: 특정 사용자 목적에 세분화된 기능을 제공해야 한다.\n일관성: 여러가지 LLM 모델을 사용하더라도, 생성 할 때마다 결과가 바뀌면 안된다. 어느정도 스펙트럼 안에서 일정한 결과 유지.\n경제성: 단어, 토큰수 운영 비용 API 비용 절감"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#scaffolding-prompting",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#scaffolding-prompting",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "비계식 설계\n인간의 학습 이론(ZPD)을 적용\n외국어를 학습하는 사람들의 학습 능력을 최대한 끌어올리려면, 최대한 쉬운 단계부터 어려운 단계까지 차츰차츰 비계식으로 올라가야 한다.\n단어 -&gt; 문장 -&gt; 문단 완성을 하는 단계별 학습 방식 사용\n프롬프트 설계를 할떄도 이 방식을 사용하면 정말 잘 설계를 할 수 있다.\nUnderstand → Ideation → Analysis → Apply → Evaluate → Generalize\n풀어야될 문제(요구사항)에 대한 이해\n문제에 대한 이해를 바탕으로 여러가지 프롬프트를 만드는 아이디에이션 진행.\n아이디에이션을 통해 구현된 프롬프트를 분석.\n실제로 도입, 적용, 평가\n일반화 할 수 있는 결과를 얻으면 배포 진행.\n단계적 사고구조 유도"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#핵심-5-tips",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#핵심-5-tips",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "최근까지 GPT 4/5\n최신모델을 사용하면 프롬프트가 조금 엉성해도 좋은 결과를 얻을 수 있다.\n그러나 GPT 3.5을 이용해 A 라는 기능을 4/5 만큼 끌어올릴 수 있다면, 이 방법이 좋은 방법이다.\n\n\n\n\n어떤 단어가 좀더 구체적, 또는 명확하게 표현이 되어있는지 확인 조금더 명확한 동작을 하는 단어가 있을 수 있다. 뭐를 나누다 라는 맥락이 없으면 뜻을 정확하게 이해하기가 어려운 언어이다. 예: 분석해, 정리해, 요약해, 작성해 등 –&gt; 정확하게 무엇을 하려고 하는지 맥락 없어도 이해 가능하다.\n\n\n\n구조화라 함은 어떻게 요소별로 나눈다던지 섹션별로 컴포지션을 나누어서, 언어모델이 읽기 편한 구조를 만든다. 대표적으로 2가지 방법(Markdown, 코드 프롬프트)이 있다. 프롬프트를 구조화하면 LLM이 입력을 더 명확하게 해석할 수 있고,\n작업 목적·규칙·입력 데이터·출력 형식을 혼동 없이 처리할 수 있다.\n구조화를 위한 핵심 기법은 Markdown, 코드 프롬프트, XML 태그이다.\n\nMarkdown 기호 사용하기 (Headers, Lists, Emphasis)\n\nMarkdown은 LLM이 가장 잘 해석하는 구조적 문법 중 하나이다.\n섹션을 명확히 나누고, 각 부분을 시각적으로 구분해줌으로써\n모델의 오류를 줄이고 반복 작업 시 유지보수성이 높아진다.\n1.1 헤더(Headers) 활용 헤더는 프롬프트의 구성을 크게 나누는 데 사용한다. 샾 개수만큼 주 제목, 부제목, 소제목 등의 계층적 우선순위 인덱스를 부여할 수 있다.\n# 역할(Role)\n너는 스포츠 기사 분석 엔진이다.\n\n## 작업(Task)\n아래 텍스트에서 장소(도시, 국가, 축구 클럽)를 모두 추출해라.\n\n## 출력 형식(Format)\n- \"장소: A, B, C\" 형태의 한 줄 출력\n- 설명 금지\n\n## 텍스트(Text)\n{{TEXT}}\n1.2 목록(Lists) 활용 하이픈이나, 번호를 이용 –&gt; 수행할 순서가 있는 프롬프트에 사용한다. 절차, 규칙, 조건을 번호 목록으로 정리하면 LLM이 하나씩 적용하기 쉽다.\n다음 규칙을 따라 작업을 수행해라.\n1. 사람 이름은 제외한다.\n2. 축구 클럽은 장소로 포함한다.\n3. 결과는 쉼표로 구분한다.\n4. 설명 문장은 쓰지 않는다.\n1.3 강조(Emphasis) 사용\n중요 조건을 굵게 또는 기울임으로 표시해 모델의 우선순위를 높인다. 실제로 이탤릭, 볼드체로 표현은 되지 않는다. 언어모델에게 일반 줄글의 텍스트가 아닌 다른 요소임을 강조.\n- 반드시 **한국어**로 작성하라.\n- 출력은 **한 줄**로만 작성한다.\n- *설명 문장은 포함하지 않는다.*\n\n코드 프롬프트(Code Prompting)\n\n구조화를 좀더 구체적으로 하는 방식에 대해서 소개. AdaPlanner 논순 발췌, 구글의 리서처 분들이 많이 사용한다. 파이썬 언어를 사용해서 언어 모델이 정확히 수행 해야 하는 내용을 작성. 자연어 대신 코드 형식으로 의도를 명세하면 애매함이 줄어들고 LLM이 구조적으로 이해해 할루시네이션이 감소한다.\n# 역할 정의\nrole = \"정보 추출 엔진\"\n\n# 작업 정의\ntask = \"텍스트에서 '장소명'을 추출해 정렬된 리스트로 반환\"\n\n# 출력 형식 정의\noutput = {\n    \"places\": [\"string\"]\n}\n코드 방식의 장점:\n\n논리 구조가 명확하게 표현됨\n조건과 출력 형식이 혼동되지 않음\n정형화된 포맷을 자주 재사용 가능\n\n\nXML 태그 기반 구조화 (Claude 최적화)\n\n엔트로픽 프롬팅 쿡북 참조하면 나오는 내용. 문장 내 구역 나눔, 해당 구적을 강조하는 효과, 반복작업에서 프롬프트 수정과 편집/용이, 언어모델의 처리 속도 향상 XML 태그는 텍스트 내 특정 영역을 명확히 구분해 Claude뿐 아니라 대부분의 LLM에서 구조적 파싱을 돕는 강력한 방식이다.\n이 방식의 장점은 탬플릿이 가능하다.\n\n태그로 영역 지정\n\n&lt;email&gt;\nShow up at 6AM because I say so.\n&lt;/email&gt;\n\nMake this email more polite.\nLLM은 &lt;email&gt;...&lt;/email&gt; 내부 텍스트만 변환하면 된다는 것을 명확히 인식한다.\n\n데이터 템플릿 예시\n\n&lt;animal&gt;{{ANIMAL}}&lt;/animal&gt;\n입력만 바꿔 여러 데이터를 동일한 템플릿에 적용할 수 있다.\n예:\n&lt;animal&gt;Cow&lt;/animal&gt;\n&lt;animal&gt;Dog&lt;/animal&gt;\n&lt;animal&gt;Seal&lt;/animal&gt;\n장점:\n\n구역별 의미가 명확\n반복 작업에서 수정이 쉽고 안정적\n모델이 태그 단위로 파싱하기 때문에 오류 감소\n\n\n구조화 기법이 필요한 이유\n\n\n명령어, 입력 데이터, 출력 형식을 명확히 구분해 모델의 해석 오류를 줄인다.\n규칙을 시각적으로 구분하여 작업 수행 품질의 일관성과 재현성을 높인다.\n재사용 가능한 템플릿이 되어 프롬프트 제작 비용과 시간 절감에 기여한다.\n\n\n\n\n\n한국어는 짧고 명확하게 작성\n톤·형식·분량을 구체적으로 지시\n\n- 나쁜 예\n5월의 봄을 주제로 시를 써주고 짧게\n작성해줘 봄의 느낌을 살려봐.\n결과: 오월의 푸른 하늘 아래\n새싹이 희망을 노래하네.\n아침 이슬에 빛나는 꽃들,\n봄바람 따라 춤추는 나비.\n모든 거리는 축제가 되어.\n\n- 좋은 예\n너는 시인이야. 한국의 계절, 봄을 상상해봐.\n5월이야. 다양한 꽃이 여기저기 폈어.\n그 분위기에 어울리는 짧은 시를 5줄 이내로\n작성해줘. 한글 고유의 멋이 시에 드러나면 좋겠어.\n결과: 봄바람, 너른 들녘을 스치며\n꽃잎들 춤추듯 흩날리네\n산들산들 부는 바람에\n눈부신 햇살 아래\n은은한 꽃 향기에 취하노라\n\n\n\n\n부정 지시보다 긍정적 행동 지시가 안정적 구체적인 내용을 단문 형식으로 제공함과 언어모델에게 추측을 해야 하는 여지를 주면 안된다.\n\n- 나쁜 예\n다음은 에이전트와 고객의 대화이다.\n고객에게 아이디와 비밀번호를 묻지\n않는다. 반복하지 않는다.\n\n- 좋은 예\n다음은 에이전트와 고객과의 대화내용이다.\n챗봇은 고객의 대화에서 드러난 문제를 진단하고 해결책을\n제시해야 한다. 사용자 아이디와 비밀번호같은 개인정보를\n묻지 않는 대신,\n사용자가 www.samplewebsite.com/help/faq에서 관련\n사항을 찾도록 안내해라."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#프롬프트-제작에-100-절대적인-것은-없다.",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#프롬프트-제작에-100-절대적인-것은-없다.",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "",
    "text": "절대 나와서는 안되는 조건을 적용하기\n되도록 LLM 이 추론을 할 수 없게 구체적으로 주기\n긍정문을 쓰되 절대 나와서 안되는 문구들은 2중 부정을 사용"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#llm-한계",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#llm-한계",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "2.1 LLM 한계",
    "text": "2.1 LLM 한계\n\n할루시네이션\n편향성\n근거 없는 출처 생성\n수학 및 추론 한계\nPrompt Injection(프롬프트 해킹)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#프롬프트-엔지니어링이-필요한-이유",
    "href": "docs/blog/posts/Prompt_Engineering/01-Prompt-Structure/02-Prompt-Structure.html#프롬프트-엔지니어링이-필요한-이유",
    "title": "Part 1 — 프롬프트 구조와 기능 탐구",
    "section": "2.2 프롬프트 엔지니어링이 필요한 이유",
    "text": "2.2 프롬프트 엔지니어링이 필요한 이유\n\n정확성 향상\n오류 및 편향 감소\n모델 행동 제어\n사용자 경험 개선\n비용 절감"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "ChatGPT 사용자 경험 연구 결과, 많은 사용자들이 AI 답변에 불만을 느끼고 있습니다.\n\n\n\n\n실제 ChatGPT 대화 로그를 분석한 연구에서: - 107명의 응답자로부터 307개의 ChatGPT 대화 로그 수집 - 511개의 사용자 불만 사례 분석 - 7가지 사용자 불만 카테고리 식별\n\n\n\n가장 빈번한 불만은 다음과 같습니다:\n\n\n\n의도와 다른 답변 제공\n맥락 파악 실패\n\n\n\n\n\n잘못된 정보 포함\n사실과 다른 내용\n\n\n\n\n\n답변이 너무 일반적임 (21.7%)\n답변 형식 오류 (20.5%)\n답변 거절 (3.9%)\n\n\n\n\n\n불만족스러운 답변에 대한 사용자 대응 전략 5가지:\n\n프롬프트 재사용: 이전 프롬프트를 그대로 재사용\n의도 구체화: 질문을 구체화하여 재질문\n오류 식별 및 수정: 잘못된 답변 지적 및 수정 요청\n작업 적응: 작업이나 요구사항 변경 후 재시도\n없음 (No Tactic): 특별한 전략 없이 진행하거나 포기\n\n\n\n\n불만족스러운 답변에 대해 60.9%가 단순히 질문을 구체화하여 재질문합니다.\n사용자들은 프롬프트 작성에 최소한의 노력만 기울입니다.\n\n‘전술 없음’ 또는 ‘프롬프트 재사용’을 자주 사용\n효과적인 프롬프트 작성 방법에 대한 지식 부족\n질문 개선 방법을 모름\nAI와 효과적 소통 방법을 이해하지 못함\n같은 질문을 반복하거나 약간만 수정\n\n사용자들은 AI의 한계를 탓하지만, 실제로는 자신의 질문 방식에 문제가 있다는 것을 인식하지 못합니다.\n이것이 프롬프트 엔지니어링의 필요성이 대두되는 이유입니다.\n\n\n\n\n급격히 변화하는 AI 시대에서 사회적 기능을 유지하려면 일상의 혁신이 필요합니다.\n단순히 질문을 던지는 것이 아니라, 깊이 생각하고 행동으로 실천해야 합니다.\n“Think First” not “Just Do it”\n\n\n\n\n과거에는 모두에게 동일한 서비스를 제공했습니다. 이후 그룹 맞춤화로 발전했고, 이제는 개개인의 고유한 특성을 고려한 하이퍼 퍼스널라이제이션 시대입니다.\nAI와의 상호작용도 개인화되어야 합니다. 각 사용자의 목적, 맥락, 선호도를 반영한 프롬프트를 작성하는 것이 프롬프트 엔지니어링입니다.\n일반적인 질문에는 일반적인 답변만 나옵니다. 개인 특성을 고려한 프롬프트는 최적화된 답변을 제공합니다. 이것이 프롬프트 엔지니어링이 중요한 이유입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#연구-개요",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#연구-개요",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "실제 ChatGPT 대화 로그를 분석한 연구에서: - 107명의 응답자로부터 307개의 ChatGPT 대화 로그 수집 - 511개의 사용자 불만 사례 분석 - 7가지 사용자 불만 카테고리 식별"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#주요-불만-사항",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#주요-불만-사항",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "가장 빈번한 불만은 다음과 같습니다:\n\n\n\n의도와 다른 답변 제공\n맥락 파악 실패\n\n\n\n\n\n잘못된 정보 포함\n사실과 다른 내용\n\n\n\n\n\n답변이 너무 일반적임 (21.7%)\n답변 형식 오류 (20.5%)\n답변 거절 (3.9%)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#사용자들의-해결-전략",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#사용자들의-해결-전략",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "불만족스러운 답변에 대한 사용자 대응 전략 5가지:\n\n프롬프트 재사용: 이전 프롬프트를 그대로 재사용\n의도 구체화: 질문을 구체화하여 재질문\n오류 식별 및 수정: 잘못된 답변 지적 및 수정 요청\n작업 적응: 작업이나 요구사항 변경 후 재시도\n없음 (No Tactic): 특별한 전략 없이 진행하거나 포기"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#문제의-근본-원인-사용자의-최소한의-노력",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#문제의-근본-원인-사용자의-최소한의-노력",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "불만족스러운 답변에 대해 60.9%가 단순히 질문을 구체화하여 재질문합니다.\n사용자들은 프롬프트 작성에 최소한의 노력만 기울입니다.\n\n‘전술 없음’ 또는 ‘프롬프트 재사용’을 자주 사용\n효과적인 프롬프트 작성 방법에 대한 지식 부족\n질문 개선 방법을 모름\nAI와 효과적 소통 방법을 이해하지 못함\n같은 질문을 반복하거나 약간만 수정\n\n사용자들은 AI의 한계를 탓하지만, 실제로는 자신의 질문 방식에 문제가 있다는 것을 인식하지 못합니다.\n이것이 프롬프트 엔지니어링의 필요성이 대두되는 이유입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#변화하는-사회에서의-일상의-혁신",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#변화하는-사회에서의-일상의-혁신",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "급격히 변화하는 AI 시대에서 사회적 기능을 유지하려면 일상의 혁신이 필요합니다.\n단순히 질문을 던지는 것이 아니라, 깊이 생각하고 행동으로 실천해야 합니다.\n“Think First” not “Just Do it”"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#하이퍼-퍼스널라이제이션-시대의-프롬프트-엔지니어링",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/01-프롬프트분석.html#하이퍼-퍼스널라이제이션-시대의-프롬프트-엔지니어링",
    "title": "Part 1 — 프롬프트 분석의 필요성과 가치",
    "section": "",
    "text": "과거에는 모두에게 동일한 서비스를 제공했습니다. 이후 그룹 맞춤화로 발전했고, 이제는 개개인의 고유한 특성을 고려한 하이퍼 퍼스널라이제이션 시대입니다.\nAI와의 상호작용도 개인화되어야 합니다. 각 사용자의 목적, 맥락, 선호도를 반영한 프롬프트를 작성하는 것이 프롬프트 엔지니어링입니다.\n일반적인 질문에는 일반적인 답변만 나옵니다. 개인 특성을 고려한 프롬프트는 최적화된 답변을 제공합니다. 이것이 프롬프트 엔지니어링이 중요한 이유입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "",
    "text": "이 장을 통해 다음을 학습할 수 있습니다:\n\n사용자의 의도를 정확하게 알기 위해 그들의 언어 표현을 이해한다: 사용자가 프롬프트에서 표현하는 언어를 분석하여 정확한 의도를 파악합니다.\n그 뒷턴에서 어떻게 상호작용을 하고 있고, 어떤 반응을 보이고 그 반응의 의미가 무엇인지 확인: AI 응답을 받은 사용자가 어떻게 상호작용하는지, 어떤 반응을 보이는지, 그리고 그 반응의 의미를 분석합니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#이론의-기원과-창시자",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#이론의-기원과-창시자",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "2.1 이론의 기원과 창시자",
    "text": "2.1 이론의 기원과 창시자\n대화 분석(Conversation Analysis, CA)은 1960년대 후반부터 1970년대 초반에 Harvey Sacks, Emanuel Schegloff, Gail Jefferson 등에 의해 창시된 사회학적 연구 방법론입니다. 이들은 Harold Garfinkel의 민속방법론(Ethnomethodology)의 영향을 받아, 일상적인 대화가 어떻게 구조화되고 조직화되는지를 체계적으로 연구하기 시작했습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#핵심-연구-방법론",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#핵심-연구-방법론",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "2.2 핵심 연구 방법론",
    "text": "2.2 핵심 연구 방법론\n대화 분석은 다음과 같은 특징을 가집니다:\n\n자연스러운 대화 관찰: 실험실이 아닌 실제 일상 상황에서 발생하는 자연스러운 대화를 녹음하고 전사(transcription)하여 분석합니다\n미시적 분석: 대화의 세부적인 구조, 턴 교환, 순서, 타이밍 등을 면밀히 분석합니다\n참여자의 관점: 대화 참여자들이 실제로 어떻게 대화를 구성하고 이해하는지에 초점을 둡니다"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#네-가지-기준의-이론적-근거",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#네-가지-기준의-이론적-근거",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "2.3 네 가지 기준의 이론적 근거",
    "text": "2.3 네 가지 기준의 이론적 근거\n여기서 사용되는 네 가지 기준(Turn, Action, Structure, Stance)은 대화 분석 이론의 핵심 개념들을 사용자-AI 상호작용에 적용한 것입니다:\n\nTurn(턴): Sacks, Schegloff, Jefferson이 개발한 턴 교환 시스템(Turn-Taking System) 이론\nAction(행위): 대화에서 수행되는 행위의 유형을 분류하는 개념\nStructure(구조): 특히 Preference Structure(선호 구조) 이론 - Sacks와 Schegloff가 개발한 것으로, 대화에서 선호되는 응답과 비선호되는 응답의 구조적 차이를 분석\nStance(태도): 대화 참여자의 입장과 태도를 나타내는 개념"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#현대적-적용-사용자-ai-상호작용-분석",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#현대적-적용-사용자-ai-상호작용-분석",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "2.4 현대적 적용: 사용자-AI 상호작용 분석",
    "text": "2.4 현대적 적용: 사용자-AI 상호작용 분석\n전통적인 대화 분석은 사람과 사람 간의 대화를 분석했지만, 이 이론을 사용자-AI 상호작용에 적용하여 사용자 세그먼트를 분류하고 프롬프트를 기획하는 데 활용할 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#첫-번째-기준-turn",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#첫-번째-기준-turn",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "3.1 첫 번째 기준: Turn",
    "text": "3.1 첫 번째 기준: Turn\n\n3.1.1 싱글턴\n싱글턴은 한 번의 질문-답변으로 완결되는 대화 구조입니다.\n\n사용자가 한 번의 질문으로 원하는 정보를 얻습니다\n대화가 짧고 간결합니다\n\n\n\n\n3.1.2 멀티턴\n멀티턴은 여러 번의 질문-답변을 통해 목적을 달성하는 대화 구조입니다. 주제의 개수와는 크게 상관이 없이, 여러번의 질문-답변의 구조로 대화를 진행하면 멀티턴이라고 할 수 있습니다.\n\n여러 가지 주제를 다룹니다\n또는 한 주제에 대해 단계적으로 질문을 이어갑니다\n여러 번의 상호작용을 통해 목적을 달성합니다"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#두-번째-기준-action",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#두-번째-기준-action",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "3.2 두 번째 기준: Action",
    "text": "3.2 두 번째 기준: Action\n\n3.2.1 사용자의 행위\n사용자가 AI와 상호작용할 때 수행하는 행위(Action)는 크게 두 가지로 분류할 수 있습니다:\n\n정보 검색 유형: 특정 정보를 찾거나 질문에 답을 얻는 행위\n\n예: “오늘 날씨 알려줘”, “파이썬 문법 설명해줘”, “건강식품 추천해줘”\n\n다른 행위 유형: 정보 검색 이외의 다양한 행위\n\n작문: 텍스트 생성, 요약, 번역 등\n상담: 전문가의 조언이나 안내를 받는 행위\n캐릭터 AI 게임: 특정 캐릭터나 페르소나와의 대화형 게임이나 역할극\n나만의 상담사: 개인 맞춤형 상담 서비스나 심리 상담 등\n창작 활동: 스토리 작성, 시나리오 작성, 아이디어 브레인스토밍 등\n학습 도우미: 문제 풀이, 개념 설명, 학습 계획 수립 등\n\n\n\n\n3.2.2 Action 유형 및 판단 근거\nAction 유형을 판단하는 여러 가지 근거가 있습니다:\n1. 동사 분석\n사용자의 동사를 보면 유형을 판단할 수 있습니다:\n\n정보 검색 유형: “알려줘”, “설명해줘”, “찾아봐”, “추천해줘”, “어떻게 해야 해”, “방법 알려줘” 등\n작문 유형: “써줘”, “쓸거야”, “써봐”, “작성해줘”, “만들어줘”, “나열해줘”, “추가해줘”, “변경해줘” 등\n번역 유형: “번역해줘”, “번역해봐”, “영어로”, “한국어로” 등\n상담 유형: “조언해줘”, “도와줘”, “어떻게 생각해”, “추천해줘” (상황에 따라) 등\n\n2. 키워드 분석\n프롬프트에 포함된 키워드를 분석하여 판단할 수 있습니다:\n\n정보 검색 키워드: “알려줘”, “설명”, “방법”, “어떻게”, “무엇”, “왜”, “언제”, “어디서” 등\n작문 키워드: “작성”, “글”, “문서”, “요약”, “정리”, “구조”, “형식” 등\n창작 키워드: “이름 지어줘”, “아이디어”, “스토리”, “시나리오” 등\n상담 키워드: “조언”, “도움”, “추천”, “어떻게 해야 할까” 등\n\n3. 프롬프트의 목적 분석\n사용자가 원하는 최종 결과물의 성격을 분석합니다:\n\n정보 제공이 목적: 기존 정보를 찾거나 설명을 받는 경우 → 정보 검색\n새로운 콘텐츠 생성이 목적: 텍스트, 문서, 아이디어 등을 새로 만드는 경우 → 작문/창작\n상호작용이 목적: 대화, 게임, 역할극 등을 즐기는 경우 → 다른 행위 유형\n\n4. 문맥 분석\n프롬프트의 전체적인 문맥을 분석합니다:\n\n질문 형식: “~는 무엇인가?”, “~는 어떻게 하는가?” → 정보 검색\n요청 형식: “~를 작성해줘”, “~를 만들어줘” → 작문\n역할 설정: “너는 ~야”, “당신은 ~입니다” → 상담 또는 게임/역할극\n\n5. 출력물의 성격 분석\nAI가 생성해야 하는 결과물의 성격을 분석합니다:\n\n기존 정보의 설명: 정보 검색\n새로운 텍스트 생성: 작문\n대화형 상호작용: 상담, 게임, 역할극\n\n\n\n\n\n\n\n\n사용자 프롬프트\nAction\n\n\n\n\n콜드브루의 나라별 브랜드명 알려줘\n정보 검색\n\n\n네이버 블로그에 국내 핀테크 기업에 대한 소개글을 쓸거야 서론 적어줘\n작문\n\n\n여러 개의 이미지가 포함된 파일에서 한 개의 파일만 따로 분리하는 방법 설명해봐\n정보 검색\n\n\n이 문장에서 앞뒤 내용 반전시키는 문장을 추가해줘\n작문\n\n\n반지의제왕 소설을 발단, 전개, 위기, 절정, 결말의 5막 구조 형식으로 요약해서 써봐\n작문\n\n\n이링크와비슷한제품을팔고있는사이트세개를찾아봐\n정보 검색\n\n\n너는 면접 전문가야. 나는 대학의 행정 팀에 지원할 예정이야. 예상되는면접질문을나열해줘\n작문\n\n\n\n\n\n3.2.3 사용자의 Action을 추출하는 Prompt 예시\nPrompt :\n주어진 텍스트를 읽고 사용자의 \"Intention\"을\n추출해. Intention은 다음처럼 단어로 뽑아야해.\n- 정보 검색, 작문, 번역, 상담\n\n\n3.2.4 핵심 포인트\n\n💡 실무적 활용: 이렇게 다양한 분석 방법을 통해 어떤 누군가의 프롬프트를 보고 유형을 판단하면, 해당 프롬프트가 정보 검색 유형인지 작문 유형인지 등을 파악할 수 있습니다. 이를 통해 정보 검색을 더 잘할 수 있게끔 또는 작문을 더 잘할 수 있게끔 고도화할 수 있는 아이디어를 얻을 수 있습니다.\n\n\n\n3.2.5 ⚠️ 중요: 추후 액션 범주 파악을 위한 아이디어 회의 필요\n\n💡 기획 단계에서 유념해야 할 사항: 현재 프로그램이 아직 기획되지 않은 단계라면, 사용자들이 어떤 질문을 던질지 예측하기 어렵습니다. 그러나 프로그램이 출시되고 실제 사용자 데이터가 축적된 후에는 반드시 사용자들의 액션 범주를 파악하기 위한 아이디어 회의를 진행해야 합니다.\n이 시점에서 Action 기준을 활용하여 사용자 발화 데이터를 분석하면, 실제로 사용자들이 어떤 행위 유형(정보 검색, 작문, 번역, 상담 등)을 주로 사용하는지 파악할 수 있습니다. 이를 통해 서비스 개선 방향을 설정하고, 각 액션 유형에 맞는 최적화된 프롬프트를 기획할 수 있습니다.\n지금 당장은 실행할 수 없더라도, 이 분석 방법을 기억하고 유념해두시기 바랍니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#세-번째-기준-structure",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#세-번째-기준-structure",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "3.3 세 번째 기준: Structure",
    "text": "3.3 세 번째 기준: Structure\n\n3.3.1 AI 답변을 받은 사용자는 만족할까? 불만족할까?\nStructure는 사용자가 AI의 응답에 대해 만족하거나 불만족할 때 나타나는 대화 구조를 분석하는 기준입니다.\n\nAI의 결과에 만족한 사용자는 “보이지만 눈에 띄지 않는(Seen but unnoticed)” 특성을 보입니다. 즉, 만족한 사용자의 응답은 대화 흐름에 자연스럽게 녹아들어 특별히 주목받지 않습니다. 반면에 불만족을 하는 경우에는 “눈에 띄고 설명 가능하며(noticeable and accountable)”, 심지어 “제재 가능한(sanctionable)” 수준까지 명시적으로 표현하는 경향이 있습니다. 이는 불만족이 대화 구조에서 더 두드러지고 설명이 필요하며, 때로는 수정이나 보완을 요구하는 행동으로 이어질 수 있음을 의미합니다.\n\n\n3.3.2 Preference 구조의 세부 특성 이해하기\n이미지에서 보여주는 Preference 구조를 더 자세히 살펴보면:\n\n“보이지만 눈에 띄지 않는(Seen but unnoticed)” - Preferred Response\n\n\n만족한 사용자의 응답은 대화 흐름에 자연스럽게 녹아듭니다\n특별한 주목이나 설명이 필요하지 않습니다\n예: “감사합니다”, “좋아요”, “완벽해요” 등 짧고 긍정적인 응답\n\n\n“눈에 띄고 설명 가능하며(noticeable and accountable)” - Dispreferred Response의 첫 번째 단계\n\n\nNoticeable(눈에 띄는): 불만족한 응답은 대화에서 두드러지게 나타납니다\nAccountable(설명 가능한): 불만족한 응답은 왜 그런 응답을 했는지에 대한 설명이나 정당화가 필요합니다. 즉, 사용자가 불만을 표현할 때는 그 이유를 설명하거나, 왜 수정이 필요한지 명시적으로 표현합니다\n예시: “이 답변은 제가 원하는 형식이 아니에요. 좀 더 구체적으로 설명해주세요” → 불만을 표현하면서 그 이유를 설명함\n\n\n“눈에 띄고, 설명 가능하며, 제재 가능한(noticeable, accountable, and sanctionable)” - Dispreferred Response의 두 번째 단계\n\n\n앞의 특성에 더해 Sanctionable(제재 가능한): 불만이 더 강해지면 수정이나 보완을 요구하는 수준까지 이릅니다\n사용자가 AI의 응답에 대해 명시적으로 수정을 요구하거나, 잘못된 부분을 지적합니다\n예시: “이건 완전히 잘못된 정보예요. 다시 답변해주세요” → 불만을 표현하고, 이유를 설명하며, 수정을 요구함\n\n핵심 차이점: - Preferred: 설명이나 정당화가 필요 없음 (자연스러운 흐름) - Dispreferred: 불만을 표현할 때 반드시 그 이유를 설명하거나 정당화해야 함 (accountable)\n\n\n3.3.3 Preferred Response (선호 응답 구조)\n사용자가 AI의 응답에 만족할 때 나타나는 구조입니다. 만족한 사용자는 보이지만 두드러지지는 않는 성향을 보입니다. 특징은 다음과 같습니다:\n\n답변 길이가 짧다: 단답형 응답\n언어 표현이 다양하다: 감사, 긍정적 평가 등 다양한 표현 사용\n주요 표현: “감사합니다”, “고맙습니다”, “땡큐”, “대단하다”, “좋아”, “와우!”, “맘에 들어요” 등\n\n\n\n3.3.4 Dispreferred Response (비선호 응답 구조)\n사용자가 AI의 응답에 불만족할 때 나타나는 구조입니다. 특징은 다음과 같습니다:\n\n답변 길이가 길다: 불만을 설명하거나 수정을 요청\n불만의 정도: 약한 불만 → 중간 불만 → 강한 불만\n주요 행동: 아니 ~ 재질문, 확실해?, 할줄아는게 뭐니?, 부인, 비난, 반박, 거절, 질책 등\n\n\n\n3.3.5 사용자의 만족과 불만족 시에 따른 언어 특징\n\n\n\n\n\n\n\n\n구분\n만족\n불만족\n\n\n\n\n턴 구조\n선호\n비선호\n\n\n특징\n답변 길이가 짧다. 단답형이다. 언어 표현이 다양하다.\n답변 길이가 길다.\n\n\n행동\n답변에 대한 감사, 답변에 대한 평가\n불만의 정도: 약한, 중간, 강함, 부인, 비난, 반박, 거절, 질책"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#네-번째-기준-stance",
    "href": "docs/blog/posts/Prompt_Engineering/02-프롬프트의_이해/03-대화분석.html#네-번째-기준-stance",
    "title": "Part 3 — 대화 분석(사용자-AI 상호작용 매커니즘)",
    "section": "3.4 네 번째 기준: Stance",
    "text": "3.4 네 번째 기준: Stance\n\n3.4.1 사용자는 AI에게 감정이 있을까? 없을까?\nStance는 “어떤 것에 대해서 내 입장은 무엇인가?”라는 사전적인 의미를 지니고 있습니다. 이는 사용자가 AI와의 대화에서 보이는 감정적 태도를 분석하는 기준입니다.\n정보검색 유형이나 다른 행위의 유형 같은 경우에는 특별한 감정이 없는 경우가 많습니다.\n\n\n3.4.2 Affiliation: 사용자와 AI 간의 관계 설정\nStance 분석에서 가장 중요한 핵심 개념은 Affiliation(유대감, 관계 설정)입니다. 감정이 있고 없고의 문제는 실제로 “나와의 관계를 어떤 식으로 두느냐”를 의미합니다. 즉, 사용자가 AI를 단순한 도구로 보는지, 아니면 상호작용하는 상대방으로 인식하는지를 나타냅니다.\n\n\n3.4.3 감정적 태도 vs. 비감정적 태도\n사용자는 때때로 AI에게 감정을 표현하기도 합니다. 이는 단순한 정보 교환을 넘어서는 상호작용을 의미할 수 있습니다.\n\n감정적 태도 (Affiliation 있음): 감정 표현, 친근한 톤, 개인적 경험 공유 등. AI와의 관계를 형성하고 유지하려는 의도가 드러남\n비감정적 태도 (Affiliation 없음): 객관적이고 사실적인 정보 요청. AI를 도구로만 활용하며 관계 형성에 관심이 없음"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "",
    "text": "프롬프트 엔지니어링 기초 기법을 이해한다.\n연구 논문을 통해 각 프롬프트 엔지니어링 기법의 핵심과 한계를 이해한다.\n실습을 통해 각 프롬프트 엔지니어링 방법을 학습한다.\n\n\n\n\n\nZero-Shot Prompting\nFew-Shot Prompting\nChain-of-Thought Prompting\nZero Shot Chain of Thought\nSelf-Consistency"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#학습-목표",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#학습-목표",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "",
    "text": "프롬프트 엔지니어링 기초 기법을 이해한다.\n연구 논문을 통해 각 프롬프트 엔지니어링 기법의 핵심과 한계를 이해한다.\n실습을 통해 각 프롬프트 엔지니어링 방법을 학습한다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#목차",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#목차",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "",
    "text": "Zero-Shot Prompting\nFew-Shot Prompting\nChain-of-Thought Prompting\nZero Shot Chain of Thought\nSelf-Consistency"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "2.1 개념",
    "text": "2.1 개념\nZero-Shot Prompting은 언어 모델에게 예제나 시연(Demonstrations)을 주지 않고 작업을 수행하는 방법입니다. 언어 모델이 기존 지식을 사용하여 작업을 추론하며, 대량의 데이터를 사전 학습했기 때문에 가능합니다.\n제로샷 프롬프트는 언어 모델의 강력한 기능 중 하나로, 최소한의 정보만으로도 다양한 작업을 수행할 수 있습니다. 특히 데이터가 없을때, 유용하게 사용 가능, 실무를 하면서 제일 많이 사용하는 기술이 제로샷이다. 이 기술은 특히 데이터가 부족한 상황에서 유용하게 활용됩니다.\n\n전통적인 Fine-tuning 방법과 비교설명: 예시가 주어질 떄 마다 Gradient를 계산해 weight 를 업데이트 한다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#예시",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#예시",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "2.2 예시",
    "text": "2.2 예시\n\n2.2.1 텍스트 분류 (Text Classification)\n다음 텍스트에서 긍정, 부정, 중립 중 하나로 분류해.\n텍스트: 나는 마라탕 맛이 그저 그랬어.\nSentiment:\n\n\n2.2.2 번역 (Translation)\n다음 한국어 단어를 영어로 번역해줘.\n단어: 인공 눈물\n\n\n2.2.3 폐쇄형 질의응답 (Closed Question Answering)\n대한민국의 수도는 서울이야?"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#instruction-tuning",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#instruction-tuning",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "2.3 Instruction Tuning",
    "text": "2.3 Instruction Tuning\n\n2.3.1 기술 개요\n\n언어 모델의 제로샷 능력을 향상시키기 위한 기술\n자연어 지시로 60개 이상의 NLP 데이터셋에 대해 모델을 미세 조정\nInstruction Tuning은 모델 크기가 충분할 때 효과적\n\n\n\n2.3.2 기술의 장점\n\n지시에 따라 모델의 응답을 미세 조정할 수 있음\n챗봇이 더 정확하고 좋은 결과물을 얻어냄\n다수의 작업 클러스터에서 Instruction tuning 시 모델이 해보지 않은 작업에 대한 성능 향상\n\n\n\n2.3.3 연구 결과\n연구 논문: Finetuned Language Models are Zero-Shot Learners (2021)\n\n137B 파라미터 모델 FLAN은 제로샷 성능에서 기본 모델보다 우수함\nInstruction Tuning은 사전 학습 데이터 없이도 NLP 데이터셋에서 모델의 성능을 올림\n자연어 추론, 독해, 폐쇄형 QA, 번역 등 다양한 작업에서 우수한 성능을 보임"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#rlhf-reinforcement-learning-from-human-feedback",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#rlhf-reinforcement-learning-from-human-feedback",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "2.4 RLHF (Reinforcement Learning from Human Feedback)",
    "text": "2.4 RLHF (Reinforcement Learning from Human Feedback)\n\n2.4.1 개념\n\n인간의 피드백을 활용하여 모델의 응답을 개선하는 방법\nInstruction Tuning과 RLHF를 함께 사용하여 모델의 성능 향상\nChatGPT의 학습 방법\n\n\n\n2.4.2 연구 배경\n연구 논문: Deep Reinforcement Learning from Human Preferences (2017)\n\n인간 피드백을 이용한 강화 학습(RL)\n인간 선호도 기반 피드백으로 보상 함수 최적화\n기존 보상 함수 설계 대신, 인간 피드백을 사용하여 더 나은 성능 달성"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-2",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-2",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "3.1 개념",
    "text": "3.1 개념\nFew-Shot Prompting은 언어 모델에게 예제나 시연(Demonstrations)을 주며 작업을 수행하는 방법입니다. Zero-shot prompting은 복잡한 문제 수행에 한계가 있어, Few-shot prompting을 통해 모델의 성능을 향상시킬 수 있습니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#특징",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#특징",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "3.2 특징",
    "text": "3.2 특징\n\nFew-shot prompting은 모델의 파라미터 수가 충분히 클 때 효과가 있음\n어렵고 복잡한 과제일수록, 예제를 많이 사용하여 해결할 수 있음"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#예시-1",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#예시-1",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "3.3 예시",
    "text": "3.3 예시\nA \"whatpu\" is a small, furry animal native to Tanzania.\nAn example of a sentence that uses the word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus.\n\nTo do a \"farduddle\" means to jump up and down really fast.\nAn example of a sentence that uses the word farduddle is:\n출력: When we won the game, we all started to farduddle in celebration."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-1",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-1",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "3.4 연구 결과",
    "text": "3.4 연구 결과\nFew-Shot Prompting 성능이 어느정도 인지 확인해보자.\n\n3.4.1 예시의 중요성\n연구 논문: Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? (2022)\n\n예시를 사용하면 모델의 성능을 향상시킴\n예시의 품질이 중요: 정답 라벨(Gold labels)이 가장 좋지만, 랜덤 예시도 모델의 추론에 도움이 됨\n프롬프트 포맷: 올바른 입력 라벨 형식을 유지하는 것이 중요\n\n\n\n3.4.2 성능 비교\n\nGold labels (정답 라벨이 있는 예시): 모델 성능이 가장 높음\nRandom labels (랜덤 라벨이 있는 예시): 예시가 없는 경우보다는 성능이 좋음\nNo input label format (입력 라벨 형식 없음): 성능이 크게 저하\n\n\n\n3.4.3 포맷 예시\n올바른 포맷:\n이건 정말 굉장해! // 긍정\n와 정말 나쁘다! // 부정\n그 영화 진짜 대박이더라. // 긍정\n아우 정말 끔찍해 // 부정\n랜덤 포맷 (성능 저하):\n부정 와 정말 나쁘다! 긍정\n이건 정말 굉장해! 긍정"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "3.5 한계점",
    "text": "3.5 한계점\n\n복잡한 추론(Reasoning) 문제는 잘 하지 못함\n예시를 제공해도 복잡한 수학 문제나 논리 문제에서는 정확한 답을 내지 못하는 경우가 많음"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-3",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-3",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "4.1 개념",
    "text": "4.1 개념\nChain-of-Thought Prompting (CoT)은 복잡한 과제 수행을 위해, LLM에게 더 자세한 안내 문구를 작성해주는 것입니다. 복잡한 산술(Arithmetic), 상식(Commonsense), 그리고 기호 추론(Symbolic reasoning) 작업에 사용하면 효율적입니다.\n답이 나오는 과정에 대해 설명한 예시를 보여주고, 문제를 풀기 위한 답을 생성할 때 예시와 같은 방식으로 LLM이 설명하도록 하는 방식입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#작동-방식",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#작동-방식",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "4.2 작동 방식",
    "text": "4.2 작동 방식\n\n4.2.1 Standard Prompting vs Chain-of-Thought Prompting\nStandard Prompting:\nQ: 카페테리아에 사과가 23개 있었습니다. 그들은 20개를 점심을 만드는 데 사용했고 6개를 더 샀습니다. 이제 사과가 몇 개 있습니까?\nA: 정답은 27입니다. ❌\nChain-of-Thought Prompting:\nQ: 카페테리아에 사과가 23개 있었습니다. 그들은 20개를 점심을 만드는 데 사용했고 6개를 더 샀습니다. 이제 사과가 몇 개 있습니까?\nA: 카페테리아에 원래 사과가 23개 있었습니다. 그들은 20개를 점심을 만드는 데 사용했습니다. 그래서 23 - 20 = 3개가 남았습니다. 그들은 6개를 더 샀기 때문에, 이제 3 + 6 = 9개가 있습니다. 정답은 9입니다. ✔"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-2",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-2",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "4.3 연구 결과",
    "text": "4.3 연구 결과\n연구 논문: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022)\n\n산술, 기호, 상식 추론에서 CoT 기법의 성능이 좋음\n수학 단어 문제를 푸는 성능 비교:\n\nFine-tuned GPT-3 175B: 55%\nPaLM 540B (standard prompting): 18%\nPaLM 540B (chain-of-thought prompting): 57%\n\n\n\n4.3.1 모델 크기의 영향\n\nChain of thought는 모델의 파라미터가 클 때 효과적인 기법\n일반적인 스케일링 곡선보다 더 높은 성능을 보임\n모델 크기가 증가할수록 CoT의 효과가 더욱 커짐"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점-1",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점-1",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "4.4 한계점",
    "text": "4.4 한계점\n\n모델의 파라미터 수가 적은 모델에서는 기법의 성능이 떨어짐\nCoT는 100B 파라미터 이상의 모델에서만 성능 향상을 보임\n사람이 직접 사고의 과정을 문장으로 작성해야 하는 번거로움\n프롬프트의 완성도가 높지 않으면 결과가 좋지 않음"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-4",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-4",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "5.1 개념",
    "text": "5.1 개념\nZero Shot Chain of Thought는 “단계적으로 생각해봐” (Let’s Think Step by Step)와 같은 트리거 문구를 사용하여 예시 없이도 Chain-of-Thought 추론을 유도하는 방법입니다."
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-3",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-3",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "5.2 연구 결과",
    "text": "5.2 연구 결과\n연구 논문: Large Language Models are Zero-Shot Reasoners (2022)\n\nZero-shot과 Zero-shot CoT의 성능 비교\n“Let’s think step by step”과 같은 트리거 문구를 사용하면 예시 없이도 추론 과정을 유도할 수 있음\nGSM8K(초등학교 수준의 수학 문제 데이터셋)에서 성능 향상을 보임\n\n\n5.2.1 다른 트리거 예시\n\n“단계적으로 생각해봐”\n“Let’s solve this step by step”\n“First, let’s think about…”"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-5",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#개념-5",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "6.1 개념",
    "text": "6.1 개념\nSelf-Consistency는 Chain-of-thought prompting을 개선한 기법으로, 다양한 추론 경로를 만들어 그 중에서 가장 일관된 답변을 선택하는 방식입니다. 복잡한 산술 문제나 논리 문제에 효과적입니다.\n\n6.1.1 핵심 아이디어\n가장 많은 답이 정답일 확률이 높다는 단순한 아이디어를 기반으로 합니다.\n예시: - 학생 A: 11 - 학생 B: 11 - 학생 C: 12 - 학생 D: 11 - 학생 E: 11 - 최종 답: 11 (다수결)"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#작동-방식-1",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#작동-방식-1",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "6.2 작동 방식",
    "text": "6.2 작동 방식\n\nChain of thought: 문제 제시\nSample a diverse set of reasoning paths: 다양한 추론 경로 샘플링\nMarginalize out reasoning paths to aggregate final answers: 최종 답 선택"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-4",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#연구-결과-4",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "6.3 연구 결과",
    "text": "6.3 연구 결과\n연구 논문: Self-Consistency Improves Chain-of-Thought Reasoning in Language Models (2022)\n\n6.3.1 세 단계 프로세스\n\n1단계: CoT 프롬프트 엔지니어링으로 시작. 언어 모델에게 문제를 해결하는 과정을 단계별로 설명하도록 함.\n2단계: 여러 가지 경로 샘플링. 새로운 문제에 대한 여러 계산 경로를 탐색.\n3단계: 모델이 일관된 답변을 도출하기 위해 2단계 샘플링을 통해 나온 결과를 종합하여 추론. 가장 많이 생성된 답변을 선택.\n\n\n\n6.3.2 성능\n\n수학 문제 정답을 맞춘 확률이 높음\n다수결 투표 방식의 정확도가 높음\n복잡한 추론 작업에서 비용 효율적\n다양한 추론 경로를 통한 높은 정확도"
  },
  {
    "objectID": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점-2",
    "href": "docs/blog/posts/Prompt_Engineering/04-프롬프트_제작의_기초/01-프롬프트_제작의_기초.html#한계점-2",
    "title": "프롬프트 엔지니어링 기초 기법 (Basic)",
    "section": "6.4 한계점",
    "text": "6.4 한계점\n\n복잡한 추론 작업을 해결할 때 비용이 많이 듦\n복잡한 추론을 해결하려면, 상당한 양의 훈련 데이터 필요\n프롬프트가 명확하지 않거나 간결하지 않을 때, Self Consistency의 기법 성능이 저하됨"
  }
]