---
title: "대규모 코드베이스 메타데이터 추출: 분산 에이전트에서 하이브리드 파이프라인까지"
subtitle: "152개 Python 파일, 1,411개 항목을 추출하며 배운 것들"
description: |
  Claude Code의 분산 에이전트로 대규모 코드베이스(AutoMSA, 152 Python 파일)의 메타데이터를 전수 추출한 실전 경험입니다.
  단일 에이전트 실패 -> 3개 분산 -> 7개 분산 -> 하이브리드 파이프라인 설계까지,
  각 단계에서 발견한 문제와 해결책을 정리합니다.
categories:
  - AI
  - Claude_Code
  - Code_Analysis
  - Architecture
author: Junhyun Lee
date: 02/13/2026
format:
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 대규모 코드베이스 메타데이터 추출

## 왜 코드베이스 메타데이터가 필요한가

인실리코(in-silico) 알고리즘 코드를 분석하는 **코드 설명 Agent**를 구축하려면, Agent가 코드를 이해하기 위한 구조화된 메타데이터가 필요합니다.

여기서 말하는 메타데이터란:

- 모든 클래스/함수/변수의 **물리명, 논리명, 데이터 타입, 기본값, 의미**
- 모듈 간 **입출력 연결 관계**
- 파이프라인 **단계 간 의존성**
- 중간 산출물의 **파일 스키마** (구분자, 인코딩, 컬럼 정의)
- 알고리즘 **파라미터, 임계값, 허용 범위**
- **암묵적 규칙** (코드에 명시되지 않은 관계, 상태 전이 조건)

이 메타데이터를 14개 카테고리로 분류하여 `METADATA_INVESTIGATION.md`라는 단일 문서로 생성하는 것이 목표입니다.

---

## 대상: AutoMSA 파이프라인

| 항목 | 값 |
|------|-----|
| 레포지토리 | `insilico-core-automsa` |
| Python 파일 수 | 152개 |
| 총 코드 줄 수 | ~29,178줄 |
| 파이프라인 단계 | 9개 서비스 (5 Step) |
| 역할 | 유전자/단백질 키워드 기반 서열 수집, BLAST 확장, 대표 서열 선정, 다중 서열 정렬(MSA) 자동화 |

파이프라인 구조:

```
AutoMSA -> DRS -> monoom -> multiom -> multiom-output -> multitom -> multiplex-excel
```

이 규모의 코드베이스에서 원자적 수준의 메타데이터를 추출하면 **1,400개 이상의 항목**이 나옵니다.

---

## 1차 시도: 단일 에이전트 (실패)

### 접근

하나의 Claude Code 에이전트에게 전체 152개 파일을 분석하라고 요청했습니다.

### 결과

**Context window 한계에 도달.** 약 50개 파일을 읽은 시점에서 컨텍스트가 부족해져, 나머지 100개 파일은 조사되지 않았습니다.

### 교훈

> **대규모 코드베이스는 단일 에이전트로 처리할 수 없다.** Context window는 유한하고, 152개 파일 x 평균 192줄 = ~29K줄의 코드를 한 번에 읽고 분석하는 것은 불가능합니다.

---

## 2차 시도: 3개 분산 에이전트

### 접근

14개 카테고리를 3개 그룹으로 나누어 병렬 실행:

- Agent A: Cat 1-3 (파이프라인 구조, 중간 산출물)
- Agent B: Cat 4-7 (입력/파라미터, DB, 품질)
- Agent C: Cat 8-14 (라이브러리, 환경, 인프라, 로깅, 기타)

### 결과

3개 에이전트 모두 완료. 그러나 **산출물의 형식이 프롬프트와 불일치**:

| 문제 | 상세 |
|------|------|
| N.1-N.4 구조 누락 | 각 카테고리에 "추출 가능 이유", "조사 대상 파일", "해석 메타데이터" 섹션이 없음 |
| 테이블 헤더 영문 | 한국어 8열 헤더 대신 영어 헤더 사용 |
| 통계표 누락 | 최종 카테고리별 항목 수 집계표 없음 |
| 품질 체크리스트 누락 | 12개 항목 체크리스트 없음 |

### 교훈

> **LLM은 추상적 규칙보다 구체적 예시를 훨씬 잘 따른다.** 프롬프트에 "N.1, N.2, N.3, N.4 구조로 작성하세요"라고 써도, 실제 완성된 예시가 없으면 에이전트가 자의적으로 해석합니다.

---

## 3차 시도: 7개 분산 에이전트 + 형식 교정

### 접근

- 참조 문서(multiom 레포의 METADATA_INVESTIGATION.md)를 Gold Standard로 제공
- 7개 에이전트로 더 세밀하게 분할:
  - Cat 1 / Cat 2-3 / Cat 4 / Cat 5-6 / Cat 7-8 / Cat 9-11 / Cat 12-14
- 각 에이전트에게 **구체적 형식 예시**와 **참조 문서**를 함께 전달

### 결과

| 카테고리 | 항목 수 |
|----------|---------|
| 1. 모듈 간 I/O | 213 |
| 2. 파이프라인 의존성 | 62 |
| 3. 중간 산출물 | 218 |
| 4. 입력 JSON/UI | 128 |
| 5. 알고리즘 파라미터 | 129 |
| 6. DB 스키마/쿼리 | 110 |
| 7. 품질 규칙 | 99 |
| 8. 라이브러리/API | 53 |
| 9. 환경 변수 | 35 |
| 10. 병렬 처리 | 18 |
| 11. 실행 모드 | 45 |
| 12. 인프라 | 44 |
| 13. 로깅 | 81 |
| 14. 기타 | 176 |
| **합계** | **1,411** |

형식은 올바르게 나왔으나, **Cat 3의 파일 스키마가 누락**되었습니다.

### 교훈

> **중요한 요구사항은 독립 섹션으로 분리해야 한다.** Cat 3 설명 안에 "파일 스키마를 반드시 기술하세요"라고 묻어 놓으면 에이전트가 놓칩니다. **CRITICAL** 표시와 함께 별도 섹션으로 올려야 합니다.

---

## Cat 3 파일 스키마 보강: 전용 에이전트

Cat 3의 36개 중간 산출물 파일에 대해 전용 에이전트를 추가 투입했습니다.

### 추출한 파일 스키마 속성 (9가지)

각 파일에 대해:

| 속성 | 예시 (InExCrossInfo.txt) |
|------|--------------------------|
| 구분자 | `\t` (탭) |
| 인코딩 | UTF-8 |
| 헤더 행 | 없음 |
| 행 의미 | 1행 = 1개 키-값 쌍 |
| 정렬 순서 | dict 순회 순서 |
| 생성 함수 | `get_in_ex_cross_info.py::output_default_in_ex_cross_file()` |
| 소비 함수 | `in_ex_param_info.py::InExParamInfo.__init__()` |
| 좌표 체계 | N/A |
| 컬럼 정의 | key(str) + value(str) |

### 부록으로 추가된 내용

- FASTA 헤더 타입 종합 정리 (SeqInfo 11가지 + MatchSeqInfo 4가지 헤더 형식)
- TSV 엔티티 컬럼 비교표 (IN/EX/CROSS 37개 컬럼 대조)

### 최종 산출물

- **3,782줄**의 METADATA_INVESTIGATION.md
- **1,411개** 원자적 메타데이터 항목
- **36개** 파일 스키마 상세
- 총 **8개 에이전트** 사용 (7개 카테고리 + 1개 파일 스키마)

---

## 핵심 인사이트 1: 분산 에이전트 프롬프트 설계 원칙

이 경험에서 도출한 6가지 프롬프트 개선 원칙입니다.

### 원칙 1: 분산 실행 프로토콜을 프롬프트에 명시하라

단일 에이전트 전제의 프롬프트를 여러 에이전트에게 나눠주면 실패합니다. **어떻게 나누고, 어떻게 합칠 것인지**를 프롬프트 자체에 포함해야 합니다.

```markdown
## 분산 실행 프로토콜

### 분할 단위
- Agent A: Cat 1-2 (파이프라인 구조)
- Agent B: Cat 3 (중간 산출물) -- 단독
- Agent C: Cat 4-5 (입력/파라미터)
...

### 에이전트별 산출물 규칙
- 파일명: `/tmp/{repo}_metadata/cat_{NN}.md`
- 자기 완결적 (헤더 불필요, 카테고리 내용만)
- 마지막에 `<!-- ROW_COUNT: {숫자} -->` 주석 추가
```

### 원칙 2: 카테고리별 대상 파일을 사전 매핑하라

모든 에이전트가 152개 파일을 각자 탐색하면 토큰 낭비입니다. 파일을 미리 분류해서 에이전트 범위를 좁혀야 합니다.

### 원칙 3: 추상적 규칙 대신 완성된 예시를 제공하라

"N.1-N.4 구조로 작성하세요"보다 **실제 완성된 카테고리 1개를 통째로** 예시로 보여주는 것이 효과적입니다.

### 원칙 4: 중요 요구사항은 독립 섹션 + CRITICAL 표시로 분리하라

카테고리 설명 안에 묻어 놓으면 에이전트가 놓칩니다.

### 원칙 5: 레포 규모별 자동 분할 기준을 포함하라

| Python 파일 수 | 권장 에이전트 수 |
|---------------|-----------------|
| ~30개 | 1 (단일) |
| 30~80개 | 3 |
| 80~150개 | 5~7 |
| 150개+ | 7~10 |

### 원칙 6: 오케스트레이터용 병합 체크리스트를 명시하라

병합 단계에서 통계표 누락, 행 수 불일치가 발생합니다. 오케스트레이터가 따를 체크리스트가 필요합니다.

---

## 핵심 인사이트 2: LLM 추출의 근본적 한계

프롬프트를 아무리 정밀하게 만들어도 해결할 수 없는 문제가 있습니다.

### 비결정성(Non-determinism) 문제

같은 프롬프트 + 같은 코드로 두 번 실행하면:

| 문제 | 예시 |
|------|------|
| 행 수 불일치 | 이번 1,411개, 다음번 1,380개 |
| 행 순서 변동 | `uclust_pct_id`가 3번이었다가 7번 |
| 논리명 흔들림 | "UCLUST Percent Identity" vs "UCLUST Identity Threshold" |
| 누락/추가 | private 메서드 포함 여부가 달라짐 |
| 카테고리 분류 흔들림 | 같은 항목이 Cat 5에 갔다가 Cat 7에 감 |

### 왜 이것이 문제인가

코드 설명 Agent가 메타데이터를 참조할 때, **실행마다 메타데이터가 달라지면 Agent의 응답도 불안정**해집니다. 주기적으로 메타데이터를 갱신해야 하는 운영 환경에서는 치명적입니다.

### 재현성 한계

| 접근법 | 재현성 |
|--------|--------|
| 정밀 프롬프트만 | ~70% |
| 프롬프트 + 참조 예시 | ~80% |
| **프롬프트로는 한계** | **100%는 불가능** |

---

## 핵심 인사이트 3: 하이브리드 파이프라인이 정답이다

### 발견: 8열 테이블의 75%는 결정적으로 추출 가능하다

```
| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |
     ├─────────────────────────────────────────┤  ├──────────────────┤
           코드 파서로 100% 결정적 추출               LLM 해석 필요
```

| 컬럼 | 추출 방법 | 결정적? |
|------|-----------|---------|
| `#` | 자동 번호 | 100% |
| `파일` | `glob("**/*.py")` | 100% |
| `클래스/함수/변수` | Python AST | 100% |
| `물리명` | Python AST | 100% |
| **`논리명`** | **LLM** | **비결정적** |
| `데이터 타입` | AST + type hints | 95% |
| `값/기본값` | AST | 100% |
| **`의미`** | **LLM** | **비결정적** |

**8열 중 6열은 코드 파서, 2열만 LLM으로 채우면 됩니다.**

### 하이브리드 아키텍처

```
Phase 1: 결정적 추출 (Python AST 파서)
───────────────────────────────────────
  glob + ast.parse → 모든 클래스/함수/변수/상수 추출
  ↓
  규칙 기반 카테고리 분류 (rules.yaml)
  ↓
  skeleton.json (8열 중 6열 완성, 논리명/의미는 빈 칸)

Phase 2: LLM 해석
─────────────────
  skeleton.json + 소스코드 컨텍스트 → Claude API
  ↓
  논리명 + 의미 + N.1/N.4 해석 메타데이터 생성
  ↓
  enriched.json (이전 실행 결과 참조하여 안정성 확보)

Phase 3: 렌더링
─────────────────
  enriched.json + Jinja2 템플릿
  ↓
  METADATA_INVESTIGATION.md (항상 동일 구조)

Phase 4: Diff 검증
─────────────────
  이전 버전과 비교 → 변경 리포트
  ↓
  안정성 점수, 신규/삭제/변경 항목 식별
```

### Phase 1: AST 추출기의 핵심

Python `ast` 모듈로 코드의 **사실(fact)**을 결정적으로 추출합니다:

```python
import ast, glob, os

class PythonASTExtractor:
    def extract_all(self, repo_path: str) -> list[dict]:
        items = []
        # sorted()로 파일 순서 고정
        for py_file in sorted(glob.glob(f"{repo_path}/**/*.py", recursive=True)):
            tree = ast.parse(open(py_file).read())
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    for item in node.body:
                        if isinstance(item, ast.AnnAssign):
                            items.append({
                                # id = 파일:줄번호:물리명 (실행 간 앵커)
                                "id": f"{rel_path}:{item.lineno}:{item.target.id}",
                                "file": rel_path,
                                "line_number": item.lineno,
                                "structure": f"{node.name} field",
                                "physical_name": item.target.id,
                                "data_type": ast.unparse(item.annotation),
                                "default_value": ast.unparse(item.value) if item.value else "-",
                                "logical_name": "",   # LLM이 채움
                                "meaning": "",        # LLM이 채움
                            })
        return sorted(items, key=lambda x: (x["file"], x["line_number"]))
```

**핵심:** `id`가 `파일:줄번호:물리명`으로 구성되어, 코드가 동일하면 **항상 동일한 ID**가 생성됩니다. 이 ID가 실행 간 일관성의 앵커 역할을 합니다.

### Phase 1: 규칙 기반 카테고리 분류

```yaml
# rules.yaml
categories:
  - id: 3
    name: "중간 산출물"
    rules:
      - type: "file_path_match"
        file_patterns: ["*/dto/*.py"]
        structure_types: ["class_field"]
        priority: 20  # DTO 필드는 무조건 Cat 3

  - id: 6
    name: "DB 스키마 및 쿼리"
    rules:
      - type: "file_path_match"
        file_patterns: ["*/repository/*.py", "*/entity/**/*.py"]
        priority: 18

# 매칭되지 않은 항목은 Cat 14 (기타)
fallback_category: 14
```

파일 경로, 구조 유형, 물리명 패턴으로 카테고리를 **규칙 기반**으로 분류합니다. LLM이 개입하지 않으므로 **항상 동일한 분류 결과**가 나옵니다.

### Phase 2: 이전 결과 참조 메커니즘

LLM이 담당하는 `논리명`과 `의미`의 안정성을 확보하는 핵심 메커니즘:

```python
def build_prompt(self, skeleton_items, previous_items):
    prompt = """
    ## 기존 항목 검증 (변경 불필요 시 그대로 유지)

    아래 항목들은 이전 실행에서 이미 해석되었습니다.
    코드 변경으로 인해 의미가 달라진 항목만 수정하세요.
    **변경이 없으면 반드시 그대로 유지하세요.**

    ## 신규 항목 해석 (새로 작성)

    아래 항목들은 이전 실행에 없던 신규 항목입니다.
    logical_name(영문 Title Case)과 meaning(한글 1~2문장)을 작성하세요.
    """
```

이전 실행 결과를 **명시적으로 제공**하고 "변경 없으면 유지"를 지시하면, LLM이 담당하는 2열도 **95%+ 안정성**을 확보합니다.

### Phase 3: Jinja2 템플릿

```jinja2
{% for cat in categories %}
## {{ cat.number }}. {{ cat.name }}

### {{ cat.number }}.1 추출 가능 이유
{{ cat.rationale }}

### {{ cat.number }}.2 조사 대상 파일 목록
{% for f in cat.files %}
- `{{ f }}`
{% endfor %}

### {{ cat.number }}.3 상세 내용

| # | 파일 | 클래스/함수/변수 | 물리명 | 논리명 | 데이터 타입 | 값/기본값 | 의미 |
|---|------|------------------|--------|--------|-------------|-----------|------|
{% for item in cat.items %}
| {{ item.number }} | `{{ item.file }}` | {{ item.structure }} | `{{ item.physical_name }}` | {{ item.logical_name }} | `{{ item.data_type }}` | {{ item.default }} | {{ item.meaning }} |
{% endfor %}

### {{ cat.number }}.4 해석 메타데이터
{{ cat.interpretation }}
{% endfor %}
```

같은 JSON이 들어오면 **항상 동일한 Markdown**이 출력됩니다. 병합 과정의 ad-hoc 작업이 완전히 제거됩니다.

### Phase 4: Diff 검증

```
=== 메타데이터 변경 리포트 ===
이전: 1,411개 -> 현재: 1,423개 (+12)
안정성 점수: 0.9915 (99.15%)

[신규 추가] 12개:
  + automsa/common/module/new_feature.py:15:threshold  (Cat 5)
  ...

[삭제] 0개
[의미 변경] 0개
[카테고리 이동] 0개
```

---

## 비용/시간/재현성 비교

| 항목 | 순수 LLM (7 에이전트) | 하이브리드 |
|------|----------------------|-----------|
| Phase 1 (추출) | - | **30초** (AST) |
| Phase 2 (해석) | 7 에이전트 x 5분 = **35분** | 14 API 호출 x 30초 = **7분** |
| Phase 3 (렌더링) | 수동 병합 **10분** | **1초** (Jinja2) |
| Phase 4 (검증) | 수동 확인 | **1초** (자동) |
| **총 시간** | **~50분** | **~8분** |
| **토큰 비용** | ~500K tokens | ~80K tokens (1/6) |
| **재현성** | ~70% | **~99%** |
| **코드 변경 시** | 전체 재실행 | 변경분만 재실행 |

---

## 점진적 도입 경로

한 번에 전체를 구축할 필요 없습니다:

### Step 1: AST Skeleton만 생성 (즉시 효과)

```bash
python extractor.py phase1 /path/to/repo --output skeleton.json
```

이것만으로도 **행 수, 행 순서, 카테고리 분류가 100% 고정**됩니다. skeleton.json을 에이전트에게 전달하면, 에이전트는 빈 칸(논리명, 의미)만 채우면 됩니다.

> Step 1만으로 가장 큰 문제(비결정성)의 80%가 해결됩니다.

### Step 2: LLM API 자동화 + 이전 결과 참조 (1주)

Claude API로 논리명/의미를 자동 채우고, previous.json 참조 메커니즘을 추가합니다.

### Step 3: 템플릿 + Diff 리포트 + CI/CD 통합 (2주)

Jinja2 렌더링, Diff 검증을 완성하고, 코드 변경 시 자동 실행되도록 CI/CD에 통합합니다.

---

## 실전에서 배운 세부 교훈들

### 에이전트 분할 시 Cat 3은 항상 단독 할당

Cat 3 (중간 산출물)은 DTO 필드 추출 + 파일 스키마 분석이 합쳐져 **가장 분량이 큽니다** (이번 작업에서 218개 항목 + 637줄 파일 스키마). 다른 카테고리와 묶으면 context window를 초과합니다.

### 파일 스키마 추출은 AST만으로 부족

TSV/FASTA 파일의 스키마(구분자, 컬럼 순서, 생산/소비 함수)는 AST로 **함수 시그니처**는 추출할 수 있지만, **실제 write 로직** (f-string 안의 컬럼 순서, 조건부 컬럼 등)은 코드 흐름 분석이 필요합니다. 이 부분은 LLM이 여전히 필요합니다.

### 병합 단계에서 빠지기 쉬운 항목들

- 통계표의 행 수 (에이전트가 `*`로 남겨두는 경우)
- 파이프라인 구조 다이어그램
- 전체 파일 목록 (상단)
- 품질 검증 체크리스트 (하단)

이들은 **오케스트레이터(리더 에이전트 또는 스크립트)**가 자동 생성해야 합니다.

### Permission 모드 주의

`bypassPermissions` 모드로 에이전트를 실행해도 `/tmp` 디렉토리 생성에 실패하는 경우가 있었습니다. 산출물 디렉토리는 **에이전트 실행 전에 미리 생성**해 두어야 합니다.

---

## 결론

대규모 코드베이스의 메타데이터를 주기적으로 추출해야 하는 상황에서:

1. **순수 LLM 접근은 한계가 있다** -- 비결정적이고, 비용이 높고, 느리다
2. **코드 파서(AST)가 사실을 고정하고, LLM은 해석만 담당하는 하이브리드가 정답이다**
3. **이전 실행 결과를 참조**하면 LLM 해석의 안정성도 95%+로 올릴 수 있다
4. **프롬프트 개선은 여전히 중요하지만**, 아키텍처 변경 없이 프롬프트만으로는 근본 문제가 해결되지 않는다

한마디로: **"무엇을 추출할 것인가"는 프롬프트의 영역이지만, "어떻게 안정적으로 반복할 것인가"는 아키텍처의 영역이다.**
